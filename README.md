# ðŸ‡ªðŸ‡¸ 3-Tier Data Lakehouse for Spanish Mobility Analysis

## ðŸ“‘ Table of Contents
1. [Authors](#authors)
2. [Methodology](#methodology)
3. [Repository Structure](#repository-structure)
4. [Data Access Logic](#data-access-logic)
5. [Setup & Installation](#setup--installation)
6. [Cloud Lakehouse Configuration](#cloud-lakehouse-configuration)
7. [Verification](#verification)
8. [Data Pipelines (DAGs)](#data-pipelines-dags)
9. [Visualization Dashboard](#visualization-dashboard)

## Authors

*   **MarÃ­a LÃ³pez HernÃ¡ndez**
*   **Fernando Blanco Membrives**
*   **Joan SÃ¡nchez VerdÃº**

**Date:** January 2026

This project implements a **3-tier Data Lakehouse** (Bronze, Silver, Gold) designed to process and analyze public mobility data from the Spanish Ministry of Transport (MITMA) and the National Statistics Institute (INE). 

The infrastructure follows a Medallion Architecture, utilizing **DuckDB** for processing, **AWS S3** for storage, and **Neon (Postgres)** for metadata management, with a **Streamlit** layer for interactive visualization.

## Methodology

This project follows an **Agile** methodology. We work in iterative sprints with clear goals and continuous delivery. 

Detailed documentation for each stage of developmentâ€”including sprint goals, task breakdowns, assignments, and retrospective outcomesâ€”can be found in the following directory:
`docs/sprint_logs/` (e.g., `Sprint-1.md`, `Sprint-2.md`, etc.)

## Repository Structure

```text
.
â”œâ”€â”€ airflow/                    # Airflow Orchestration (Astro Project)
â”‚   â”œâ”€â”€ dags/                   # DAGs for ELT, Gold transformations, and testing
â”‚   â”œâ”€â”€ include/                # Static assets and generated artifacts
â”‚   â”‚   â””â”€â”€ results/            # Outputs from Gold DAGs (PNG, HTML, MD)
â”‚   â”‚       â”œâ”€â”€ bq1/            # Mobility Patterns results
â”‚   â”‚       â”œâ”€â”€ bq2/            # Infrastructure Gaps results
â”‚   â”‚       â””â”€â”€ bq3/            # Functional Classification results
â”‚   â””â”€â”€ requirements.txt        # Airflow-specific dependencies
â”œâ”€â”€ app/                        # Visualization Layer
â”‚   â””â”€â”€ main.py                 # Streamlit application entry point
â”œâ”€â”€ data/                       # Local Data Storage (NOT tracked by Git)
â”‚   â”œâ”€â”€ raw/                    # Source files for experimentation
â”‚   â””â”€â”€ lakehouse/              # Local Bronze/Silver/Gold layers
â”œâ”€â”€ docs/                       # Project Documentation
â”œâ”€â”€ notebooks/                  # Jupyter Notebooks for logic prototyping
â”œâ”€â”€ .gitignore                  # Git exclusion rules
â”œâ”€â”€ requirements.txt            # Dependencies for Streamlit App
â””â”€â”€ README.md                   # Project overview
```

### Data Access Logic
*   **Airflow DAGs:** These are designed for automation. They fetch data directly from official sources via **HTTP requests** and process them into the cloud infrastructure.
*   **Notebooks:** These were used during the different **sprints** to prototype logic. They rely on the local `data/` folder and the root `requirements.notebooks.txt`.
*   **Streamlit Visualziation:** ``app/main.py`` is the entrypoint to visualize all the data generated by the DAGs. Results can also be found in ``airflow/include/results/bqX`` folder, being `X`the number of the use case.


## Setup & Installation

### 1. Airflow Orchestration (Astro)
The orchestration layer is managed via the **Astro CLI**.

1. **Prerequisites:** Ensure **Docker Desktop** and **Astro CLI** are installed and running.
2. Open a terminal and navigate to the `airflow/` directory:
   ```bash
   cd airflow
   ```
3. **Initialize the project** (only needed once):
   ```bash
   astro dev init
   ```
   *If prompted because the folder is not empty, type `y` and press enter to confirm (this preserves your existing DAGs).*
4. **Start the environment**:
   ```bash
   astro dev start
   ```
5. **Access the UI**: Once initialized, open your browser at `http://localhost:8080/`.
6. **Stop the services**: To shut down the containers without removing your work, run:
   ```bash
   astro dev stop
   ```
7. **Access the UI**: Open `http://localhost:8080/`.

### 2. Visualization Dashboard (Streamlit)
The dashboard provides a professional interface to explore the results generated by the Gold Layer DAGs.

1. **Install Dependencies**: From the **root** of the project, ensure your virtual environment is active and run:
   ```bash
   pip install -r requirements.txt
   ```
2. **Run the Dashboard**:
   ```bash
   streamlit run app/main.py
   ```
3. **Access the UI**: The dashboard will open automatically at `http://localhost:8501/`.

## Cloud Lakehouse Configuration

To enable cloud storage and elastic compute, configure the following in the Airflow UI (**Admin -> Connections**):

### Connection 1: AWS S3 (`aws_s3_conn`)
*   **Conn Type:** `Amazon Web Services`
*   **Login:** Your AWS Access Key ID
*   **Password:** Your AWS Secret Access Key
*   **Extra:** 
    ```json
    {
    "region_name": "eu-central-1",
    "bucket_name": "Your_bucket_name"
    }
    ```
*   **Note:** The IAM User associated with these keys must have the `batch:SubmitJob`, `batch:DescribeJobs`, and `iam:PassRole` permissions.

### Connection 2: Neon Postgres (`neon_catalog_conn`)
*   **Conn Type:** `Postgres`
*   **Host:** Your Neon hostname (e.g., `ep-winter-rain...aws.neon.tech`)
*   **Schema/Database:** `neondb`
*   **Login:** `neondb_owner`
*   **Password:** Your_Neon_Password
*   **Port:** `5432`

### ðŸš€ AWS Batch Infrastructure
Heavy SQL transformations are offloaded to **AWS Batch** using a custom `duckrunner` image to handle large datasets without local memory constraints.

## Verification

Before running main pipelines, execute these test DAGs:
1.  **`00_connection_test`**: Verifies local connectivity to AWS and Neon.
2.  **`0_aws_batch_test`**: Verifies the remote execution environment on AWS Batch.

## Data Pipelines (DAGs)

*   **Infrastructure & Dimensions:** Ingests static data (INE/MITMA dimensions) to build schema foundations.
*   **Mobility Ingestion:** Parameterized worker pipeline for cleaning and transforming high-volume daily mobility files. It exists 2 versions: `_local`, which executes all the logic locally, and `_batch`. which executes the same logic but in AWS batch.
*   **Gold Generations (DAGs 31â€“33):** Analytical pipelines that materialize aggregated Gold tables and generate visual assets (Kepler maps, PNGs) stored in `airflow/include/results/`. These files are automatically picked up by the Streamlit app.

---

## Visualization Dashboard
The Streamlit application is the primary consumption layer for business users. It features:
*   **Interactive Sidebar:** Navigation between the three Business Questions (BQs).
*   **Metadata Explorer:** Direct access to execution parameters (dates, polygons) extracted from the Airflow Markdown reports.
*   **Multi-format Viewing:** Seamless rendering of Matplotlib plots, Plotly heatmaps, and high-density Kepler.gl maps.
*   **Orchestration Shortcut:** A direct link button to the Airflow UI for real-time pipeline monitoring.

![alt text](docs/images/Results/bq1-1.png)

**Note:** The Streamlit app relies on the artifacts generated by the Gold DAGs. If a view appears empty, ensure the corresponding DAG has been executed in Airflow.