# üá™üá∏ 3-Tier Data Lakehouse for Spanish Mobility Analysis

## üìë Table of Contents
1. [Authors](#-authors)
2. [Repository Structure](#-repository-structure)
3. [Methodology](#-methodology)
4. [Setup & Installation](#-setup--installation)
5. [Visualization Dashboard](#-visualization-dashboard)
6. [Cloud Lakehouse Configuration](#Ô∏è-cloud-lakehouse-configuration)
7. [Verification](#-verification)
8. [Data Pipelines (DAGs)](#Ô∏è-data-pipelines-dags)

## üë• Authors

*   **Mar√≠a L√≥pez Hern√°ndez**
*   **Fernando Blanco Membrives**
*   **Joan S√°nchez Verd√∫**

**Date:** January 2026

This project implements a **3-tier Data Lakehouse** (Bronze, Silver, Gold) designed to process and analyze public mobility data from the Spanish Ministry of Transport (MITMA) and the National Statistics Institute (INE). 

The infrastructure follows a Medallion Architecture, utilizing **DuckDB** for processing, **AWS S3** for storage, and **Neon (Postgres)** for metadata management, with a **Streamlit** layer for interactive visualization.

## üìà Methodology

This project follows an **Agile** methodology. We work in iterative sprints with clear goals and continuous delivery. 

Detailed documentation for each stage of development‚Äîincluding sprint goals, task breakdowns, assignments, and retrospective outcomes‚Äîcan be found in the following directory:
`docs/sprint_logs/` (e.g., `Sprint-1.md`, `Sprint-2.md`, etc.)

## üìÇ Repository Structure

```text
.
‚îú‚îÄ‚îÄ airflow/                    # Airflow Orchestration (Astro Project)
‚îÇ   ‚îú‚îÄ‚îÄ dags/                   # DAGs for ELT, Gold transformations, and testing
‚îÇ   ‚îú‚îÄ‚îÄ include/                # Static assets and generated artifacts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results/            # Outputs from Gold DAGs (PNG, HTML, MD)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bq1/            # Mobility Patterns results
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bq2/            # Infrastructure Gaps results
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ bq3/            # Functional Classification results
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt        # Airflow-specific dependencies
‚îú‚îÄ‚îÄ app/                        # Visualization Layer
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # Streamlit application entry point
‚îú‚îÄ‚îÄ data/                       # Local Data Storage (NOT tracked by Git)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Source files for experimentation
‚îÇ   ‚îî‚îÄ‚îÄ lakehouse/              # Local Bronze/Silver/Gold layers
‚îú‚îÄ‚îÄ docs/                       # Project Documentation
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter Notebooks for logic prototyping
‚îú‚îÄ‚îÄ .gitignore                  # Git exclusion rules
‚îú‚îÄ‚îÄ requirements.txt            # Dependencies for Notebooks and Streamlit App
‚îî‚îÄ‚îÄ README.md                   # Project overview
```

### üí° Data Access Logic
*   **Airflow DAGs:** These are designed for automation. They fetch data directly from official sources via **HTTP requests** and process them into the cloud infrastructure.
*   **Notebooks:** These were used during the different **sprints** to prototype logic. They rely on the local `data/` folder and the root `requirements.notebooks.txt`.
*   **Streamlit Visualziation:** ``app/main.py`` is the entrypoint to visualize all the data generated by the DAGs. Results can also be found in ``airflow/include/results/bqX`` folder, being `X`the number of the use case.


## üöÄ Setup & Installation

### 1. Airflow Orchestration (Astro)
The orchestration layer is managed via the **Astro CLI**.

1. **Prerequisites:** Ensure **Docker Desktop** and **Astro CLI** are installed and running.
2. Open a terminal and navigate to the `airflow/` directory:
   ```bash
   cd airflow
   ```
3. **Initialize the project** (only needed once):
   ```bash
   astro dev init
   ```
   *If prompted because the folder is not empty, type `y` and press enter to confirm (this preserves your existing DAGs).*
4. **Start the environment**:
   ```bash
   astro dev start
   ```
5. **Access the UI**: Once initialized, open your browser at `http://localhost:8080/`.
6. **Stop the services**: To shut down the containers without removing your work, run:
   ```bash
   astro dev stop
   ```
7. **Access the UI**: Open `http://localhost:8080/`.

### 2. Visualization Dashboard (Streamlit)
The dashboard provides a professional interface to explore the results generated by the Gold Layer DAGs.

1. **Install Dependencies**: From the **root** of the project, ensure your virtual environment is active and run:
   ```bash
   pip install -r requirements.txt
   ```
2. **Run the Dashboard**:
   ```bash
   streamlit run app/main.py
   ```
3. **Access the UI**: The dashboard will open automatically at `http://localhost:8501/`.

## ‚òÅÔ∏è Cloud Lakehouse Configuration

To enable cloud storage and elastic compute, configure the following in the Airflow UI (**Admin -> Connections**):

### Connection 1: AWS S3 (`aws_s3_conn`)
*   **Conn Type:** `Amazon Web Services`
*   **Login:** Your AWS Access Key ID
*   **Password:** Your AWS Secret Access Key
*   **Extra:** 
    ```json
    {
    "region_name": "eu-central-1",
    "bucket_name": "Your_bucket_name"
    }
    ```
*   **Note:** The IAM User associated with these keys must have the `batch:SubmitJob`, `batch:DescribeJobs`, and `iam:PassRole` permissions.

### Connection 2: Neon Postgres (`neon_catalog_conn`)
*   **Conn Type:** `Postgres`
*   **Host:** Your Neon hostname (e.g., `ep-winter-rain...aws.neon.tech`)
*   **Schema/Database:** `neondb`
*   **Login:** `neondb_owner`
*   **Password:** Your_Neon_Password
*   **Port:** `5432`

### üöÄ AWS Batch Infrastructure
Heavy SQL transformations are offloaded to **AWS Batch** using a custom `duckrunner` image to handle large datasets without local memory constraints.

## üß™ Verification

Before running main pipelines, execute these test DAGs:
1.  **`00_connection_test`**: Verifies local connectivity to AWS and Neon.
2.  **`0_aws_batch_test`**: Verifies the remote execution environment on AWS Batch.

## ‚öôÔ∏è Data Pipelines (DAGs)

*   **Infrastructure & Dimensions:** Ingests static data (INE/MITMA dimensions) to build schema foundations.
*   **Mobility Ingestion:** Parameterized worker pipeline for cleaning and transforming high-volume daily mobility files.
*   **Gold Generations (DAGs 31‚Äì33):** Analytical pipelines that materialize aggregated Gold tables and generate visual assets (Kepler maps, PNGs) stored in `airflow/include/results/`. These files are automatically picked up by the Streamlit app.

---

## üìä Visualization Dashboard
The Streamlit application is the primary consumption layer for business users. It features:
*   **Interactive Sidebar:** Navigation between the three Business Questions (BQs).
*   **Metadata Explorer:** Direct access to execution parameters (dates, polygons) extracted from the Airflow Markdown reports.
*   **Multi-format Viewing:** Seamless rendering of Matplotlib plots, Plotly heatmaps, and high-density Kepler.gl maps.
*   **Orchestration Shortcut:** A direct link button to the Airflow UI for real-time pipeline monitoring.

**Note:** The Streamlit app relies on the artifacts generated by the Gold DAGs. If a view appears empty, ensure the corresponding DAG has been executed in Airflow.