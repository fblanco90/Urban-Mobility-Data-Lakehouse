{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba21534a",
   "metadata": {},
   "source": [
    "### Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6f6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 1. CONFIGURATION & CONSTANTS ---\n",
    "RAW_DATA_PATH = '../data/raw'\n",
    "LAKEHOUSE_PATH = '../data/lakehouse'\n",
    "METADATA_PATH = os.path.join(LAKEHOUSE_PATH, 'metadata.duckdb')\n",
    "\n",
    "# Official Data Source URLs\n",
    "URL_MITMA = \"https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad\"\n",
    "URL_INE = \"https://www.ine.es/\"\n",
    "URL_CNIG = \"https://centrodedescargas.cnig.es/CentroDescargas/index.jsp\"\n",
    "URL_MTDFP = \"https://datos.gob.es/es/catalogo/l01280796-calendario-laboral\"\n",
    "\n",
    "# --- 2. INFRASTRUCTURE FUNCTION ---\n",
    "\n",
    "def init_lakehouse():\n",
    "    \"\"\"\n",
    "    Initializes the DuckDB connection, loads the DuckLake extension, \n",
    "    attaches the persistent catalog, and ensures logical schemas exist.\n",
    "    \"\"\"\n",
    "    print(\"--- ðŸ”Œ Initializing Lakehouse Infrastructure ---\")\n",
    "    \n",
    "    # Ensure physical directory exists\n",
    "    os.makedirs(LAKEHOUSE_PATH, exist_ok=True)\n",
    "    \n",
    "    # Create an in-memory compute session\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    \n",
    "    # 1. Install & Load DuckLake Extension\n",
    "    try:\n",
    "        con.execute(\"INSTALL ducklake;\")\n",
    "        con.execute(\"LOAD ducklake;\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading 'ducklake': {e}\")\n",
    "        raise e\n",
    "\n",
    "    # 2. Attach the Catalog (Metadata Layer)\n",
    "    # This separates compute (RAM) from storage (Disk)\n",
    "    con.execute(f\"ATTACH 'ducklake:{METADATA_PATH}' AS lakehouse\")\n",
    "    \n",
    "    # 3. Ensure Logical Schemas Exist\n",
    "    for schema in ['bronze', 'silver', 'gold']:\n",
    "        con.execute(f\"CREATE SCHEMA IF NOT EXISTS lakehouse.{schema}\")\n",
    "        \n",
    "    print(f\"âœ… Lakehouse catalog attached at: {METADATA_PATH}\")\n",
    "    return con\n",
    "\n",
    "# --- 3. INGESTION FUNCTIONS ---\n",
    "\n",
    "def ingest_mobility_bronze(con):\n",
    "    \"\"\"\n",
    "    Ingests the massive Mobility Fact Table using a Partitioning Strategy (by Date).\n",
    "    Pattern: Create Structure -> Configure Partitioning -> Insert Data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸšš Ingesting Mobility Data (Fact Table) ---\")\n",
    "    \n",
    "    # Locate all compressed CSV files for municipalities\n",
    "    mitma_files = glob.glob(os.path.join(RAW_DATA_PATH, 'mitma', '*_Viajes_municipios.csv.gz'))\n",
    "    \n",
    "    if not mitma_files:\n",
    "        print(\"âŒ No mobility files found!\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Create empty table structure (LIMIT 0)\n",
    "    # This defines the schema without processing all data yet.\n",
    "    print(\"-> Creating table structure...\")\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE lakehouse.bronze.mobility_sample_week AS\n",
    "        SELECT \n",
    "            *,\n",
    "            CURRENT_TIMESTAMP AS ingestion_timestamp,\n",
    "            '{URL_MITMA}' AS source_url\n",
    "        FROM read_csv_auto({mitma_files}, filename=true, all_varchar=true)\n",
    "        LIMIT 0;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Step 2: Configure Partitioning in DuckLake\n",
    "    # This ensures data is physically organized by folders (e.g., /fecha=20230101/)\n",
    "    print(\"-> Configuring partition strategy (by date)...\")\n",
    "    con.execute(\"\"\"\n",
    "        ALTER TABLE lakehouse.bronze.mobility_sample_week \n",
    "        SET PARTITIONED BY (fecha);\n",
    "    \"\"\")\n",
    "    \n",
    "    # Step 3: Insert Massive Data\n",
    "    print(\"-> Inserting data (this may take a while)...\")\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO lakehouse.bronze.mobility_sample_week \n",
    "        SELECT \n",
    "            *,\n",
    "            CURRENT_TIMESTAMP AS ingestion_timestamp,\n",
    "            '{URL_MITMA}' AS source_url\n",
    "        FROM read_csv_auto({mitma_files}, filename=true, all_varchar=true);\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"âœ… Successfully ingested: lakehouse.bronze.mobility_sample_week\")\n",
    "\n",
    "def ingest_dimensions_bronze(con):\n",
    "    \"\"\"\n",
    "    Ingests all auxiliary dictionary tables using a helper function \n",
    "    to handle different CSV formats (separators, encoding).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ“š Ingesting Dimension Tables ---\")\n",
    "\n",
    "    # Inner helper function to avoid code repetition\n",
    "    def ingest_one(table, filename, url, folder='mitma', sep=';', encoding='utf-8', **kwargs):\n",
    "        path = os.path.join(RAW_DATA_PATH, folder, filename)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"âš ï¸ Missing file: {filename}\")\n",
    "            return\n",
    "\n",
    "        # Construct dynamic options for read_csv_auto\n",
    "        options = [f\"all_varchar=true\", f\"sep='{sep}'\", f\"encoding='{encoding}'\"]\n",
    "        for k, v in kwargs.items():\n",
    "            # Handle boolean SQL syntax conversion (True -> 'true')\n",
    "            val = str(v).lower() if isinstance(v, bool) else f\"'{v}'\"\n",
    "            options.append(f\"{k}={val}\")\n",
    "        \n",
    "        options_str = \", \".join(options)\n",
    "        \n",
    "        # Escape single quotes in URL for SQL safety\n",
    "        safe_url = url.replace(\"'\", \"''\")\n",
    "\n",
    "        # Execute CTAS (Create Table As Select)\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE lakehouse.bronze.{table} AS\n",
    "            SELECT \n",
    "                *,\n",
    "                CURRENT_TIMESTAMP AS ingestion_timestamp,\n",
    "                '{safe_url}' AS source_url\n",
    "            FROM read_csv_auto('{path}', {options_str});\n",
    "        \"\"\")\n",
    "        print(f\"âœ… Ingested: lakehouse.bronze.{table}\")\n",
    "\n",
    "    # --- INGESTION LIST ---\n",
    "    \n",
    "    # 1. MITMA Files (Using Pipe '|' separator)\n",
    "    ingest_one('zoning_municipalities', 'nombres_municipios.csv', URL_MITMA, sep='|', header=True)\n",
    "    ingest_one('population_municipalities', 'poblacion_municipios.csv', URL_MITMA, sep='|')\n",
    "    ingest_one('mapping_ine_mitma', 'relacion_ine_zonificacionMitma.csv', URL_MITMA, sep='|')\n",
    "    \n",
    "    # 2. INE / CNIG Files (Using Semicolon ';' separator)\n",
    "    ingest_one('ine_rent_municipalities', 'ine_renta.csv', URL_INE, folder='ine', sep=';')\n",
    "    ingest_one('municipal_coordinates', 'municipios_coordenadas.csv', URL_CNIG, folder='ine', sep=';')\n",
    "    ingest_one('work_calendars', 'calendario.csv', URL_MTDFP, folder='ine', sep=';')\n",
    "\n",
    "# --- 4. VALIDATION FUNCTION ---\n",
    "\n",
    "def validate_bronze(con):\n",
    "    \"\"\"\n",
    "    Performs a quick check to verify that tables exist in the Bronze schema.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ” Checking Bronze Layer Status ---\")\n",
    "    # We query the internal system metadata to see what tables are registered\n",
    "    res = con.execute(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM duckdb_tables() \n",
    "        WHERE schema_name = 'bronze'\n",
    "    \"\"\").df()\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8af1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. MAIN EXECUTION PIPELINE ---\n",
    "\n",
    "def run_bronze_pipeline():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire Bronze Layer process.\n",
    "    \"\"\"\n",
    "    con = init_lakehouse()          # 1. Initialize Infrastructure\n",
    "    ingest_mobility_bronze(con)     # 2. Load Mobility Data (Partitioned)\n",
    "    ingest_dimensions_bronze(con)   # 3. Load Dimension Tables\n",
    "    validate_bronze(con)            # 4. Final Validation\n",
    "    return con\n",
    "\n",
    "# Execute only if running as a script/notebook\n",
    "if __name__ == \"__main__\":\n",
    "    con = run_bronze_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24972ecb",
   "metadata": {},
   "source": [
    "### Silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c400cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SILVER LAYER FUNCTIONS ---\n",
    "\n",
    "def build_silver_dim_zones(con):\n",
    "    \"\"\"\n",
    "    Construye la dimensiÃ³n de zonas (dim_zones) limpiando y deduplicando.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: dim_zones ---\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.dim_zones AS\n",
    "    WITH unique_mapping AS (\n",
    "        SELECT DISTINCT \n",
    "            CAST(municipio_mitma AS VARCHAR) as mitma_ref,\n",
    "            MIN(CAST(municipio_ine AS VARCHAR)) as ine_ref\n",
    "        FROM lakehouse.bronze.mapping_ine_mitma\n",
    "        WHERE municipio_mitma IS NOT NULL\n",
    "        GROUP BY municipio_mitma\n",
    "    ),\n",
    "    raw_zones AS (\n",
    "        SELECT \n",
    "            TRIM(z.ID) AS mitma_code,\n",
    "            CASE \n",
    "                WHEN TRIM(m.ine_ref) = 'NA' THEN NULL \n",
    "                ELSE TRIM(m.ine_ref) \n",
    "            END AS ine_code,\n",
    "            TRIM(z.name) AS zone_name\n",
    "        FROM lakehouse.bronze.zoning_municipalities z\n",
    "        LEFT JOIN unique_mapping m \n",
    "            ON TRIM(z.ID) = TRIM(m.mitma_ref)\n",
    "        WHERE z.ID IS NOT NULL AND z.ID != 'ID'\n",
    "        GROUP BY z.ID, z.name, m.ine_ref\n",
    "    )\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY mitma_code) AS zone_id,\n",
    "        mitma_code,\n",
    "        ine_code,\n",
    "        zone_name,\n",
    "        'municipal' AS zone_level,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM raw_zones\n",
    "    ORDER BY zone_id;\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(\"âœ… Created: lakehouse.silver.dim_zones\")\n",
    "\n",
    "def build_silver_metric_population(con):\n",
    "    \"\"\"\n",
    "    Limpia y estructura los datos de poblaciÃ³n.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: metric_population ---\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.metric_population AS\n",
    "    SELECT \n",
    "        z.zone_id,\n",
    "        CAST(TRY_CAST(column1 AS DOUBLE) AS BIGINT) AS population,\n",
    "        '2023' AS year,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.bronze.population_municipalities p\n",
    "    JOIN lakehouse.silver.dim_zones z ON TRIM(p.column0) = z.mitma_code\n",
    "    WHERE column0 IS NOT NULL \n",
    "      AND NOT regexp_matches(column1, '[a-zA-Z]')\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(\"âœ… Created: lakehouse.silver.metric_population\")\n",
    "\n",
    "def build_silver_dim_coordinates(con):\n",
    "    \"\"\"\n",
    "    Limpia las coordenadas geogrÃ¡ficas.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: dim_coordinates ---\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.dim_coordinates AS\n",
    "    SELECT \n",
    "        z.zone_id,\n",
    "        TRY_CAST(REPLACE(c.LATITUD_ETRS89, ',', '.') AS DOUBLE) AS latitude,\n",
    "        TRY_CAST(REPLACE(c.LONGITUD_ETRS89, ',', '.') AS DOUBLE) AS longitude,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.bronze.municipal_coordinates c\n",
    "    JOIN lakehouse.silver.dim_zones z \n",
    "        ON LEFT(c.COD_INE, 5) = z.ine_code \n",
    "    WHERE z.zone_id IS NOT NULL;\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(\"âœ… Created: lakehouse.silver.dim_coordinates\")\n",
    "\n",
    "def build_silver_metric_ine_rent(con):\n",
    "    \"\"\"\n",
    "    Limpia los datos de renta media del INE.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: metric_ine_rent ---\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.metric_ine_rent AS\n",
    "    SELECT \n",
    "        z.zone_id,\n",
    "        CAST(TRY_CAST(REPLACE(r.Total, '.', '') AS DOUBLE) AS BIGINT) AS income_per_capita,\n",
    "        CAST(r.Periodo AS INTEGER) AS year,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.bronze.ine_rent_municipalities r\n",
    "    JOIN lakehouse.silver.dim_zones z \n",
    "        ON split_part(r.Municipios, ' ', 1) = z.ine_code\n",
    "    WHERE r.\"Indicadores de renta media\" = 'Renta neta media por persona'\n",
    "      AND (r.Distritos IS NULL OR r.Distritos = '')\n",
    "      AND (r.Secciones IS NULL OR r.Secciones = '')\n",
    "      AND r.Total IS NOT NULL\n",
    "      AND z.zone_id IS NOT NULL;\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(\"âœ… Created: lakehouse.silver.metric_ine_rent\")\n",
    "\n",
    "def build_silver_fact_mobility(con):\n",
    "    \"\"\"\n",
    "    Creating the mobility fact table (Fact Table).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: fact_mobility ---\")\n",
    "    \n",
    "    # 1. Create Schema\n",
    "    schema_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.fact_mobility (\n",
    "        date DATE,\n",
    "        hour INTEGER,\n",
    "        origin_zone_id BIGINT,\n",
    "        destination_zone_id BIGINT,\n",
    "        trips DOUBLE,\n",
    "        trips_km DOUBLE,\n",
    "        processed_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    con.execute(schema_query)\n",
    "    \n",
    "    # 2. Insert Data (Sorted for performance instead of PARTITIONED BY)\n",
    "    print(\"-> Inserting data...\")\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO lakehouse.silver.fact_mobility\n",
    "    SELECT \n",
    "        strptime(fecha, '%Y%m%d') AS date,\n",
    "        CAST(periodo AS INTEGER) AS hour,\n",
    "        zo.zone_id AS origin_zone_id,\n",
    "        zd.zone_id AS destination_zone_id,\n",
    "        TRY_CAST(REPLACE(REPLACE(viajes, '.', ''), ',', '.') AS DOUBLE) AS trips,\n",
    "        TRY_CAST(REPLACE(REPLACE(viajes_km, '.', ''), ',', '.') AS DOUBLE) AS trips_km,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.bronze.mobility_sample_week m\n",
    "    LEFT JOIN lakehouse.silver.dim_zones zo ON TRIM(m.origen) = zo.mitma_code\n",
    "    LEFT JOIN lakehouse.silver.dim_zones zd ON TRIM(m.destino) = zd.mitma_code\n",
    "    WHERE viajes IS NOT NULL\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    con.execute(insert_query)\n",
    "    print(\"âœ… Created: lakehouse.silver.fact_mobility\")\n",
    "\n",
    "def build_silver_dim_festive_types(con):\n",
    "    \"\"\"\n",
    "    Normaliza los tipos de festivos.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: dim_festive_types ---\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.dim_festive_types AS\n",
    "    SELECT DISTINCT\n",
    "        CASE \n",
    "            WHEN \"Tipo de Festivo\" ILIKE '%festivo nacional%' OR \"Tipo de Festivo\" ILIKE '%fiesta nacional%'\n",
    "            THEN 'NationalFestive'\n",
    "            ELSE \"Tipo de Festivo\"\n",
    "        END AS festive_type,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.bronze.work_calendars\n",
    "    WHERE \"Tipo de Festivo\" IS NOT NULL\n",
    "      AND (\"Tipo de Festivo\" ILIKE '%festivo nacional%' OR \"Tipo de Festivo\" ILIKE '%fiesta nacional%');\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(\"âœ… Created: lakehouse.silver.dim_festive_types\")\n",
    "\n",
    "def build_silver_bridge_zones_festives(con):\n",
    "    \"\"\"\n",
    "    Crea el puente entre zonas y dÃ­as festivos.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Silver Table: bridge_zones_festives ---\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.bridge_zones_festives AS\n",
    "    SELECT z.zone_id,\n",
    "        CAST(strptime(c.\"Dia\", '%d/%m/%Y') AS DATE) AS festive_date,\n",
    "        ft.festive_type,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.silver.dim_zones AS z\n",
    "    CROSS JOIN lakehouse.bronze.work_calendars AS c\n",
    "    JOIN lakehouse.silver.dim_festive_types AS ft\n",
    "    ON ft.festive_type =\n",
    "        CASE WHEN c.\"Tipo de Festivo\" ILIKE '%festivo nacional%' OR c.\"Tipo de Festivo\" ILIKE '%fiesta nacional%'\n",
    "            THEN 'NationalFestive'\n",
    "            ELSE c.\"Tipo de Festivo\"\n",
    "        END;\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(\"âœ… Created: lakehouse.silver.bridge_zones_festives\")\n",
    "\n",
    "def validate_silver_layer(con):\n",
    "    \"\"\"\n",
    "    Verifica que las tablas se han creado.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ” Checking Silver Layer Status ---\")\n",
    "    res = con.execute(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM duckdb_tables() \n",
    "        WHERE database_name = 'lakehouse' AND schema_name = 'silver';\n",
    "    \"\"\").df()\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99d865",
   "metadata": {},
   "source": [
    "#### Orchestrate Bronze Layer and Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29cda30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING DUCKLAKE PIPELINE ðŸš€\n",
      "============================================================\n",
      "--- ðŸ”Œ Initializing Lakehouse Infrastructure ---\n",
      "âœ… Lakehouse catalog attached at: ../data/lakehouse\\metadata.duckdb\n",
      "\n",
      "==================== STAGE 1: BRONZE LAYER ====================\n",
      "\n",
      "--- ðŸšš Ingesting Mobility Data (Fact Table) ---\n",
      "-> Creating table structure...\n",
      "-> Configuring partition strategy (by date)...\n",
      "-> Inserting data (this may take a while)...\n",
      "âœ… Successfully ingested: lakehouse.bronze.mobility_sample_week\n",
      "\n",
      "--- ðŸ“š Ingesting Dimension Tables ---\n",
      "âœ… Ingested: lakehouse.bronze.zoning_municipalities\n",
      "âœ… Ingested: lakehouse.bronze.population_municipalities\n",
      "âœ… Ingested: lakehouse.bronze.mapping_ine_mitma\n",
      "âœ… Ingested: lakehouse.bronze.ine_rent_municipalities\n",
      "âœ… Ingested: lakehouse.bronze.municipal_coordinates\n",
      "âœ… Ingested: lakehouse.bronze.work_calendars\n",
      "\n",
      "--- ðŸ” Checking Bronze Layer Status ---\n",
      "                  table_name\n",
      "0    ine_rent_municipalities\n",
      "1      zoning_municipalities\n",
      "2       mobility_sample_week\n",
      "3  population_municipalities\n",
      "4      municipal_coordinates\n",
      "5          mapping_ine_mitma\n",
      "6             work_calendars\n",
      "\n",
      "==================== STAGE 2: SILVER LAYER ====================\n",
      "\n",
      "--- Building Silver Table: dim_zones ---\n",
      "âœ… Created: lakehouse.silver.dim_zones\n",
      "\n",
      "--- Building Silver Table: metric_population ---\n",
      "âœ… Created: lakehouse.silver.metric_population\n",
      "\n",
      "--- Building Silver Table: metric_ine_rent ---\n",
      "âœ… Created: lakehouse.silver.metric_ine_rent\n",
      "\n",
      "--- Building Silver Table: dim_coordinates ---\n",
      "âœ… Created: lakehouse.silver.dim_coordinates\n",
      "\n",
      "--- Building Silver Table: dim_festive_types ---\n",
      "âœ… Created: lakehouse.silver.dim_festive_types\n",
      "\n",
      "--- Building Silver Table: bridge_zones_festives ---\n",
      "âœ… Created: lakehouse.silver.bridge_zones_festives\n",
      "\n",
      "--- Building Silver Table: fact_mobility ---\n",
      "-> Inserting data...\n",
      "âœ… Created: lakehouse.silver.fact_mobility\n",
      "\n",
      "--- ðŸ” Checking Silver Layer Status ---\n",
      "              table_name\n",
      "0          fact_mobility\n",
      "1              dim_zones\n",
      "2  bridge_zones_festives\n",
      "3      dim_festive_types\n",
      "4      metric_population\n",
      "5        metric_ine_rent\n",
      "6        dim_coordinates\n",
      "\n",
      "============================================================\n",
      "âœ… PIPELINE FINISHED SUCCESSFULLY in 126.10 seconds.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_full_data_pipeline():\n",
    "    \"\"\"\n",
    "    MASTER PIPELINE: Orchestrates the full End-to-End ETL process (Bronze -> Silver).\n",
    "    This function simulates a DAG run (e.g., Airflow) by executing tasks in dependency order.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"ðŸš€ STARTING DUCKLAKE PIPELINE ðŸš€\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- STEP 0: INFRASTRUCTURE INITIALIZATION ---\n",
    "    # We initialize the DuckDB session, load extensions, and attach the Catalog.\n",
    "    con = init_lakehouse()\n",
    "\n",
    "    # --- STEP 1: BRONZE LAYER (Raw Ingestion) ---\n",
    "    print(\"\\n\" + \"=\"*20 + \" STAGE 1: BRONZE LAYER \" + \"=\"*20)\n",
    "    \n",
    "    # 1.1 Ingest Massive Data (Fact Table)\n",
    "    # This handles the partitioning and bulk load of mobility data.\n",
    "    ingest_mobility_bronze(con)\n",
    "    \n",
    "    # 1.2 Ingest Reference Data (Dimensions)\n",
    "    # Loads dictionaries (INE, MITMA, Calendars, Coordinates).\n",
    "    ingest_dimensions_bronze(con)\n",
    "    \n",
    "    # 1.3 Validation\n",
    "    validate_bronze(con)\n",
    "\n",
    "    # --- STEP 2: SILVER LAYER (Cleaning & Star Schema) ---\n",
    "    print(\"\\n\" + \"=\"*20 + \" STAGE 2: SILVER LAYER \" + \"=\"*20)\n",
    "    \n",
    "    # 2.1 Core Dimension (CRITICAL DEPENDENCY)\n",
    "    # We MUST build 'dim_zones' first because all other tables JOIN with it.\n",
    "    build_silver_dim_zones(con)\n",
    "    \n",
    "    # 2.2 Enriched Dimensions & Metrics\n",
    "    # These tables depend on 'dim_zones' but can run in parallel with each other.\n",
    "    build_silver_metric_population(con)\n",
    "    build_silver_metric_ine_rent(con)\n",
    "    build_silver_dim_coordinates(con)\n",
    "    build_silver_dim_festive_types(con)\n",
    "    \n",
    "    # 2.3 Bridge Tables & Fact Tables\n",
    "    # These run last because they rely on the dimensions created above.\n",
    "    build_silver_bridge_zones_festives(con)\n",
    "    build_silver_fact_mobility(con)\n",
    "    \n",
    "    # 2.4 Validation\n",
    "    validate_silver_layer(con)\n",
    "\n",
    "    # --- FINAL SUMMARY ---\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"âœ… PIPELINE FINISHED SUCCESSFULLY in {elapsed:.2f} seconds.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # We return the active connection so you can query the results interactively\n",
    "    return con\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline and store the connection in 'con'\n",
    "    con = run_full_data_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
