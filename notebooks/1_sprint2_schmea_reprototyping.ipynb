{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51af566",
   "metadata": {},
   "source": [
    "# Sprint 2: Schema Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4fff9",
   "metadata": {},
   "source": [
    "#### Initial configuration and conection to duckdb and ducklake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d48e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 1. Configuration & Paths ---\n",
    "RAW_DATA_PATH = '../data/raw/'\n",
    "LAKEHOUSE_PATH = '../data/lakehouse'\n",
    "METADATA_PATH = os.path.join(LAKEHOUSE_PATH, 'metadata.duckdb')\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "os.makedirs(LAKEHOUSE_PATH, exist_ok=True)\n",
    "\n",
    "# --- 2. Initialize DuckDB & Load DuckLake Extension ---\n",
    "# Connect to in-memory DuckDB (Compute Layer)\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "print(\"--- Initializing DuckLake Extension ---\")\n",
    "\n",
    "# ‚úÖ ACTIVATE DUCKLAKE: This downloads/installs the extension if missing\n",
    "# and loads it into the current session.\n",
    "try:\n",
    "    con.execute(\"INSTALL ducklake;\")\n",
    "    con.execute(\"LOAD ducklake;\")\n",
    "    print(\"‚úÖ Extension 'ducklake' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading 'ducklake'. Make sure the extension is available in your environment.\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf475ba",
   "metadata": {},
   "source": [
    "#### Attach the Catalog and Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Attach the Catalog ---\n",
    "# We attach the persistent storage. \n",
    "# Note: Depending on your specific DuckLake version, the syntax for ATTACH might vary slightly.\n",
    "# This assumes standard syntax where we point to the metadata file.\n",
    "con.execute(f\"ATTACH 'ducklake:{METADATA_PATH}' AS lakehouse\")\n",
    "print(f\"‚úÖ Lakehouse catalog attached at: {METADATA_PATH}\")\n",
    "\n",
    "# --- 4. Schema Management ---\n",
    "# Create logical schemas within the managed catalog\n",
    "schemas = ['bronze', 'silver', 'gold']\n",
    "for schema in schemas:\n",
    "    con.execute(f\"CREATE SCHEMA IF NOT EXISTS lakehouse.{schema}\")\n",
    "print(f\"‚úÖ Schemas ready: {', '.join(schemas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a57f38",
   "metadata": {},
   "source": [
    "---\n",
    "# Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c502f1b",
   "metadata": {},
   "source": [
    "### Create table of mobility without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Ingestion: Mobility Data (Bronze Layer) ---\n",
    "mitma_raw_glob_path = os.path.join(RAW_DATA_PATH, 'mitma', '*_Viajes_municipios.csv.gz')\n",
    "mobility_files = glob.glob(mitma_raw_glob_path)\n",
    "\n",
    "print(f\"\\n--- Ingesting Mobility Data ---\")\n",
    "if not mobility_files:\n",
    "    print(\"‚ùå No mobility files found!\")\n",
    "else:\n",
    "    print(f\"-> Found {len(mobility_files)} files.\")\n",
    "    \n",
    "    # Use CREATE TABLE to let DuckLake manage the data\n",
    "    # This creates a transaction, writes the Parquet files, and updates metadata.\n",
    "    query_mobility = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE lakehouse.bronze.mobility_sample_week \n",
    "        AS\n",
    "        SELECT \n",
    "            *,\n",
    "            CURRENT_TIMESTAMP AS ingestion_timestamp,\n",
    "            'https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad' AS source_url\n",
    "        FROM read_csv_auto({mobility_files}, filename=true, all_varchar=true)\n",
    "        LIMIT 0;\n",
    "    \"\"\"\n",
    "    con.execute(query_mobility)\n",
    "    print(f\"‚úÖ Table created: lakehouse.bronze.mobility_sample_week \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906efe23",
   "metadata": {},
   "source": [
    "#### Creating partition for mobility date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc287ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(f\"\"\"\n",
    "        ALTER TABLE lakehouse.bronze.mobility_sample_week  \n",
    "        SET PARTITIONED BY (fecha);\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82bba3f",
   "metadata": {},
   "source": [
    "#### Inserting the data into the partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mobility = f\"\"\"\n",
    "    INSERT INTO lakehouse.bronze.mobility_sample_week \n",
    "    SELECT \n",
    "        *,\n",
    "        CURRENT_TIMESTAMP AS ingestion_timestamp,\n",
    "        'https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad' AS source_url\n",
    "    FROM read_csv_auto({mobility_files}, filename=true, all_varchar=true, ignore_errors=true);\n",
    "\"\"\"\n",
    "con.execute(query_mobility)\n",
    "print(f\"‚úÖ Transformed & Ingested: lakehouse.bronze.mobility_sample_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d0800",
   "metadata": {},
   "source": [
    "#### Data and Schema Preview of Mobility Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86774fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSPECTION & METADATA CHECK ---\n",
    "print(\"\\n--- üîç INSPECTION: Mobility Table ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Verify that 'origen' and 'destino' columns look like municipal codes (5 digits)\n",
    "print(\"\\n[1] Data Preview (First 5 rows):\")\n",
    "con.execute(\"SELECT * FROM lakehouse.bronze.mobility_sample_week  LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e86106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Schema Check\n",
    "# Confirm column names and ensure types are currently VARCHAR (as expected for Bronze)\n",
    "print(\"\\n[2] Schema (Columns & Types):\")\n",
    "con.execute(\"DESCRIBE lakehouse.bronze.mobility_sample_week \").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(\"SUMMARIZE lakehouse.bronze.mobility_sample_week \").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c0dec",
   "metadata": {},
   "source": [
    "### Ingesting other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Ingestion: Auxiliary Tables (Refactorizado con Linaje) ---\n",
    "\n",
    "# Define URLs \n",
    "URL_MITMA = \"https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad\"\n",
    "URL_INE = \"https://www.ine.es/\"\n",
    "URL_CNIG = \"https://centrodedescargas.cnig.es/CentroDescargas/index.jsp\"\n",
    "URL_MTDFP = \"https://datos.gob.es/es/catalogo/l01280796-calendario-laboral\"\n",
    "\n",
    "def ingest_dimension(table_name, filename, source_url, folder='mitma', sep=';', encoding='utf-8', **kwargs):\n",
    "    path = os.path.join(RAW_DATA_PATH, folder, filename)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        # Escape single quotes in the URL for SQL safety\n",
    "        safe_url = source_url.replace(\"'\", \"''\")\n",
    "\n",
    "        options = f\"filename=true, all_varchar=true, sep='{sep}', encoding='{encoding}'\"\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            # Handle boolean SQL syntax (true/false instead of True/False)\n",
    "            if isinstance(value, bool):\n",
    "                sql_val = str(value).lower()\n",
    "            else:\n",
    "                sql_val = f\"'{value}'\"\n",
    "            options += f\", {key}={sql_val}\"\n",
    "            \n",
    "        print(f\"-> Ingesting {table_name} with options: [{options}]\")\n",
    "        \n",
    "        # Read the CSV and append audit columns\n",
    "        # We use robust typing (try_cast/all_varchar) to prevent failures if any numeric column contains irregular characters\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE lakehouse.bronze.{table_name} AS\n",
    "            SELECT \n",
    "                *,\n",
    "                CURRENT_TIMESTAMP AS ingestion_timestamp,\n",
    "                '{safe_url}' AS source_url\n",
    "            FROM read_csv_auto('{path}', {options});\n",
    "        \"\"\")\n",
    "        print(f\"‚úÖ Ingested: lakehouse.bronze.{table_name} (Source: {source_url})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing file: {filename}\")\n",
    "\n",
    "print(\"\\n--- Ingesting Dictionaries & Dimensions ---\")\n",
    "\n",
    "# 1. Nombres de Distritos (MITMA)\n",
    "# Fuente: Open Data Movilidad\n",
    "ingest_dimension('zoning_municipalities', 'nombres_municipios.csv', source_url=URL_MITMA, folder='mitma', sep='|', header=True)\n",
    "\n",
    "# 2. Poblaci√≥n por Distrito (MITMA)\n",
    "# Fuente: Open Data Movilidad\n",
    "ingest_dimension('population_municipalities', 'poblacion_municipios.csv', source_url=URL_MITMA, folder='mitma', sep='|')\n",
    "\n",
    "# 3. Relaci√≥n Zonificaci√≥n MITMA <-> INE\n",
    "# Fuente: Open Data Movilidad\n",
    "ingest_dimension('mapping_ine_mitma', 'relacion_ine_zonificacionMitma.csv', source_url=URL_MITMA, folder='mitma', sep='|')\n",
    "\n",
    "# 4. Renta Media (INE)\n",
    "# Fuente: Instituto Nacional de Estad√≠stica\n",
    "ingest_dimension('ine_rent_municipalities', 'ine_renta.csv', source_url=URL_INE, folder='ine', sep=';')\n",
    "\n",
    "# 5. Coordenadas Municipales (IGN/CNIG)\n",
    "# Fuente: Centro de Descargas del CNIG\n",
    "ingest_dimension('municipal_coordinates', 'municipios_coordenadas.csv', source_url=URL_CNIG, folder='ine', sep=';')\n",
    "\n",
    "# 6. Calendarios Laborales (MTDFP)\n",
    "# Fuente: Centro de Descargas del MTDFP\n",
    "ingest_dimension('work_calendars', 'calendario.csv', source_url=URL_MTDFP, folder='ine', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b47c83",
   "metadata": {},
   "source": [
    "#### Inspection: Rent Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.bronze.mapping_ine_mitma\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} WHERE municipio_ine LIKE 'NA' LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f75f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Schema Metadata (Structure)\n",
    "# Shows column names and types. Since we used 'all_varchar=true', everything should be VARCHAR.\n",
    "print(\"\\n[2] Schema Metadata (Columns & Types):\")\n",
    "con.execute(f\"DESCRIBE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile (Statistics)\n",
    "# Check 'approx_unique' to see how many municipalities have data\n",
    "# Check 'null_percentage' to ensure the ingestion didn't fail silently\n",
    "print(\"\\n[3] Quality Statistics (Nulls & Uniques):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5c81f",
   "metadata": {},
   "source": [
    "#### Final Check: table names and schema name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61710e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Final Check ---\n",
    "print(\"\\n--- Current Lakehouse State (Bronze Layer) ---\")\n",
    "\n",
    "# We use the internal system function 'duckdb_tables()'\n",
    "# This function sees EVERYTHING connected to the current session, regardless of the extension used.\n",
    "query_check = \"\"\"\n",
    "    SELECT table_name, schema_name\n",
    "    FROM duckdb_tables()\n",
    "    WHERE database_name = 'lakehouse' \n",
    "      AND schema_name = 'bronze';\n",
    "\"\"\"\n",
    "df_result = con.execute(query_check).df()\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c2aae",
   "metadata": {},
   "source": [
    "---\n",
    "# Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf15322",
   "metadata": {},
   "source": [
    "### Zone table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8191a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names to ensure our JOIN uses the right keys\n",
    "print(\"--- Checking Columns ---\")\n",
    "print(\"Zones (MITMA):\", con.execute(\"DESCRIBE lakehouse.bronze.zoning_municipalities\").fetch_df()['column_name'].tolist())\n",
    "print(\"Mapping (INE):\", con.execute(\"DESCRIBE lakehouse.bronze.work_calendars\").fetch_df()['column_name'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78da910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: dim_zones ---\")\n",
    "\n",
    "query_dim_zones = \"\"\"--sql\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.dim_zones AS\n",
    "    WITH unique_mapping AS (\n",
    "        -- CRITICAL CHANGE: We GROUP BY the MITMA code.\n",
    "        -- We take the FIRST (Minimum) INE code found as the 'Representative' code.\n",
    "        -- This ensures 1 MITMA Zone = 1 Row.\n",
    "        SELECT DISTINCT \n",
    "            CAST(municipio_mitma AS VARCHAR) as mitma_ref,\n",
    "            MIN(CAST(municipio_ine AS VARCHAR)) as ine_ref\n",
    "        FROM lakehouse.bronze.mapping_ine_mitma\n",
    "        WHERE municipio_mitma IS NOT NULL\n",
    "            AND municipio_ine IS NOT NULL\n",
    "            AND municipio_ine NOT LIKE 'NA'\n",
    "            AND municipio_mitma NOT LIKE 'NA'\n",
    "        GROUP BY municipio_mitma\n",
    "    ),\n",
    "    raw_zones AS (\n",
    "        SELECT \n",
    "            TRIM(z.ID) AS mitma_code,\n",
    "            TRIM(m.ine_ref)  AS ine_code,\n",
    "            TRIM(z.name) AS zone_name\n",
    "        FROM lakehouse.bronze.zoning_municipalities z\n",
    "        INNER JOIN unique_mapping m \n",
    "            ON TRIM(z.ID) = TRIM(m.mitma_ref)\n",
    "        WHERE z.ID IS NOT NULL AND z.ID != 'ID'\n",
    "        GROUP BY z.ID, z.name, m.ine_ref\n",
    "    )\n",
    "    SELECT\n",
    "        -- 1. Codes\n",
    "        ROW_NUMBER() OVER (ORDER BY mitma_code) AS zone_id,\n",
    "        mitma_code,\n",
    "        ine_code,\n",
    "        zone_name,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "        \n",
    "    FROM raw_zones\n",
    "    ORDER BY zone_id;\n",
    "\"\"\"\n",
    "con.execute(query_dim_zones)\n",
    "print(\"‚úÖ Created: lakehouse.silver.dim_zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7069a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.dim_zones\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} LIMIT 5\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coverage\n",
    "total = con.execute(\"SELECT COUNT(*) FROM lakehouse.silver.dim_zones\").fetchone()[0]\n",
    "mapped = con.execute(\"SELECT COUNT(*) FROM lakehouse.silver.dim_zones WHERE ine_code IS NOT NULL\").fetchone()[0]\n",
    "print(f\"-> Statistics: {mapped}/{total} zones have been successfully mapped to INE codes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e318ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78da0ad",
   "metadata": {},
   "source": [
    "### Population table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: metric_population ---\")\n",
    "\n",
    "query_pop = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.metric_population AS\n",
    "    SELECT \n",
    "        -- 1. Linking Key (Map column0 -> zone_id)\n",
    "        z.zone_id,\n",
    "        \n",
    "        -- 2. The Metric (Map column1 -> population)\n",
    "        -- Logic:\n",
    "        --   a. Cast to Integer\n",
    "        CAST(TRY_CAST(column1 AS DOUBLE) AS BIGINT) AS population,\n",
    "        \n",
    "        -- 3. Metadata\n",
    "        2023 AS year,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "        \n",
    "    FROM lakehouse.bronze.population_municipalities p\n",
    "        JOIN lakehouse.silver.dim_zones z ON TRIM(p.column0) = z.mitma_code\n",
    "    \n",
    "    WHERE \n",
    "        -- Filter out empty rows\n",
    "        column0 IS NOT NULL \n",
    "        -- Filter out the header row (if the first row contains text like 'ID' or 'Poblacion')\n",
    "        AND NOT regexp_matches(column1, '[a-zA-Z]') -- Exclude rows where population contains letters\n",
    "\"\"\"\n",
    "con.execute(query_pop)\n",
    "print(\"‚úÖ Created: lakehouse.silver.metric_population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.metric_population\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c77f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        SUM(population) as total_population_spain\n",
    "    FROM lakehouse.silver.metric_population\n",
    "\"\"\").fetchone()\n",
    "print(f\"-> Integrity Check: {stats[0]} rows loaded. Total Population: {stats[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227e3ad",
   "metadata": {},
   "source": [
    "### Coordinates table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: dim_coordinates ---\")\n",
    "\n",
    "query_coords = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.dim_coordinates AS\n",
    "    SELECT \n",
    "        z.zone_id,\n",
    "        \n",
    "        -- Coordinates\n",
    "        TRY_CAST(REPLACE(c.LATITUD_ETRS89, ',', '.') AS DOUBLE) AS latitude,\n",
    "        TRY_CAST(REPLACE(c.LONGITUD_ETRS89, ',', '.') AS DOUBLE) AS longitude,\n",
    "        \n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "        \n",
    "    FROM lakehouse.bronze.municipal_coordinates c\n",
    "    \n",
    "    -- SIMPLE JOIN: Exact string match\n",
    "    JOIN lakehouse.silver.dim_zones z \n",
    "        ON LEFT(c.COD_INE, 5) = z.ine_code \n",
    "        \n",
    "    WHERE z.zone_id IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query_coords)\n",
    "print(\"‚úÖ Created: lakehouse.silver.dim_coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CRITICAL CHECK ---\n",
    "# Let's see if the \"Simple Join\" worked or if we lost data due to \"01001\" vs \"1001\"\n",
    "total_zones_with_ine = con.execute(\"SELECT COUNT(*) FROM lakehouse.silver.dim_zones WHERE ine_code IS NOT NULL\").fetchone()[0]\n",
    "matched_coords = con.execute(\"SELECT COUNT(*) FROM lakehouse.silver.dim_coordinates\").fetchone()[0]\n",
    "\n",
    "print(f\"\\n[Match Statistics]\")\n",
    "print(f\"Zones with INE Codes: {total_zones_with_ine}\")\n",
    "print(f\"Zones with Coordinates: {matched_coords}\")\n",
    "\n",
    "if matched_coords < (total_zones_with_ine * 0.5):\n",
    "    print(\"‚ö†Ô∏è WARNING: Very low match rate. It is highly likely one table has leading zeros ('01001') and the other does not ('1001').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.dim_coordinates\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199cd48",
   "metadata": {},
   "source": [
    "### Rent table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c38521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: metric_ine_rent ---\")\n",
    "\n",
    "query_rent = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.metric_ine_rent AS\n",
    "    SELECT \n",
    "        -- 1. Master Key (Zone ID from our Dimension)\n",
    "        z.zone_id,\n",
    "        \n",
    "        -- 2. The Metric (Cleaned)\n",
    "        -- Format: \"13.500\" -> 13500. Handle \"dirty\" data (like \".\") using TRY_CAST\n",
    "        CAST(TRY_CAST(REPLACE(r.Total, '.', '') AS DOUBLE) AS BIGINT) AS income_per_capita,\n",
    "        \n",
    "        -- 3. Time Reference\n",
    "        CAST(r.Periodo AS INTEGER) AS year,\n",
    "        \n",
    "        -- 4. Metadata\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "        \n",
    "    FROM lakehouse.bronze.ine_rent_municipalities r\n",
    "    \n",
    "    -- JOIN Logic: Match Extracted INE Code to Zone INE Code\n",
    "    -- We split \"01001 Name\" by space to get \"01001\"\n",
    "    JOIN lakehouse.silver.dim_zones z \n",
    "        ON split_part(r.Municipios, ' ', 1) = z.ine_code\n",
    "        \n",
    "    WHERE \n",
    "        -- Filter 1: Only the specific indicator requested\n",
    "        r.\"Indicadores de renta media\" = 'Renta neta media por persona'\n",
    "        \n",
    "        -- Filter 2: Ensure we are at Municipality level (Districts/Sections must be empty/null)\n",
    "        AND (r.Distritos IS NULL OR r.Distritos = '')\n",
    "        AND (r.Secciones IS NULL OR r.Secciones = '')\n",
    "        \n",
    "        -- Filter 3: Valid data\n",
    "        AND CAST(TRY_CAST(REPLACE(r.Total, '.', '') AS DOUBLE) AS BIGINT) IS NOT NULL\n",
    "        AND z.zone_id IS NOT NULL;\n",
    "\"\"\"\n",
    "con.execute(query_rent)\n",
    "print(\"‚úÖ Created: lakehouse.silver.metric_ine_rent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.metric_ine_rent\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Available Years::\")\n",
    "con.execute(f\"SELECT year, COUNT(*) as zones FROM lakehouse.silver.metric_ine_rent GROUP BY year\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70448d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.metric_ine_rent\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} ORDER BY zone_id LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66666e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f628117",
   "metadata": {},
   "source": [
    "### Mobility data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: fact_mobility ---\")\n",
    "\n",
    "# 1. Create Empty Table Structure (Partitioned)\n",
    "query_schema = \"\"\"\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.fact_mobility (\n",
    "        date DATE,\n",
    "        hour INTEGER,\n",
    "        origin_zone_id BIGINT,\n",
    "        destination_zone_id BIGINT,\n",
    "        trips DOUBLE,\n",
    "        processed_at TIMESTAMP\n",
    "    );\n",
    "\"\"\"\n",
    "con.execute(query_schema)\n",
    "\n",
    "# 2. Configure Partitioning (Crucial for Speed)\n",
    "con.execute(\"ALTER TABLE lakehouse.silver.fact_mobility SET PARTITIONED BY (date);\")\n",
    "\n",
    "# 3. Insert Data (Transforming on the fly)\n",
    "query_insert = \"\"\"--sql\n",
    "    INSERT INTO lakehouse.silver.fact_mobility\n",
    "    SELECT \n",
    "        -- 1. Time Dimensions\n",
    "        try_strptime(fecha, '%Y%m%d') AS date,\n",
    "        -- dayofweek(strptime(fecha, '%Y%m%d')) AS day_of_week, -- 0=Sunday, 1=Monday... (DuckDB specific, verify range)\n",
    "        CAST(periodo AS INTEGER) AS hour,\n",
    "        \n",
    "        -- 2. Spatial Dimensions\n",
    "        zo.zone_id AS origin_zone_id,\n",
    "        zd.zone_id AS destination_zone_id,\n",
    "        \n",
    "        -- 3. Metrics (Spanish Format Handling: 1.200,50 -> 1200.50)\n",
    "        -- Remove thousands separator (.) then replace decimal comma (,) with dot (.)\n",
    "        TRY_CAST(REPLACE(REPLACE(viajes, '.', ''), ',', '.') AS DOUBLE) AS trips,\n",
    "        \n",
    "        -- 4. Audit\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "        \n",
    "    FROM lakehouse.bronze.mobility_sample_week m\n",
    "        INNER JOIN lakehouse.silver.dim_zones zo ON TRIM(m.origen) = zo.mitma_code\n",
    "        INNER JOIN lakehouse.silver.dim_zones zd ON TRIM(m.destino) = zd.mitma_code\n",
    "\n",
    "    WHERE viajes IS NOT NULL\n",
    "        AND try_strptime(fecha, '%Y%m%d') IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "print(\"-> Processing and Inserting Data (this may take a moment)...\")\n",
    "con.execute(query_insert)\n",
    "print(\"‚úÖ Created: lakehouse.silver.fact_mobility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.fact_mobility\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696292d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA: Referential Integrity Check ---\n",
    "# Are there zones in our trips that don't exist in our dictionary?\n",
    "\n",
    "print(\"--- üîç Checking for Orphan Keys ---\")\n",
    "\n",
    "query_orphans = \"\"\"\n",
    "    SELECT \n",
    "        m.origin_zone_id,\n",
    "        COUNT(*) as trip_count\n",
    "    FROM lakehouse.silver.fact_mobility m\n",
    "    LEFT JOIN lakehouse.silver.dim_zones z \n",
    "        ON m.origin_zone_id = z.zone_id\n",
    "    WHERE z.zone_id IS NULL\n",
    "    GROUP BY m.origin_zone_id\n",
    "    ORDER BY trip_count DESC\n",
    "    LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "orphans = con.execute(query_orphans).fetch_df()\n",
    "\n",
    "if orphans.empty:\n",
    "    print(\"‚úÖ PERFECT: All origin zones in mobility data exist in dim_zones.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è WARNING: Found {len(orphans)} zone IDs in mobility data that are MISSING from dim_zones.\")\n",
    "    print(\"Top missing zones (by trip volume):\")\n",
    "    print(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71023911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9fb37",
   "metadata": {},
   "source": [
    "### Calendar tables\n",
    "- Festive types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34aa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: dim_festive_types ---\")\n",
    "\n",
    "query_rent = \"\"\"--sql\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.dim_festive_types AS\n",
    "    SELECT DISTINCT\n",
    "        CASE \n",
    "            WHEN \"Tipo de Festivo\" ILIKE '%festivo nacional%'\n",
    "                 OR \"Tipo de Festivo\" ILIKE '%fiesta nacional%'\n",
    "            THEN 'NationalFestive'\n",
    "            ELSE \"Tipo de Festivo\"\n",
    "        END AS festive_type,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.bronze.work_calendars\n",
    "    WHERE \"Tipo de Festivo\" IS NOT NULL\n",
    "        AND (\"Tipo de Festivo\" ILIKE '%festivo nacional%' OR\n",
    "            \"Tipo de Festivo\" ILIKE '%fiesta nacional%');\n",
    "\"\"\"\n",
    "con.execute(query_rent)\n",
    "print(\"‚úÖ Created: lakehouse.silver.dim_festive_types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.dim_festive_types\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table}  LIMIT 50\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bd3da",
   "metadata": {},
   "source": [
    "- Bridge table with zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building Silver Table: bridge_zones_festives ---\")\n",
    "\n",
    "query_rent = \"\"\"--sql\n",
    "    CREATE OR REPLACE TABLE lakehouse.silver.bridge_zones_festives AS\n",
    "    SELECT z.zone_id,\n",
    "        CAST(strptime(c.\"Dia\", '%d/%m/%Y') AS DATE) AS festive_date,\n",
    "        ft.festive_type,\n",
    "        CURRENT_TIMESTAMP AS processed_at\n",
    "    FROM lakehouse.silver.dim_zones AS z\n",
    "    CROSS JOIN lakehouse.bronze.work_calendars AS c\n",
    "    JOIN lakehouse.silver.dim_festive_types AS ft\n",
    "    ON ft.festive_type =\n",
    "        CASE WHEN c.\"Tipo de Festivo\" ILIKE '%festivo nacional%'\n",
    "                OR c.\"Tipo de Festivo\" ILIKE '%fiesta nacional%'\n",
    "            THEN 'NationalFestive'\n",
    "            ELSE c.\"Tipo de Festivo\"\n",
    "        END;\n",
    "\"\"\"\n",
    "con.execute(query_rent)\n",
    "print(\"‚úÖ Created: lakehouse.silver.bridge_zones_festives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57912502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table we want to inspect\n",
    "target_table = \"lakehouse.silver.bridge_zones_festives\"\n",
    "\n",
    "print(f\"\\n--- üîç INSPECTING: {target_table} ---\")\n",
    "\n",
    "# 1. Content Preview\n",
    "# Check if the columns were separated correctly (look for separate columns, not one big text blob)\n",
    "# Also verify the 'source_url' is correct\n",
    "print(\"\\n[1] Content Preview (First 5 rows):\")\n",
    "con.execute(f\"SELECT * FROM {target_table} LIMIT 10\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f729dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quality Profile\n",
    "# Check for 100% nulls or weird values. This might take a moment.\n",
    "print(\"\\n[3] Data Quality Profile (Nulls & Unique Values):\")\n",
    "con.execute(f\"SUMMARIZE {target_table}\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdf829",
   "metadata": {},
   "source": [
    "#### Information test\n",
    "- Zone information in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c79cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2023_strict = \"\"\"\n",
    "    SELECT \n",
    "        z.zone_id,\n",
    "        z.ine_code,\n",
    "        z.zone_name,\n",
    "        p.year,\n",
    "        p.population,\n",
    "        r.income_per_capita AS rent,\n",
    "        c.latitude, \n",
    "        c.longitude\n",
    "        \n",
    "    FROM lakehouse.silver.dim_zones z\n",
    "    \n",
    "    -- 1. Population: Strictly 2023 (Inner Join, as this is our baseline)\n",
    "    JOIN lakehouse.silver.metric_population p \n",
    "        ON z.zone_id = p.zone_id \n",
    "        AND p.year = 2023\n",
    "        \n",
    "    -- 2. Rent: Strictly 2023 (Left Join)\n",
    "    -- If the Rent table has year=2021, this condition fails, and you get NULL (clean exclusion)\n",
    "    LEFT JOIN lakehouse.silver.metric_ine_rent r \n",
    "        ON z.zone_id = r.zone_id \n",
    "        AND r.year = 2023\n",
    "        \n",
    "    -- 3. Coordinates (Static)\n",
    "    LEFT JOIN lakehouse.silver.dim_coordinates c \n",
    "        ON z.zone_id = c.zone_id\n",
    "        \n",
    "    ORDER BY z.zone_id;\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- üìä Consolidated 2023 View ---\")\n",
    "con.execute(query_2023_strict).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862cb9e",
   "metadata": {},
   "source": [
    "- Mobility information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analytical Query: Mobility with Context (Names + Festivities) ---\n",
    "\n",
    "query_mobility_enriched = \"\"\"\n",
    "    SELECT \n",
    "        -- 1. Date\n",
    "        f.date,\n",
    "        \n",
    "        -- 2. Names (Resolved from IDs)\n",
    "        -- If ID is NULL (International), we label it 'External'\n",
    "        COALESCE(zo.zone_name, 'External/International') AS origin_name,\n",
    "        COALESCE(zd.zone_name, 'External/International') AS destination_name,\n",
    "        \n",
    "        -- 3. Is Origin Festive? (True/False)\n",
    "        CASE \n",
    "            WHEN bfo.festive_type IS NOT NULL THEN TRUE \n",
    "            ELSE FALSE \n",
    "        END AS is_origin_festive,\n",
    "        \n",
    "        -- 4. Is Destination Festive? (True/False)\n",
    "        CASE \n",
    "            WHEN bfd.festive_type IS NOT NULL THEN TRUE \n",
    "            ELSE FALSE \n",
    "        END AS is_dest_festive\n",
    "        \n",
    "    FROM lakehouse.silver.fact_mobility f\n",
    "    \n",
    "    -- Join Dimensions to get Names\n",
    "    LEFT JOIN lakehouse.silver.dim_zones zo \n",
    "        ON f.origin_zone_id = zo.zone_id\n",
    "    LEFT JOIN lakehouse.silver.dim_zones zd \n",
    "        ON f.destination_zone_id = zd.zone_id\n",
    "        \n",
    "    -- Join Bridge for ORIGIN (Match Zone AND Date)\n",
    "    LEFT JOIN lakehouse.silver.bridge_zones_festives bfo \n",
    "        ON f.origin_zone_id = bfo.zone_id \n",
    "            AND f.date = bfo.festive_date\n",
    "        \n",
    "    -- Join Bridge for DESTINATION (Match Zone AND Date)\n",
    "    LEFT JOIN lakehouse.silver.bridge_zones_festives bfd \n",
    "        ON f.destination_zone_id = bfd.zone_id \n",
    "            AND f.date = bfd.festive_date\n",
    "    \n",
    "    WHERE is_origin_festive IS True\n",
    "        AND is_dest_festive IS True\n",
    "\n",
    "    ORDER BY destination_name desc\n",
    "        \n",
    "    -- Limit to avoid printing billions of rows\n",
    "    LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- üìä Mobility Enriched with Names & Holiday Status ---\")\n",
    "con.execute(query_mobility_enriched).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Current Lakehouse State (Silver Layer) ---\")\n",
    "\n",
    "# We use the internal system function 'duckdb_tables()'\n",
    "# This function sees EVERYTHING connected to the current session, regardless of the extension used.\n",
    "query_check = \"\"\"\n",
    "    SELECT table_name, schema_name\n",
    "    FROM duckdb_tables()\n",
    "    WHERE database_name = 'lakehouse' \n",
    "      AND schema_name = 'silver';\n",
    "\"\"\"\n",
    "df_result = con.execute(query_check).df()\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c72e28",
   "metadata": {},
   "source": [
    "---\n",
    "# Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec0c99",
   "metadata": {},
   "source": [
    "#### Business Question 1: Typical Day Mobility\n",
    "We need to identify distinct Mobility Patterns using Unsupervised Machine Learning. We apply the K-Means algorithm with K = 3. Based on domain knowledge, we expect these groups to represent:\n",
    "- Labor Days: High peaks at 08:00, 15:00 and 21:00.\n",
    "- Saturdays: Smoother curve, leisure activity.\n",
    "- Sundays/Holidays: Flat curve, low activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. DATA PREPARATION (Fetch & Pivot) ---\n",
    "# Fetching aggregated hourly data from the Silver Layer (fact_mobility)\n",
    "print(\"   -> Fetching data from Silver Layer...\")\n",
    "\n",
    "query_fetch = \"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        hour,\n",
    "        SUM(trips) as total_trips\n",
    "    FROM lakehouse.silver.fact_mobility\n",
    "    WHERE trips IS NOT NULL\n",
    "    GROUP BY date, hour\n",
    "    ORDER BY date, hour;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and convert to Pandas DataFrame\n",
    "df = con.execute(query_fetch).df()\n",
    "\n",
    "if df.empty:\n",
    "    print(\"   ‚ö†Ô∏è Warning: No data found in fact_mobility.\")\n",
    "else:\n",
    "    # PIVOT: Transform from Long to Wide format (Rows=Days, Columns=Hours 0-23)\n",
    "    df_pivot = df.pivot(index='date', columns='hour', values='total_trips').fillna(0)\n",
    "\n",
    "    # Ensure all hour columns (0 to 23) exist\n",
    "    for h in range(24):\n",
    "        if h not in df_pivot.columns:\n",
    "            df_pivot[h] = 0\n",
    "            \n",
    "    # Sort columns numerically to ensure correct vector order\n",
    "    df_pivot = df_pivot.sort_index(axis=1)\n",
    "\n",
    "    # --- 2. NORMALIZATION ---\n",
    "    # Normalize row-wise (each day sums to 1). \n",
    "    # This allows comparing the \"shape\" of the curve rather than the total volume.\n",
    "    print(\"   -> Normalizing daily profiles...\")\n",
    "    row_sums = df_pivot.sum(axis=1)\n",
    "    df_normalized = df_pivot.div(row_sums, axis=0).fillna(0)\n",
    "\n",
    "    # --- 3. CLUSTERING (K-Means) ---\n",
    "    # We use k=3 to capture basic patterns: \n",
    "    # 1. Weekdays (Work), 2. Saturdays (Leisure), 3. Sundays/Holidays (Rest)\n",
    "    n_clusters = 3\n",
    "    print(f\"   -> Running K-Means Clustering (k={n_clusters})...\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(df_normalized)\n",
    "\n",
    "    # Save results to a temporary DataFrame\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': df_normalized.index,\n",
    "        'cluster_id': clusters\n",
    "    })\n",
    "\n",
    "    # --- 4. BUILD GOLD TABLE: dim_mobility_patterns ---\n",
    "    # Materialize the model results into the Gold Layer\n",
    "    # --- 5. BUILD GOLD TABLE: typical_day_by_cluster (Using CTE) ---\n",
    "    print(\"\\n--- üèóÔ∏è Building Gold Table: typical_day_by_cluster ---\")\n",
    "\n",
    "    # 1. Register the clustering results DataFrame as a virtual view\n",
    "    con.register('view_dim_clusters', df_results)\n",
    "\n",
    "    # 2. Define query using WITH clause\n",
    "    query_typical_cte = \"\"\"\n",
    "        CREATE OR REPLACE TABLE lakehouse.gold.typical_day_by_cluster AS\n",
    "        \n",
    "        WITH dim_mobility_patterns AS (\n",
    "            -- We select directly from the registered Python DataFrame view\n",
    "            SELECT \n",
    "                date, \n",
    "                cluster_id \n",
    "            FROM view_dim_clusters\n",
    "        )\n",
    "        \n",
    "        SELECT \n",
    "            p.cluster_id,\n",
    "            f.hour,\n",
    "            \n",
    "            -- Metrics\n",
    "            ROUND(AVG(f.trips), 2) as avg_trips,\n",
    "            ROUND(AVG(f.trips_km), 2) as avg_trips_km,\n",
    "            SUM(f.trips) as total_trips_sample,\n",
    "            \n",
    "            CURRENT_TIMESTAMP as processed_at\n",
    "            \n",
    "        FROM lakehouse.silver.fact_mobility f\n",
    "        JOIN dim_mobility_patterns p ON f.date = p.date\n",
    "        GROUP BY p.cluster_id, f.hour\n",
    "        ORDER BY p.cluster_id, f.hour;\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Execute\n",
    "    con.execute(query_typical_cte)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"‚úÖ Created: lakehouse.gold.typical_day_by_cluster\")\n",
    "    \n",
    "    # 1. Assign result to variable\n",
    "    print(\"\\n--- üìä Cluster Interpretation ---\")\n",
    "    \n",
    "    # We query 'view_dim_clusters' directly because the permanent dim_ table \n",
    "    # does not exist in this version of the script.\n",
    "    analysis_df = con.execute(\"\"\"\n",
    "        SELECT \n",
    "            cluster_id, \n",
    "            COUNT(*) as days_in_cluster,\n",
    "            -- Calculate day name on the fly since it's not in the dataframe\n",
    "            MODE(dayname(date)) as typical_day\n",
    "        FROM view_dim_clusters\n",
    "        GROUP BY cluster_id\n",
    "        ORDER BY days_in_cluster DESC\n",
    "    \"\"\").df()\n",
    "\n",
    "    # 2. Explicitly print using to_string() to ensure full visibility\n",
    "    if not analysis_df.empty:\n",
    "        print(analysis_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è The table is empty.\")\n",
    "\n",
    "    # 4. Clean up view\n",
    "    con.unregister('view_dim_clusters')\n",
    "\n",
    "    print(\"\\n‚úÖ Gold Layer Analysis Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e923c0",
   "metadata": {},
   "source": [
    "To verify the accuracy of the Unsupervised Machine Learning model, we must validate its output against the actual calendar data stored in the Silver Layer. Success criteria:\n",
    "- Sundays and National Holidays should fall into the same cluster (typically the low-mobility cluster).\n",
    "- Weekdays should dominate a separate, high-mobility cluster.\n",
    "- Saturdays typically form their own cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- üïµÔ∏è‚Äç‚ôÇÔ∏è VALIDATION: 3 Clusters vs. Real Calendar ---\")\n",
    "\n",
    "# SAFETY NET: Ensure the view is registered (in case the previous cell unregistered it)\n",
    "con.register('view_dim_clusters', df_results)\n",
    "\n",
    "# This query cross-references your clusters with your actual calendar\n",
    "query_validation = \"\"\"\n",
    "WITH national_holidays AS (\n",
    "    -- 1. Get only unique NATIONAL holidays from the bridge table\n",
    "    -- Use DISTINCT to avoid duplicating rows per zone\n",
    "    SELECT DISTINCT festive_date\n",
    "    FROM lakehouse.silver.bridge_zones_festives\n",
    "    WHERE festive_type = 'NationalFestive'\n",
    "),\n",
    "labeled_data AS (\n",
    "    SELECT \n",
    "        p.cluster_id,\n",
    "        p.date,\n",
    "        dayname(p.date) as day_of_week,\n",
    "        \n",
    "        -- 2. Create \"Ground Truth\" label for comparison\n",
    "        CASE \n",
    "            -- Priority 1: Is it a national holiday according to Silver data?\n",
    "            WHEN h.festive_date IS NOT NULL THEN 'National Holiday'\n",
    "            \n",
    "            -- Priority 2: Natural weekend\n",
    "            WHEN dayname(p.date) = 'Sunday' THEN 'Sunday'\n",
    "            WHEN dayname(p.date) = 'Saturday' THEN 'Saturday'\n",
    "\n",
    "            -- Priority 3: Remaining days\n",
    "            ELSE 'Weekday (Mon-Fri)'\n",
    "        END as real_category\n",
    "        \n",
    "    -- UPDATED: Select from the virtual view instead of the physical gold table\n",
    "    FROM view_dim_clusters p\n",
    "    LEFT JOIN national_holidays h ON p.date = h.festive_date\n",
    ")\n",
    "-- 3. Count how many days of each type fell into each cluster\n",
    "SELECT \n",
    "    cluster_id,\n",
    "    real_category,\n",
    "    COUNT(*) as total_days\n",
    "FROM labeled_data\n",
    "GROUP BY cluster_id, real_category\n",
    "ORDER BY cluster_id, total_days DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display the full result\n",
    "df_val = con.execute(query_validation).df()\n",
    "\n",
    "if df_val.empty:\n",
    "    print(\"‚ö†Ô∏è Something went wrong, the table is empty.\")\n",
    "else:\n",
    "    print(df_val.to_string(index=False))\n",
    "\n",
    "con.unregister('view_dim_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Query the data for plotting ---\n",
    "print(\"Querying typical day demand data...\")\n",
    "\n",
    "# 1. RE-REGISTER the view \n",
    "# (Necessary because the previous cell unregistered it, but we need it to calculate labels)\n",
    "con.register('view_dim_clusters', df_results)\n",
    "\n",
    "# 2. Query: Calculate labels dynamically from the view and join with the Gold table\n",
    "query = \"\"\"\n",
    "WITH cluster_labels AS (\n",
    "    SELECT \n",
    "        cluster_id, \n",
    "        -- Calculate day name on the fly since we are using the view\n",
    "        MODE(dayname(date)) as label\n",
    "    FROM view_dim_clusters\n",
    "    GROUP BY cluster_id\n",
    ")\n",
    "SELECT \n",
    "    t.hour,\n",
    "    -- Create a readable label for the legend: \"Cluster 0 (Sunday)\"\n",
    "    'Cluster ' || t.cluster_id || ' (' || l.label || ')' as pattern_name,\n",
    "    t.avg_trips\n",
    "FROM lakehouse.gold.typical_day_by_cluster t\n",
    "JOIN cluster_labels l ON t.cluster_id = l.cluster_id\n",
    "ORDER BY t.hour;\n",
    "\"\"\"\n",
    "\n",
    "demand_df = con.execute(query).df()\n",
    "\n",
    "if demand_df.empty:\n",
    "    print(\"ERROR: 'gold.typical_day_by_cluster' table is empty. No data to plot.\")\n",
    "else:\n",
    "    # --- Prepare data for plotting ---\n",
    "    print(\"Pivoting data for plotting...\")\n",
    "    # Pivot: Index=Hour, Columns=Pattern Name, Values=Average Trips\n",
    "    pivot_df = demand_df.pivot(index='hour', columns='pattern_name', values='avg_trips')\n",
    "    \n",
    "    # --- Create the plot ---\n",
    "    print(\"Generating plot with matplotlib...\")\n",
    "    \n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Plot the data\n",
    "    pivot_df.plot(kind='line', ax=ax, marker='o', markersize=4)\n",
    "    \n",
    "    ax.set_title('Typical Daily Mobility Patterns (Clustered Profiles)', fontsize=16)\n",
    "    ax.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax.set_ylabel('Average Trips per Hour', fontsize=12)\n",
    "    \n",
    "    # Set x-ticks to be clear\n",
    "    ax.set_xticks(range(0, 24))\n",
    "    ax.set_xticklabels([f'{h:02d}:00' for h in range(24)], rotation=45, ha='right')\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(title='Identified Pattern')\n",
    "    \n",
    "    # Ensure labels are not cut off\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Optional: Clean up again if you want to keep memory clean\n",
    "con.unregister('view_dim_clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b4c33",
   "metadata": {},
   "source": [
    "#### Business Question 2: Infrastructure Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar y cargar la extensi√≥n espacial\n",
    "con.execute(\"INSTALL 'spatial';\")\n",
    "con.execute(\"LOAD 'spatial';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Gold table 'gold_infrastructure_gaps'...\")\n",
    "\n",
    "# Esta consulta responde a la Pregunta de Negocio 2\n",
    "gold_bq2_query = \"\"\"--sql\n",
    "CREATE OR REPLACE TABLE lakehouse.gold.gold_infrastructure_gaps AS\n",
    "\n",
    "WITH od_pairs AS (\n",
    "    SELECT\n",
    "        origin_zone_id,\n",
    "        destination_zone_id,\n",
    "        SUM(trips) AS total_actual_trips\n",
    "    FROM lakehouse.silver.fact_mobility\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "\n",
    "model_calculation AS (\n",
    "    SELECT\n",
    "        m.origin_zone_id AS org_zone_id,\n",
    "        m.destination_zone_id AS dest_zone_id,\n",
    "        p.population AS total_population,                     -- P_i\n",
    "        r.income_per_capita AS rent,              -- E_j\n",
    "        m.total_actual_trips AS total_trips,                    -- Viajes reales\n",
    "        \n",
    "        -- Calcular distancia (d_ij) en KM usando la extensi√≥n espacial\n",
    "        -- Usamos GREATEST(0.5, ...) para evitar distancias de 0 (ej. viajes en la misma zona)\n",
    "        -- y as√≠ prevenir errores de divisi√≥n por cero.\n",
    "        GREATEST(\n",
    "            0.5, -- Distancia m√≠nima de 0.5 km\n",
    "            st_distance_spheroid(\n",
    "                st_point(c_org.longitude, c_org.latitude), \n",
    "                st_point(c_dest.longitude, c_dest.latitude)\n",
    "            ) / 1000 -- Convertir metros (salida de st_distance) a KM\n",
    "        ) AS geographic_distance_km             -- d_ij\n",
    "            \n",
    "    FROM od_pairs AS m\n",
    "    JOIN lakehouse.silver.metric_population AS p ON m.origin_zone_id = p.zone_id\n",
    "    JOIN lakehouse.silver.metric_ine_rent AS r ON m.destination_zone_id = r.zone_id\n",
    "    JOIN lakehouse.silver.dim_coordinates as c_org ON m.origin_zone_id = c_org.zone_id\n",
    "    JOIN lakehouse.silver.dim_coordinates as c_dest ON m.destination_zone_id = c_dest.zone_id\n",
    "\n",
    "    \n",
    "    -- Filtramos datos malos para evitar errores en el modelo\n",
    "    WHERE p.population > 0 \n",
    "      AND r.income_per_capita > 0\n",
    "      AND c_org.latitude IS NOT NULL\n",
    "      AND c_dest.latitude IS NOT NULL\n",
    "      AND m.origin_zone_id != m.destination_zone_id -- Evitar viajes dentro de la misma zona\n",
    ")\n",
    "\n",
    "-- Calcular el modelo final y el mismatch\n",
    "SELECT\n",
    "    org_zone_id,\n",
    "    dest_zone_id,\n",
    "    total_trips,\n",
    "    total_population,\n",
    "    rent,\n",
    "    geographic_distance_km,\n",
    "    \n",
    "    -- Calcular Modelo de Gravedad T_ij = k * (P_i * E_j) / (d_ij^2)\n",
    "    -- Asumimos k=1 para este PoC del Sprint 1\n",
    "    (1.0 * (CAST(total_population AS DOUBLE) * CAST(rent AS DOUBLE))) / \n",
    "    (geographic_distance_km * geographic_distance_km) AS estimated_potential_trips, -- T_ij\n",
    "        \n",
    "    -- Calcular Mismatch Ratio \n",
    "    -- (Viajes Reales / Viajes Estimados)\n",
    "    total_trips / NULLIF(estimated_potential_trips, 0) AS mismatch_ratio\n",
    "\n",
    "FROM model_calculation;\n",
    "\"\"\"\n",
    "\n",
    "con.execute(gold_bq2_query)\n",
    "print(\"‚úì Tabla 'gold.gold_infrastructure_gaps' creada.\")\n",
    "\n",
    "# --- Verificaci√≥n ---\n",
    "print(\"\\n--- Verificaci√≥n BQ2: Top 10 Zonas con 'Mismatch' (potencialmente peor servidas) ---\")\n",
    "verification_bq2 = \"\"\"--sql\n",
    "    SELECT \n",
    "        org_zone_id,\n",
    "        dest_zone_id,\n",
    "        total_trips,\n",
    "        estimated_potential_trips,\n",
    "        geographic_distance_km,\n",
    "        mismatch_ratio\n",
    "    FROM lakehouse.gold.gold_infrastructure_gaps\n",
    "    WHERE total_trips > 10 -- Filtrar pares con muy pocos viajes\n",
    "    AND org_zone_id != dest_zone_id -- Evitar viajes dentro de la misma zona\n",
    "    ORDER BY mismatch_ratio ASC -- Ordenamos por ratio m√°s bajo (peor servicio)\n",
    "    LIMIT 10;\n",
    "\"\"\"\n",
    "display(con.execute(verification_bq2).df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Verificaci√≥n BQ2: Top 10 Zonas con 'Mismatch' (potencialmente peor servidas) ---\")\n",
    "verification_bq2 = \"\"\"--sql\n",
    "    SELECT \n",
    "        org_zone_id,\n",
    "        dest_zone_id,\n",
    "        total_trips,\n",
    "        estimated_potential_trips,\n",
    "        geographic_distance_km,\n",
    "        mismatch_ratio\n",
    "    FROM lakehouse.gold.gold_infrastructure_gaps\n",
    "    WHERE total_trips > 10 -- Filtrar pares con muy pocos viajes\n",
    "    ORDER BY mismatch_ratio ASC -- Ordenamos por ratio m√°s bajo (peor servicio)\n",
    "    LIMIT 15;\n",
    "\"\"\"\n",
    "display(con.execute(verification_bq2).df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2262c4",
   "metadata": {},
   "source": [
    "# Close connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
