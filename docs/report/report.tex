\documentclass[11pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{graphicx}     % For images
\usepackage{booktabs}     % For nice tables
\usepackage{hyperref}     % For links (repo URL)
\usepackage{listings}     % For code snippets
\usepackage{xcolor}       % For code coloring
\usepackage{float}        % For figure placement
\usepackage{titlesec}     % Custom section titles
\usepackage{parskip}      % Space between paragraphs
\usepackage{amsmath}
\usepackage{pdflscape}
\usepackage{subcaption}


% --- CODE SNIPPET STYLING ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- METADATA ---
\title{\textbf{Building a Scalable 3-Tier Data Lakehouse for Mobility Analysis in Spain}\\ Big Data Engineering Project \\}
\author{
    \textbf{María López Hernández} \and \textbf{Joan Sánchez Verdú} \and \textbf{Fernando Blanco Membrives} \\    
    % \textit{Link to Code Repository:}\\ \textit{\url{https://github.com/joanstudyai-code/MUCEIM-BDET-Project.git}}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    \noindent 
    \textbf{Abstract.} The rapid digitalization of urban transport has generated massive volumes of mobility data, rendering traditional analytical workflows obsolete. This paper presents a scalable 3-tier Data Lakehouse architecture designed to analyze large-scale Spanish mobility data. By integrating high-volume Origin-Destination matrices from MITMA with INE socio-economic indicators, the system provides a robust foundation for urban analytics. The infrastructure leverages a serverless stack—utilizing \textit{DuckDB} for vectorized processing and \textit{DuckLake} for ACID transactions—orchestrated via Apache Airflow. We validate the architecture through three analytical use cases: temporal pattern clustering, infrastructure gap detection via gravity models, and functional zoning classification, demonstrating a cost-effective solution for data-driven transport planning.
\end{abstract}

% --- SECTIONS ---

\section{Introduction}

The availability of high-resolution mobility data from the Spanish Ministry of Transport (MITMA) has opened new avenues for urban planning. However, the sheer volume of these datasets (comprising billions of daily trip records) presents significant engineering challenges that exceed the capabilities of traditional analytical tools. Transport experts often lack the scalable infrastructure required to transform this raw big data into actionable insights.

This project addresses this gap by implementing a Data Lakehouse architecture. By combining the cost-efficiency of data lakes with the transactional integrity of data warehouses, we propose a solution built on DuckDB and DuckLake. This infrastructure allows for the efficient processing of large-scale mobility matrices, enriching them with socio-economic context without the overhead of enterprise systems.


\subsection{Objectives}
The primary goal is to build a robust, reproducible data platform. The specific objectives are:

\begin{itemize}
    \item \textbf{Scalable Architecture:} Implement a 3-tier Lakehouse (Bronze, Silver, Gold) that decouples storage (S3) from compute (AWS Batch) to optimize costs and performance.
    \item \textbf{Data Integration:} Automate ELT pipelines to clean and fuse heterogeneous sources, including mobility flows, census demographics, and vector geometries.
    \item \textbf{Analytical Value:} Demonstrate the system's utility through advanced use cases, such as gravity models for infrastructure analysis and functional zoning classification.
\end{itemize}











\section{Data Sources}
The data lakehouse architecture is designed to ingest and integrate public domain data exclusively, ensuring reproducibility and open access. The following subsections detail the specific datasets and sources selected to build the information system.

\subsection{MITMA Open Data}
The core mobility dataset is sourced from the Spanish Ministry of Transport, Mobility and Urban Agenda (MITMA \cite{mitma}), specifically the \textit{Estudios Básicos de Movilidad}. This dataset provides high-resolution insights into the daily movements of residents derived from mobile network big data.
From the MITMA the following files have been used:
\begin{itemize}
    \item \textbf{\textit{(202301-202312)\_Viajes\_municipios.tar}:} These files, located in the MITMA web at \textit{estudios\_basicos/por-municipios/viajes/meses-completos/}, contain the information of the mobility from an origin to a destination municipality, per day, per hour, for the year 2023.
    \item \textbf{\textit{nombres\_municipios.csv}:} This file, located in the MITMA web at \textit{zonificacion/zonificacion\_municipios/}, contains the name of each municipality.
    \item \textbf{\textit{relacion\_ine\_zonificacionMitma.csv}:} This file, located in the MITMA web at \textit{zonificacion/}, cointains the mapping between the MITMA codes and the INE codes for the municipalities.
    \item  \textbf{\textit{poblacion.csv}:} This file, located in the MITMA web at \textit{zonificacion/}, cointains the population of each municipality per year.
    \item \textbf{\textit{zonificacion\_municipios.shp}:} This ESRI Shapefile (along with its \textit{.dbf}, \textit{.shx}, and \textit{.prj} components), located in the MITMA web at \textit{zonificacion/zonificacion\_municipios/}, contains the official geospatial geometries (polygons) for the municipal study zones.
\end{itemize}

\subsection{INE Demographics and Economics}
To contextualize mobility flows and enable the gravity model analysis, mobility data is enriched with official statistics from the Spanish National Statistics Institute (INE \cite{ine}).
The statistics used from INE are the following:
\begin{itemize}
    \item \textbf{\textit{ine\_rent\_municipalities.csv}:} This file, located in the INE web at \textit{\url{https://www.ine.es/jaxiT3/files/t/es/csv_bd/}}, contains the rent information per year and municipality.
\end{itemize}

\subsection{Open Data}
To categorize mobility patterns by day type (e.g., distinguishing standard workdays from holidays), specific calendar data is integrated from Open Data portals.

\begin{itemize}
    \item \textbf{\textit{calendario\_laboral.csv}:} This file, sourced from the Madrid City Council Open Data Portal (\textit{Datos Abiertos Madrid} \cite{madrid_data}) at \textit{\url{https://datos.madrid.es/}}, contains the official working calendar. It provides the classification of dates (working days, weekends, and holidays) from 2013 to 2026 (included).
\end{itemize}







\newpage
\section{Lakehouse Architecture}
The system follows the Medallion Architecture pattern, decoupled into storage and compute layers to ensure scalability.

\subsection{Technology Stack}
The platform employs a modular architecture designed to decouple storage from compute (Figure \ref{fig:architecture}), leveraging the following core components:

\begin{itemize}
    \item \textbf{Compute Engine (DuckDB \cite{duckdb}):} A high-performance, in-process SQL OLAP engine. It handles vectorized data processing and supports out-of-core execution, allowing for the analysis of datasets larger than available RAM.

    \item \textbf{Elastic Compute (AWS Batch \cite{aws_batch}):} Provides an ephemeral, scalable cloud compute layer. It offloads heavy transformation tasks to on-demand instances, optimizing costs through the use of Spot market resources.
    
    \item \textbf{Lakehouse Management (DuckLake \cite{ducklake}):} A transaction manager that enables ACID properties and snapshot isolation over object storage, ensuring data consistency during concurrent operations.

    \item \textbf{Cloud Storage \& Catalog (Neon + S3):} A decoupled storage design where \textbf{AWS S3} holds the physical data files (Parquet/CSV) and \textbf{Neon} (Serverless Postgres) acts as the centralized metadata catalog and locking mechanism.

    \item \textbf{Orchestration (Apache Airflow \cite{airflow}):} Manages the end-to-end data lifecycle via Directed Acyclic Graphs (DAGs), handling complex dependencies, scheduling, and automated retries.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imgs/Architecture_diagram}
    \caption{High-level overview of the 3-Tier Data Lakehouse architecture, decoupling compute (DuckDB) from storage (S3).}
    \label{fig:architecture}
\end{figure}

\subsection{Data Layering Strategy}
The data pipeline is structured following the standard ``Medallion'' architecture pattern (Multi-hop), which organizes data quality into progressive stages. This design ensures that raw data is preserved for auditability while downstream layers provide cleaned and enriched datasets optimized for analytical consumption. The architecture consists of three distinct zones, each with a specific purpose and schema design.

\subsubsection{Bronze Layer (Raw)}
The Bronze layer acts as the landing zone (Staging Area) for all external data. Its primary function is to capture data from source systems in its native format with minimal modification, ensuring an immutable record of the original input.

In this implementation, a distinct table is instantiated for each source entity to maintain data isolation. To ensure pipeline robustness and prevent ingestion failures due to type mismatches, tabular data columns (such as CSV files) are initially cast as \texttt{VARCHAR}. A notable exception is the geographic data: by utilizing DuckDB's \texttt{spatial} extension, the ESRI Shapefile components are ingested via the \texttt{ST\_Read} function. This allows the system to capture municipal boundaries directly as native \texttt{GEOMETRY} objects, preserving spatial precision from the source.

Additionally, the ingestion process is configured to ignore malformed records, ensuring that isolated errors do not halt the entire pipeline. To enhance data governance and auditability, metadata columns are appended to every record, including \texttt{ingestion\_timestamp} and \texttt{source\_url}. Finally, to optimize storage and performance, the high-volume mobility data is physically partitioned by date (\texttt{fecha}), enabling efficient batch processing.

% \begin{landscape}
% \begin{figure}[p]
%     \centering
%     \includegraphics[width=1.5\textwidth]{imgs/Bronze_Schema_diagramS4}
%     \caption{Entity Diagram of the Bronze Layer. One table for each file.}
%     \label{fig:bronze_layer_diagram}
% \end{figure}
% \end{landscape}

\subsubsection{Silver Layer (Cleaned \& Integrated)}
The Silver layer represents the refined, enterprise-wide view of the data. In this stage, data undergoes validation, cleaning, standardization, and enrichment processes to transform raw inputs into structured, trustworthy tables. The schema design follows a Star Schema approach, establishing a central fact table for mobility flows linked to surrounding dimension tables for spatial and socio-economic context. All tables include a \texttt{TIMESTAMP WITH TIME ZONE} column \texttt{processed\_at} for auditability.

\paragraph{Dimension Modeling and Spatial Integration (dim\_zones)\\}
The foundation of the Silver layer is the creation of a unified spatial dimension, \texttt{dim\_zones}. This table integrates the descriptive zoning metadata (MITMA codes, INE codes, and municipality names) with the geospatial vector data ingested in the Bronze layer.
\begin{itemize}
    \item \textbf{Spatial Integration:} The \texttt{dim\_zones} table leverages the native \texttt{GEOMETRY} objects already parsed in the Bronze layer. By joining the municipal geometries with the MITMA-INE crosswalk (\texttt{mapping\_ine\_mitma}), the Silver layer establishes a clean, spatially-enabled dimension.
    \item \textbf{Surrogate Keys:} A unique integer identifier (\texttt{zone\_id}) is generated for each municipality. This decouples downstream analytics from changes in external string codes (MITMA/INE IDs) and improves join performance.
\end{itemize}


\paragraph{Spatial Connectivity (Distance Matrix)\\}
To support spatial network analysis and distance-based filtering, a derivative dimension table, \texttt{dim\_zone\_distances}, is established. This table functions as a pre-computed distance matrix representing the relationships between municipality pairs. By calculating the distance between the centroids of the geometries stored in \texttt{dim\_zones}, the system avoids computationally expensive geospatial operations during query time.
\begin{itemize}
    \item \textbf{Zone Linkage:} The table maps relationships using \texttt{origin\_zone\_id} and \texttt{destination\_zone\_id}, which act as foreign keys referencing the central \texttt{dim\_zones} table.
    \item \textbf{Metric Storage:} The \texttt{dist\_km} column stores the distance in kilometers as a \texttt{DOUBLE} precision value, allowing for precise downstream analysis of trip lengths and gravity models.
\end{itemize}


\paragraph{Socio-Economic Metrics and Filtering\\}
Complementary metric tables are generated by cleaning raw CSVs. For the \texttt{metric\_population}, data is cast to \texttt{BIGINT} while removing metadata headers. 

For the \texttt{metric\_ine\_rent} table, specific business logic is applied to the INE source file to resolve granularity mismatches. The pipeline filters the dataset to isolate the ``Average Net Income per Person'' indicator and explicitly excludes district-level or census-section entries (filtering out rows where \texttt{Distritos} or \texttt{Secciones} are populated). This ensures that the resulting economic metrics align perfectly with the municipal granularity of the \texttt{dim\_zones} table.

\paragraph{Temporal Context (Holidays)\\}
To support temporal analysis, a \texttt{dim\_zone\_holidays} table is constructed. This process involves filtering the raw working calendar for events classified as ``National Holidays'' and cross-referencing them with the zones. This dimension allows the Gold layer to accurately distinguish mobility patterns between standard working days and holidays.

\paragraph{Mobility Fact Table Construction\\}
The core mobility data is transformed into the \texttt{fact\_mobility} table. This process converts the raw, text-based daily CSVs into a strongly typed, time-series optimized format. Key transformations include:

\begin{enumerate}
    \item \textbf{Temporal Standardization:} The raw separate date and hour fields are combined into a single \texttt{TIMESTAMP WITH TIME ZONE} column, explicitly localized to 'Europe/Madrid'.
    \item \textbf{Spatial Lookup:} Origin and Destination codes are replaced by their corresponding \texttt{zone\_id} through double joins against the \texttt{dim\_zones} table, ensuring referential integrity.
    \item \textbf{Type Casting:} The ``trips'' column is cleaned (handling European decimal formats) and cast to \texttt{DOUBLE} precision to support fractional trip expansion factors.
\end{enumerate}

\paragraph{Data Observability and Quality Logging\\}
To ensure the reliability of the pipeline, a dedicated table named \texttt{data\_quality\_logs} has been implemented within the Silver layer. Unlike the business data tables, this table serves as a metadata registry to persist the results of automated audit checks performed during ingestion (e.g., null rate calculations, row count validations, or statistical anomalies). This allows for longitudinal tracking of data health.

The schema is designed to be generic enough to store heterogeneous metrics:
{\raggedright
\begin{itemize}
    \item \textbf{\texttt{check\_timestamp}:} The exact time when the audit was executed.
    \item \textbf{\texttt{table\_name}:} The target table being audited (e.g., \textit{fact\_mobility}).
    \item \textbf{\texttt{metric\_name}:} A descriptive identifier for the KPI (e.g., \textit{avg\_income\_per\_capita}, \textit{null\_zone\_rate}).
    \item \textbf{\texttt{metric\_value}:} A numeric field (\texttt{DOUBLE}) storing the result of the check.
    \item \textbf{\texttt{notes}:} Textual field for context, such as the specific batch date range or error warnings associated with the metric.
\end{itemize}
}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.75\textwidth]{imgs/Silver_Schema_diagramS4}
%     \caption{Entity-Relationship Diagram (ERD) of the Silver Layer. The schema follows a Star Schema design, centering on the \texttt{fact\_mobility} table linked to spatial (\texttt{dim\_zones}) and temporal dimensions.}
%     \label{fig:silver_layer_diagram}
% \end{figure}

\subsubsection{Gold Layer (Mart)}
The Gold layer constitutes the ``Data Mart'' of the lakehouse architecture. While the Silver layer focuses on data integrity and normalization, the Gold layer is organized around specific business problems. Tables in this layer are denormalized, pre-aggregated, and enriched with derived metrics to ensure low-latency performance for Business Intelligence (BI) tools and final reporting.

The schema consists of three distinct analytical tables, each corresponding to a specific use case:

\begin{itemize}
    \item \textbf{\texttt{gold\_typical\_day\_patterns}:} 
    This table supports temporal analysis (Use Case 1). It reduces billions of raw trip records into a lightweight set of hourly profiles.
    \begin{itemize}
        \item \textbf{Aggregation:} Data is grouped by \texttt{cluster\_id} (representing patterns like ``Weekday'' or ``Holiday'') and \texttt{hour}.
        \item \textbf{Metrics:} It stores the \texttt{avg\_trips} to allow for immediate plotting of demand curves without recalculating the underlying raw data.
    \end{itemize}

    \item \textbf{\texttt{gold\_infrastructure\_gaps}:} 
    Designed for the Gravity Model analysis (Use Case 2), this table stores Origin-Destination (OD) pairs. Unlike standard OD matrices, this table is enriched with socio-economic context.
    \begin{itemize}
        \item \textbf{Contextual Data:} Includes \texttt{total\_population} (Origin) and \texttt{rent} (Destination proxy for economic attraction) alongside the spatial impedance \texttt{dist\_km}.
        \item \textbf{Key KPI:} The \texttt{mismatch\_ratio} is the critical derivative, quantifying the gap between theoretical potential and actual observed trips.
    \end{itemize}

    \item \textbf{\texttt{gold\_zone\_functional\_classification}:} 
    This table provides a semantic layer for the spatial zones (Use Case 3). Instead of raw ID numbers, it assigns functional roles to municipalities.
    \begin{itemize}
        \item \textbf{Flow Ratios:} It persists calculated indices such as \texttt{net\_flow\_ratio} (balance of trade in trips) and \texttt{retention\_rate} (local self-sufficiency).
        \item \textbf{Categorization:} The \texttt{functional\_label} column stores the final human-readable classification (e.g., ``Bedroom Community''), simplifying downstream map visualizations.
    \end{itemize}
\end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{imgs/Gold_Schema_diagramS4} 
%     \caption{Schema design of the Gold Layer. These tables represent the final analytical products, denormalized and enriched with calculated metrics (e.g., mismatch ratios, functional labels) ready for visualization.}
%     \label{fig:gold_layer_diagram}
% \end{figure}




\section{Implementation Methodology}
The operational logic of the Data Lakehouse is implemented through a modular, automated workflow orchestrated by Apache Airflow \cite{airflow}. The system adopts a decoupled architecture where distinct Directed Acyclic Graphs (DAGs) handle specific stages of the data lifecycle based on their update frequency and functional scope.

This section details the engineering strategies employed to ensure scalable data ingestion, atomic processing of daily batches, and the automated generation of analytical models. The implementation prioritizes robustness through strict transaction management, task isolation, and the separation of heavy computational ELT from on-demand reporting queries.

\subsection{Orchestration Strategy}
To manage the dependencies between the Bronze, Silver, and Gold layers efficiently, the system is architected into a multi-DAG ecosystem. This design allows for independent scaling and maintenance of static dimensions versus high-volume mobility facts.

\begin{enumerate} 
    \item \textbf{Infrastructure \& Dimensions DAG:} Handles the ingestion of low-frequency static data (INE demographics, MITMA zoning, Calendars). It establishes the schema foundations and Bronze/Silver dimension tables.
    
    \item \textbf{Mobility Ingestion DAG:} A parameterized worker pipeline designed for high-volume processing. It accepts date ranges to ingest, clean, and transform the daily mobility files (MITMA OD Matrices) from Bronze to Silver using atomic tasks.
    
    \item \textbf{Gold Generations and consultings DAGs:} These pipelines (DAGs 31–33) consolidate the logic into unified flows. Triggered on-demand with custom parameters (e.g., specific spatial polygons or date ranges), they first materialize the aggregated Gold tables using DuckDB and immediately generate the final visual assets (Kepler.gl maps, Matplotlib charts), uploading them to S3 in a single execution context.
\end{enumerate}

\subsection{Infrastructure \& Dimensions (DAG 1)}
The foundational pipeline, \texttt{infrastructure\_and\_dimensions}, is responsible for establishing the Lakehouse schema and ingesting low-velocity reference data. Unlike the daily mobility processing, this DAG is designed to be executed on-demand or annually, as zoning definitions and census statistics change infrequently.

Before data movement begins, the pipeline ensures the environment is consistent. The task called \texttt{create\_schemas} connects to the Neon catalog to verify the existence of the logical namespaces (\textit{bronze, silver, gold}). Simultaneously, the \texttt{create\_stats\_table} task initializes the \texttt{data\_quality\_log} table in the Silver layer, which is a prerequisite for audit logging in all subsequent pipelines.

To optimize total execution time, all external data extraction tasks are configured to run in parallel. These tasks use the Airflow \textit{@task} notation arguments to specify 3 retries with a gap of 1 minute in case they fail. This phase handles heterogenous data formats through specialized ingestion logic:

\begin{itemize}
    \item \textbf{Spatial Ingestion:} The \texttt{br\_ingest\_geo\_data} task utilizes DuckDB's \texttt{spatial} extension. It performs an HTTP HEAD request to validate the existence of the remote Shapefiles (.shp, .shx, .dbf) before executing \texttt{ST\_Read} to ingest the vector geometry directly into the Bronze layer.
    \item \textbf{Statistical Ingestion:} Tasks targeting INE data (e.g., \texttt{br\_ingest\_ine\_rent}) implement specific handling for legacy formats, forcing \texttt{ISO-8859-1} encoding and tab separators to prevent parsing errors common in Spanish government datasets.
    \item \textbf{Dictionaries:} Tasks for zoning and mapping ingest standard CSVs via DuckDB's \texttt{read\_csv\_auto}, adding audit columns (\texttt{source\_url}, \texttt{ingestion\_timestamp}) on the fly.
\end{itemize}

The dependency structure in the Silver layer follows a \textit{Funnel} pattern (Figure \ref{fig:dag1_flow}). The system enforces a hard dependency on the creation of the master dimension table before populating satellite metric tables.

\begin{enumerate}
    \item \textbf{Convergence Point (\texttt{dim\_zones}):} This task acts as the synchronization barrier. It waits for the completion of three specific Bronze tasks—Geometry, Zoning names, and INE Mapping. Once available, it performs a 3-way join to construct the master \texttt{dim\_zones} table, generating the surrogate \texttt{zone\_id}.
    \item \textbf{Satellite Expansion:} Once the master dimension exists, the downstream tasks called \texttt{metric\_population}, \texttt{metric\_ine\_rent}, \texttt{dim\_zone\_distance\_matrix} and \texttt{dim\_zone\_holidays} trigger in parallel. Each task performs a join between its respective raw Bronze source and the newly created \texttt{dim\_zones} to assign the correct surrogate keys to the statistical data.
\end{enumerate}

\begin{figure}[H]
    \centering
    % Placeholder for the Graph View screenshot of DAG 1
    \includegraphics[width=1\textwidth]{imgs/infrastructure_and_dimensions-DAG}
    \caption{Dependency graph of the Infrastructure DAG.}
    \label{fig:dag1_flow}
\end{figure}



\subsection{Mobility Ingestion (DAG 2)}
The second pipeline, \texttt{mobility\_ingestion}, constitutes the core factory of the Lakehouse. It is responsible for the daily ingestion and transformation of the high-volume Origin-Destination matrices. Given that the source data is partitioned by day (one compressed CSV per day), this DAG utilizes Airflow's Dynamic Task Mapping to scale horizontally, processing date ranges supplied via run-time parameters.

Unlike the static infrastructure DAG, this workflow begins by generating a execution plan based on user input. The \texttt{generate\_date\_list} task parses the \texttt{start\_date} and \texttt{end\_date} parameters to produce a list of strings (e.g., \texttt{['20230101', '20230102', ...]}).

Simultaneously, the pipeline executes idempotent initialization tasks (\texttt{ensure\_br\_mobility...} and \texttt{ensure\_sl\_mobility...}). These tasks utilize \texttt{CREATE TABLE IF NOT EXISTS} logic to prepare the partitioning structures in the Bronze and Silver layers, ensuring that parallel workers do not encounter race conditions when attempting to write to non-existent tables.

The \texttt{br\_process\_single\_day} task is mapped over the generated date list. Each instance handles a specific day, implementing robust checks to handle the known inconsistencies in the MITMA source server:

\begin{itemize}
    \item \textbf{Pre-flight Validation:} Before attempting extraction, the task performs an HTTP \texttt{HEAD} request to the calculated URL. If the source file is missing (a common occurrence for specific dates in late 2023), the task logs a warning and skips execution gracefully rather than failing the pipeline.
    \item \textbf{Throttling and Retries:} To prevent overwhelming the source server or saturating local memory, concurrency is strictly limited via \texttt{max\_active\_tis\_per\_dag=2}. Network resilience is enforced with a retry policy of 5 attempts with a 30-second delay.
    \item \textbf{Streaming Ingestion:} Valid files are streamed directly from the remote server into the \texttt{mobility\_data} table using DuckDB's \texttt{read\_csv\_auto}, bypassing local disk storage.
\end{itemize}

The transformation phase (\texttt{sl\_process\_single\_day}) mirrors the mapping of the ingestion phase but introduces a strict cross-layer dependency: Silver processing for the batch only begins once Bronze ingestion is confirmed.

Inside this task, raw text data is transformed into the analytical schema. This involves joining the raw mobility records with the \texttt{dim\_zones} table (created in DAG 1) to resolve MITMA/INE codes into internal surrogate keys. This effectively acts as a foreign key validation constraint; records with invalid zone codes are implicitly filtered during the inner join, ensuring referential integrity in the Silver layer.

\begin{figure}[H]
    \centering
    % Placeholder for the Graph View screenshot of DAG 2
    \includegraphics[width=1\textwidth]{imgs/mobility_ingestion-DAG}
    \caption{The Mobility Ingestion DAG utilizing Dynamic Task Mapping.}
    \label{fig:dag2_flow}
\end{figure}


\subsection{Gold Generations and consultings (DAGs 3)}
The analytical layer is materialized through three dedicated Airflow pipelines. Each DAG is designed to answer a specific business question by transforming Silver data into aggregated Gold tables and subsequently generating visual reports stored in S3.

\begin{itemize}
    \item \textbf{DAG 31: Typical Day Clustering (\texttt{31\_bq1\_clustering})} \\
    This pipeline addresses the characterization of mobility patterns via unsupervised learning.
    \begin{itemize}
        \item \textbf{Transformation:} It aggregates mobility data to extract hourly profiles and applies K-Means clustering ($k=3$) using Scikit-Learn to categorize days. The pipeline uses two tables: \texttt{gold.typical\_day\_patterns} and \texttt{gold.typical\_od\_matrices} for spatial distribution.
        \item \textbf{Output:} The DAG produces a comprehensive reporting suite in the results directory, including a static Matplotlib chart for temporal profiles, an interactive Plotly HTML heatmap for detailed OD analysis, and a summary Markdown report.
    \end{itemize}

    \begin{figure}[H]
        \centering
        % First Image (Line Chart)
        \begin{subfigure}[b]{0.8\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/bq1-1}
            \caption{Temporal mobility profiles (Clustering results)}
            \label{fig:bq1_profiles}
        \end{subfigure}
        
        \vspace{0.5cm} % Add some vertical space between them

        % Second Image (Heatmap)
        \begin{subfigure}[b]{1\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/bq1-2}
            \caption{Interactive Origin-Destination Heatmap}
            \label{fig:bq1_heatmap}
        \end{subfigure}
        
        \caption{Visualization results for BQ1: Typical Daily Mobility Patterns.}
        \label{fig:bq1_results}
    \end{figure}

    \item \textbf{DAG 32: Infrastructure Gap Analysis (\texttt{32\_bq2\_gaps})} \\
    This pipeline implements a Gravity Model to quantify the disparity between theoretical potential and actual mobility.
    \begin{itemize}
        \item \textbf{Transformation:} It executes a SQL batch job to create \texttt{gold.infrastructure\_gaps} table. By integrating population, income, and distance metrics, it calculates a theoretical ``Gravity Score'' ($P \cdot E / d^2$) and compares it with actual flows to derive a \texttt{mismatch\_ratio} for every connection.
        \item \textbf{Output:} The DAG utilizes Kepler.gl to generate two interactive geospatial reports: a \textit{Service Level Ranking} (bubble map) identifying underserved zones and a \textit{Mobility Gaps} visualization (arc map) showing specific connections where demand exceeds supply, alongside a summary Markdown report.
    \end{itemize}

    \begin{figure}[H]
        \centering
        % First Image (Ranking Map)
        \begin{subfigure}[b]{1.0\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/bq2-1}
            \caption{Zone Service Level Ranking}
            \label{fig:bq2_ranking}
        \end{subfigure}

        \vspace{0.5cm}

        % Second Image (Arc Map)
        \begin{subfigure}[b]{1.0\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/bq2-2}
            \caption{Inter-urban Mobility Gaps (Arc Map)}
            \label{fig:bq2_arcs}
        \end{subfigure}
        
        \caption{Visualization results for BQ2: Infrastructure Gaps and Mobility Potential.}
        \label{fig:bq2_results}
    \end{figure}

    \item \textbf{DAG 33: Functional Classification (\texttt{33\_bq3\_functional\_classification})} \\
    This pipeline categorizes municipalities based on their role in the metropolitan network (e.g., residential vs. commercial hubs).
    \begin{itemize}
        \item \textbf{Transformation:} It calculates critical flow metrics (inflow/outflow ratios and retention rates) via a SQL batch job. A logic-based decision tree assigns a functional label (e.g., ``Bedroom Community'', ``Activity Hub'') to each zone, persisting the results in \texttt{gold.zone\_functional\_classification}.
        \item \textbf{Output:} The DAG generates a Kepler.gl Choropleth map that colors zone geometries by their functional role and overlays an activity intensity layer (bubbles). This interactive dashboard, accompanied by a Markdown summary, allows urban planners to visualize the structural hierarchy of the region.
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{imgs/bq3-1}
        \caption{BQ3 Results: Functional Zone Classification based on net flows and retention rates.}
        \label{fig:bq3_results}
    \end{figure}

\end{itemize}









\section{Results and Use Cases}

\subsection{Use Case 1: Typical Mobility Patterns and Profiling}

The first business objective focuses on identifying and characterizing standard mobility behaviors within the reference year (2023). Instead of relying on static calendar rules (e.g., assuming all Mondays are identical), we implemented an unsupervised machine learning approach in the Gold layer to discover intrinsic daily patterns—distinguishing between standard workdays, holidays, and transition days based on actual hourly flow shapes.

\paragraph{Implementation Approach\\}
The solution is orchestrated via an Airflow DAG (\texttt{31\_bq1\_clustering}). The pipeline employs a hybrid processing strategy: using DuckDB for heavy aggregations and Python (Scikit-Learn) for in-memory clustering. The logic follows a four-step process:

\begin{enumerate}
    \item \textbf{Feature Extraction:} We aggregate the granular records from \texttt{silver.fact\_mobility} into hourly profiles. To ensure the clustering algorithm focuses on the \textit{shape} of the day (e.g., morning/evening peaks) rather than absolute volume, we normalize the data vector for every date $d$:
    \begin{equation}
        V_{d, h} = \frac{T_{d, h}}{\sum_{h=0}^{23} T_{d, h}}
    \end{equation}
    where $T_{d, h}$ is the total trips on day $d$ at hour $h$.
    
    \item \textbf{Unsupervised Classification (K-Means):} These normalized vectors are fed into a K-Means algorithm (configured with $k=3$ clusters). This automatically classifies every date in the analyzed period into distinct profiles (e.g., ``Standard Workday,'' ``Weekend/Leisure,'' or ``Holiday'').
    
    \item \textbf{Pattern Projection:} Once dates are labeled, the system projects these labels back onto the massive dataset. Using DuckDB's SQL engine to handle the scale, we compute the average Origin-Destination (OD) matrix for each identified cluster, materializing the results in \texttt{gold.typical\_od\_matrices}.
\end{enumerate}

\paragraph{Visualization and Reporting\\}
To allow transport experts to validate these patterns, the pipeline automatically generates two types of analytical outputs stored in the results bucket:

\begin{itemize}
    \item \textbf{Temporal Profiles (Static):} A line chart visualization (via Matplotlib) plotting the centroids of the identified clusters. This clearly highlights the temporal differences between profiles, such as the steep morning commute peak in workdays versus the smoother midday curve typical of weekends.
    \item \textbf{Interactive OD Heatmaps:} An HTML report generated using Plotly. This tool allows analysts to interactively select a specific cluster and hour (e.g., ``Cluster 0 at 08:00 AM'') to visualize the spatial distribution of trips. The heatmap uses a logarithmic color scale to handle the high variance in trip counts between major urban centers and rural peripheries.
\end{itemize}



\subsection{Use Case 2: Infrastructure Gap Analysis}

The second business objective addresses the identification of zones where transport infrastructure fails to meet potential demand. To solve this, we implemented a \textbf{Gravity Model} in the Gold layer, comparing the theoretical interaction potential between municipalities against the actual mobility flows recorded by MITMA.

\paragraph{Implementation Approach\\}
The solution is orchestrated via an Airflow DAG (\texttt{32\_bq2\_gaps}). The logic follows a three-step process:

\begin{enumerate}
    \item \textbf{Theoretical Modeling:} We calculate a ``Gravity Score'' for every origin-destination pair using the classic impedance formula:
    % [GUIDE]: Explain the Gravity Model implementation.
    \begin{equation}
        T_{ij} = k \cdot \frac{P_i \cdot E_j}{d_{ij}^2}
    \end{equation}
    where $P_i$ is the population at the origin (from \texttt{metric\_population}), $E_j$ is the economic attractiveness (using \texttt{income\_per\_capita} from \texttt{metric\_ine\_rent} as a proxy), and $d_{ij}$ is the distance from our pre-computed \texttt{dim\_zone\_distances}.
    
    \item \textbf{Dynamic Normalization ($k$-factor):} To make the theoretical score comparable to physical trips, the code dynamically calculates a normalization factor ($k$) by computing the ratio between the total sum of actual MITMA trips and the total sum of gravity scores for the period.
    
    \item \textbf{Gap Detection:} The final metric, \texttt{mismatch\_ratio}, is derived by dividing actual trips by the normalized potential. A ratio significantly below 1.0 indicates a connection where mobility is lower than demographic and economic factors suggest, highlighting a potential infrastructure deficit.
\end{enumerate}

\paragraph{Spatial Analytics and Visualization\\}
The pipeline includes a spatial filtering mechanism allowing users to run this analysis on specific regions (e.g., passing a WKT Polygon for the ``Comunidad de Madrid''). Using DuckDB's spatial extension (\texttt{ST\_Intersects}), the system aggregates the results to generate two key visualizations using Kepler.gl:
\begin{itemize}
    \item \textbf{Service Level Ranking:} A bubble map where zones are sized by importance ($P \times E$) and colored by their average service level, instantly highlighting underserved economic hubs.
    \item \textbf{Mobility Arcs:} A flow map visualizing specific origin-destination connections that show the highest disparity between potential and actual traffic.
\end{itemize}


\subsection{Use Case 3: Functional Zoning Classification}

The third analytical product moves beyond simple traffic counting to characterize the \textit{functional role} of each municipality within the metropolitan network. By analyzing the directionality and retention of mobility flows, this use case automatically classifies zones into semantic categories (e.g., ``Bedroom Communities'' vs. ``Activity Hubs''), aiding urban planners in understanding regional dependencies.

\paragraph{Methodology and Classification Logic\\}
The implementation is encapsulated in the DAG \texttt{33\_bq3\_functional\_classification}. The logic aggregates the granular records from \texttt{fact\_mobility} into three distinct flow types per zone: \textbf{Inflow} (trips entering), \textbf{Outflow} (trips leaving), and \textbf{Internal} (trips starting and ending in the same zone). 

Using these aggregates, two key indices are calculated:
\begin{enumerate}
    \item \textbf{Net Flow Ratio:} Measures the balance of attraction.
    \[
    R_{\text{net}} = \frac{\text{Inflow} - \text{Outflow}}{\text{Inflow} + \text{Outflow}}
    \]
    \item \textbf{Retention Rate:} Measures the zone's self-sufficiency.
    \[
    R_{\text{retention}} = \frac{\text{Internal}}{\text{Outflow} + \text{Internal}}
    \]
\end{enumerate}

Based on these metrics, the SQL pipeline applies a hierarchical decision tree to assign a \texttt{functional\_label} to each municipality:

\begin{itemize}
    \item \textbf{Self-Sustaining Cell:} Zones with high retention ($R_{\text{retention}} > 0.20$), indicating a municipality that satisfies most of its residents' needs locally.
    \item \textbf{Activity Hub (Importer):} Zones with a positive net flow ($R_{\text{net}} > 0$), typically business districts or commercial centers attracting commuters.
    \item \textbf{Bedroom Community (Exporter):} Zones with a negative net flow ($R_{\text{net}} < 0$), characterizing residential areas where the population commutes out for work.
    \item \textbf{Balanced / Transit Zone:} Areas with neutral flow ratios acting as connectors.
\end{itemize}

\paragraph{Visualization Strategy\\}
The results are materialized in a Kepler.gl dashboard combining two visual layers to provide context:
\begin{itemize}
    \item \textbf{Polygon Layer (Choropleth):} Displays the administrative boundaries colored by their \texttt{functional\_label}. This reveals regional clusters (e.g., a ring of ``Bedroom Communities'' surrounding a central ``Activity Hub'').
    \item \textbf{Centroid Layer:} Displays bubbles sized by \texttt{total\_activity} (Sum of In + Out + Internal), allowing users to distinguish between major metropolitan centers and smaller rural nodes regardless of their functional classification.
\end{itemize}

\section{Discussion}
The implementation of this 3-tier Data Lakehouse represents a significant advancement over traditional file-based workflows for mobility analysis. By transitioning from ad-hoc processing of CSV files to a structured Lakehouse architecture, the system achieves three critical operational goals:

\begin{itemize}
    \item \textbf{Democratization of Big Data:} The architecture successfully decouples the complex data engineering lifecycle (ELT) from the analytical consumption. Transport experts, who previously required specialized programming skills to parse terabytes of daily CSVs, can now access the Gold layer using standard SQL queries or BI tools. The pre-computation of metrics like the \textit{gravity score} or \textit{functional labels} reduces the time-to-insight from days to seconds.
    
    \item \textbf{Cost-Effective Scalability:} The separation of storage (S3) and compute (DuckDB) allows the infrastructure to handle years of historical data with minimal overhead. Unlike coupled Data Warehouses, where costs accrue 24/7, this architecture only incurs compute costs during active transformation or querying. The use of ephemeral AWS Batch instances demonstrates that high-performance analytics can be achieved on a limited budget.
    
    \item \textbf{Contextual Enrichment:} The true value of the system lies in the integration achieved in the Silver layer. By enriching raw mobility flows with socio-economic context (INE data) and spatial distances, the system transforms raw trip counts into actionable accessibility metrics. This was demonstrated in Use Case 2, where the combination of population, rent, and distance revealed infrastructure gaps that raw mobility matrices alone could not identify.
\end{itemize}


\section{Conclusions and Limitations}

\subsection{Conclusions}
This project successfully designed and deployed a robust Data Lakehouse capable of ingesting, cleaning, and analyzing Spanish mobility data at scale. The 3-tier architecture ensures data governance, with a raw Bronze layer for auditability, a refined Silver layer for integration, and a business-ready Gold layer.

The implementation of the three use cases demonstrates the system's versatility: from identifying temporal patterns via Unsupervised Learning (Clustering) to solving domain-specific physics problems (Gravity Models) and geospatial classification. By leveraging DuckDB and DuckLake, the solution proves that ACID-compliant, high-performance analytics are achievable using open-source tools without the need for expensive enterprise licenses.

\subsection{Limitations}
Despite the success of the architecture, several limitations were identified during implementation, concerning both data quality and infrastructure constraints:

\begin{itemize}
    \item \textbf{Source Data Anomalies:} As noted in the MITMA technical documentation, the source datasets contain significant gaps during October and November 2023 due to mobile network incidents. While our pipeline's robust ingestion logic prevents these missing files from crashing the system, the resulting analytics for Q4 2023 require statistical imputation to be fully representative.
    
    \item \textbf{Infrastructure Constraints (Neon Catalog):} A critical bottleneck was observed regarding the metadata catalog management. We utilized the Neon Serverless Postgres Free Tier as the central catalog for DuckLake. It was observed that this tier enforces strict connection lifecycle limits. During the processing of large historical backfills, the catalog would frequently terminate the connection due to timeout limits on idle transactions, even while the compute engine (DuckDB) was actively writing data to S3. This necessitated the fragmentation of batch loads into smaller temporal windows to ensure transaction commit stability.
    
    \item \textbf{Vertical Scaling vs. Horizontal Distribution:} A structural limitation of using DuckDB is its single-node architecture. Unlike distributed engines like Apache Spark which scale horizontally (adding more machines to a cluster), DuckDB scales vertically (requiring a larger single machine). While AWS Batch allows us to request high-memory instances (e.g., 64GB or 128GB RAM), there is a theoretical ceiling. If a specific join or aggregation requires more memory than the largest available single EC2 instance, the query would fail or suffer severe performance degradation due to disk spilling.
    
    \item \textbf{Latency and Real-Time Analysis:} The current architecture is designed for $T+1$ (Daily) batch processing. Due to the provisioning time of AWS Batch instances and the file-based ingestion nature of the source, this system is not suitable for real-time traffic monitoring. Use cases requiring sub-minute latency (e.g., immediate accident detection or live congestion management) would require a Streaming architecture (e.g., Kafka + Flink) rather than the current Lakehouse approach.
\end{itemize}





% --- BIBLIOGRAPHY ---
\begin{thebibliography}{9}

\bibitem{mitma}
Spanish Ministry of Transport (MITMA).
\textit{Open Data Mobility}.
Available at: \url{https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad}
(Accessed on 4 January 2026).

\bibitem{ine}
Spanish National Statistics Institute (INE).
Available at: \url{https://www.ine.es/}
(Accessed on 4 January 2026).

\bibitem{cnig}
Centro Nacional de Información Geográfica (CNIG).
Available at: \url{https://www.ign.es/}
(Accessed on 4 January 2026).

\bibitem{madrid_data}
Ayuntamiento de Madrid.
\textit{Portal de Datos Abiertos}.
Available at: \url{https://datos.madrid.es/}
(Accessed on 4 January 2026).

\bibitem{duckdb}
DuckDB Documentation.
Available at: \url{https://duckdb.org/}
(Accessed on 4 January 2026).

\bibitem{aws_batch}
Amazon Web Service Batch.
\textit{AWS Batch Documentation}.
Available at: \url{https://aws.amazon.com/es/batch/}
(Accessed on 4 January 2026).

\bibitem{ducklake}
DuckLake.
\textit{DuckLake Documentation}.
Available at: \url{https://ducklake.select/docs/stable/}
(Accessed on 4 January 2026).

\bibitem{airflow}
Apache Software Foundation.
\textit{Apache Airflow Documentation}.
Available at: \url{https://airflow.apache.org/}
(Accessed on 4 January 2026).

\end{thebibliography}

\end{document}
