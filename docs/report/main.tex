\documentclass[11pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{graphicx}     % For images
\usepackage{booktabs}     % For nice tables
\usepackage{hyperref}     % For links (repo URL)
\usepackage{listings}     % For code snippets
\usepackage{xcolor}       % For code coloring
\usepackage{float}        % For figure placement
\usepackage{titlesec}     % Custom section titles
\usepackage{parskip}      % Space between paragraphs
\usepackage{amsmath}
\usepackage{pdflscape}


% --- CODE SNIPPET STYLING ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- METADATA ---
\title{\textbf{Building a Scalable 3-Tier Data Lakehouse for Mobility Analysis in Spain}\\ Big Data Engineering Project \\}
\author{
    \textbf{Joan Sánchez Verdú} \and \textbf{María López Hernández} \and \textbf{Fernando Blanco Membrives} \\    
    % \textit{Link to Code Repository:}\\ \textit{\url{https://github.com/joanstudyai-code/MUCEIM-BDET-Project.git}}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    \noindent 
    \textbf{Abstract.} This paper presents the design and implementation of a scalable 3-tier Data Lakehouse architecture tailored for the analysis of large-scale mobility data in Spain. Leveraging open data from the Spanish Ministry of Transport (MITMA) and the National Statistics Institute (INE), the system integrates high-volume Origin-Destination (OD) matrices with demographic and economic indicators. The infrastructure utilizes DuckDB for efficient analytical processing and DuckLake for ACID-compliant storage, orchestrated via Apache Airflow. We demonstrate the system's utility through two key use cases: characterizing typical daily mobility patterns and identifying transport infrastructure gaps using gravity models. The result is a robust, cost-effective platform enabling transport experts to derive data-driven insights for urban planning.
\end{abstract}

% --- SECTIONS ---

\section{Introduction}

Mobility analysis is critical for modern urban planning...

\subsection{Objectives}
The primary objective of this work was to construct a robust data infrastructure...
\begin{itemize}
    \item Build a scalable Lakehouse for mobility and economic data.
    \item Implement ELT processes across Bronze, Silver, and Gold tiers.
    \item Demonstrate analytical value through specific mobility use cases.
\end{itemize}










\section{Data Sources}
The data lakehouse architecture is designed to ingest and integrate public domain data exclusively, ensuring reproducibility and open access. The following subsections detail the specific datasets and sources selected to build the information system.

\subsection{MITMA Open Data}
The core mobility dataset is sourced from the Spanish Ministry of Transport, Mobility and Urban Agenda (MITMA \cite{mitma}), specifically the \textit{Estudios Básicos de Movilidad}. This dataset provides high-resolution insights into the daily movements of residents derived from mobile network big data.
From the MITMA the following files have been used:
\begin{itemize}
    \item \textbf{\textit{(202301-202312)\_Viajes\_municipios.tar}:} These files, located in the MITMA web at \textit{estudios\_basicos/por-municipios/viajes/meses-completos/}, contain the information of the mobility from an origin to a destination municipality, per day, per hour, for the year 2023.
    \item \textbf{\textit{nombres\_municipios.csv}:} This file, located in the MITMA web at \textit{zonificacion/zonificacion\_municipios/}, contains the name of each municipality.
    \item \textbf{\textit{relacion\_ine\_zonificacionMitma.csv}:} This file, located in the MITMA web at \textit{zonificacion/}, cointains the mapping between the MITMA codes and the INE codes for the municipalities.
    \item  \textbf{\textit{poblacion.csv}:} This file, located in the MITMA web at \textit{zonificacion/}, cointains the population of each municipality per year.
    \item \textbf{\textit{zonificacion\_municipios.shp}:} This ESRI Shapefile (along with its \textit{.dbf}, \textit{.shx}, and \textit{.prj} components), located in the MITMA web at \textit{zonificacion/zonificacion\_municipios/}, contains the official geospatial geometries (polygons) for the municipal study zones.
\end{itemize}

\subsection{INE Demographics and Economics}
To contextualize mobility flows and enable the gravity model analysis, mobility data is enriched with official statistics from the Spanish National Statistics Institute (INE \cite{ine}).
The statistics used from INE are the following:
\begin{itemize}
    \item \textbf{\textit{ine\_rent\_municipalities.csv}:} This file, located in the INE web at \textit{\url{https://www.ine.es/jaxiT3/files/t/es/csv_bd/}}, contains the rent information per year and municipality.
\end{itemize}

\subsection{Open Data}
To categorize mobility patterns by day type (e.g., distinguishing standard workdays from holidays), specific calendar data is integrated from Open Data portals.

\begin{itemize}
    \item \textbf{\textit{calendario\_laboral.csv}:} This file, sourced from the Madrid City Council Open Data Portal (\textit{Datos Abiertos Madrid} \cite{madrid_data}) at \textit{\url{https://datos.madrid.es/}}, contains the official working calendar. It provides the classification of dates (working days, weekends, and holidays) from 2013 to 2026 (included).
\end{itemize}








\section{Lakehouse Architecture}
The system follows the Medallion Architecture pattern, decoupled into storage and compute layers to ensure scalability.

\subsection{Technology Stack}
The platform is built upon a modular, open-source stack designed to decouple compute from storage, allowing for independent scaling and cost efficiency. The architecture leverages the following core components:

\begin{itemize}
    \item \textbf{Compute Engine (DuckDB \cite{duckdb}):} Selected for its high-performance, vectorized execution engine optimized for OLAP workloads. DuckDB serves as the primary processing unit, running in-process for orchestration tasks while being deployed via containerized environments for heavy transformations. This approach significantly reduces infrastructure overhead while maintaining the ability to process datasets larger than RAM through out-of-core execution.

    \item \textbf{Elastic Compute (AWS Batch \cite{aws_batch}):} To handle high-volume data and computationally intensive joins, the platform offloads heavy SQL workloads to \textit{AWS Batch}. This provides an ephemeral, cloud-native compute layer that automatically scales memory-optimized instances (e.g., EC2 R4 and R5 family) on demand. By executing DuckDB in the same cloud region as the data storage, the system minimizes network latency and maximizes throughput, while utilizing \textit{Spot Instances} to achieve up to a 90\% reduction in compute costs.
    
    \item \textbf{Lakehouse Management (DuckLake \cite{ducklake}):} A specialized extension that brings Data Lakehouse capabilities to the ecosystem. It provides ACID transaction support (Atomicity, Consistency, Isolation, Durability) and snapshot isolation. This ensures that long-running ingestion tasks do not block analytical queries and prevents data corruption during partial write failures.
    \item \textbf{Storage - Cloud Infrastructure (Neon + S3):} The system adopts a cloud-native architecture that physically separates data from metadata:
    \begin{itemize}
        \item \textbf{Data Plane (AWS S3):} Stores the actual physical files (Parquet and CSV). S3 provides durability and scalable object storage for the Bronze, Silver, and Gold layers.
        \item \textbf{Control Plane (Neon Postgres):} A serverless PostgreSQL instance acts as the centralized catalog and metadata store. It manages table definitions, schemas, and transaction locking, enabling safe concurrent writes to S3 from multiple Airflow workers.
    \end{itemize}
    \item \textbf{Orchestration (Apache Airflow \cite{airflow}):} Manages the end-to-end lifecycle of data pipelines. Airflow allows for the definition of Directed Acyclic Graphs (DAGs) to handle complex task dependencies, backfilling strategies for historical data, and granular retries in case of network or source failures.
\end{itemize}

\subsection{Data Layering Strategy}
The data pipeline is structured following the standard ``Medallion'' architecture pattern (Multi-hop), which organizes data quality into progressive stages. This design ensures that raw data is preserved for auditability while downstream layers provide cleaned and enriched datasets optimized for analytical consumption. The architecture consists of three distinct zones, each with a specific purpose and schema design.

\subsubsection{Bronze Layer (Raw)}
The Bronze layer acts as the landing zone (Staging Area) for all external data. Its primary function is to capture data from source systems in its native format with minimal modification, ensuring an immutable record of the original input.

In this implementation (Figure \ref{fig:bronze_layer_diagram}), a distinct table is instantiated for each source entity to maintain data isolation. To ensure pipeline robustness and prevent ingestion failures due to type mismatches, tabular data columns (such as CSV files) are initially cast as \texttt{VARCHAR}. A notable exception is the geographic data: by utilizing DuckDB's \texttt{spatial} extension, the ESRI Shapefile components are ingested via the \texttt{ST\_Read} function. This allows the system to capture municipal boundaries directly as native \texttt{GEOMETRY} objects, preserving spatial precision from the source.

Additionally, the ingestion process is configured to ignore malformed records, ensuring that isolated errors do not halt the entire pipeline. To enhance data governance and auditability, metadata columns are appended to every record, including \texttt{ingestion\_timestamp} and \texttt{source\_url}. Finally, to optimize storage and performance, the high-volume mobility data is physically partitioned by date (\texttt{fecha}), enabling efficient batch processing.

\begin{landscape}
\begin{figure}[p]
    \centering
    \includegraphics[width=1.5\textwidth]{imgs/Bronze_Schema_diagramS4}
    
    \caption{Entity Diagram of the Bronze Layer. One table for each file.}
    \label{fig:bronze_layer_diagram}
\end{figure}
\end{landscape}

\subsubsection{Silver Layer (Cleaned \& Integrated)}
The Silver layer represents the refined, enterprise-wide view of the data. In this stage, data undergoes validation, cleaning, standardization, and enrichment processes to transform raw inputs into structured, trustworthy tables. The schema design (Figure \ref{fig:silver_layer_diagram}) follows a Star Schema approach, establishing a central fact table for mobility flows linked to surrounding dimension tables for spatial and socio-economic context. All tables include a \texttt{TIMESTAMP WITH TIME ZONE} column \texttt{processed\_at} for auditability.

\paragraph{Dimension Modeling and Spatial Integration (dim\_zones)\\}
The foundation of the Silver layer is the creation of a unified spatial dimension, \texttt{dim\_zones}. This table integrates the descriptive zoning metadata (MITMA codes, INE codes, and municipality names) with the geospatial vector data ingested in the Bronze layer.
\begin{itemize}
    \item \textbf{Spatial Integration:} The \texttt{dim\_zones} table leverages the native \texttt{GEOMETRY} objects already parsed in the Bronze layer. By joining the municipal geometries with the MITMA-INE crosswalk (\texttt{mapping\_ine\_mitma}), the Silver layer establishes a clean, spatially-enabled dimension.
    \item \textbf{Surrogate Keys:} A unique integer identifier (\texttt{zone\_id}) is generated for each municipality. This decouples downstream analytics from changes in external string codes (MITMA/INE IDs) and improves join performance.
\end{itemize}


\paragraph{Spatial Connectivity (Distance Matrix)\\}
To support spatial network analysis and distance-based filtering, a derivative dimension table, \texttt{dim\_zone\_distances}, is established. This table functions as a pre-computed distance matrix representing the relationships between municipality pairs. By calculating the distance between the centroids of the geometries stored in \texttt{dim\_zones}, the system avoids computationally expensive geospatial operations during query time.
\begin{itemize}
    \item \textbf{Zone Linkage:} The table maps relationships using \texttt{origin\_zone\_id} and \texttt{destination\_zone\_id}, which act as foreign keys referencing the central \texttt{dim\_zones} table.
    \item \textbf{Metric Storage:} The \texttt{dist\_km} column stores the distance in kilometers as a \texttt{DOUBLE} precision value, allowing for precise downstream analysis of trip lengths and gravity models.
\end{itemize}


\paragraph{Socio-Economic Metrics and Filtering\\}
Complementary metric tables are generated by cleaning raw CSVs. For the \texttt{metric\_population}, data is cast to \texttt{BIGINT} while removing metadata headers. 

For the \texttt{metric\_ine\_rent} table, specific business logic is applied to the INE source file to resolve granularity mismatches. The pipeline filters the dataset to isolate the ``Average Net Income per Person'' indicator and explicitly excludes district-level or census-section entries (filtering out rows where \texttt{Distritos} or \texttt{Secciones} are populated). This ensures that the resulting economic metrics align perfectly with the municipal granularity of the \texttt{dim\_zones} table.

\paragraph{Temporal Context (Holidays)\\}
To support temporal analysis, a \texttt{dim\_zone\_holidays} table is constructed. This process involves filtering the raw working calendar for events classified as ``National Holidays'' and cross-referencing them with the zones. This dimension allows the Gold layer to accurately distinguish mobility patterns between standard working days and holidays.

\paragraph{Mobility Fact Table Construction\\}
The core mobility data is transformed into the \texttt{fact\_mobility} table. This process converts the raw, text-based daily CSVs into a strongly typed, time-series optimized format. Key transformations include:

\begin{enumerate}
    \item \textbf{Temporal Standardization:} The raw separate date and hour fields are combined into a single \texttt{TIMESTAMP WITH TIME ZONE} column, explicitly localized to 'Europe/Madrid'.
    \item \textbf{Spatial Lookup:} Origin and Destination codes are replaced by their corresponding \texttt{zone\_id} through double joins against the \texttt{dim\_zones} table, ensuring referential integrity.
    \item \textbf{Type Casting:} The ``trips'' column is cleaned (handling European decimal formats) and cast to \texttt{DOUBLE} precision to support fractional trip expansion factors.
\end{enumerate}

\paragraph{Data Observability and Quality Logging\\}
To ensure the reliability of the pipeline, a dedicated table named \texttt{data\_quality\_logs} has been implemented within the Silver layer. Unlike the business data tables, this table serves as a metadata registry to persist the results of automated audit checks performed during ingestion (e.g., null rate calculations, row count validations, or statistical anomalies). This allows for longitudinal tracking of data health.

The schema is designed to be generic enough to store heterogeneous metrics:
{\raggedright
\begin{itemize}
    \item \textbf{\texttt{check\_timestamp}:} The exact time when the audit was executed.
    \item \textbf{\texttt{table\_name}:} The target table being audited (e.g., \textit{fact\_mobility}).
    \item \textbf{\texttt{metric\_name}:} A descriptive identifier for the KPI (e.g., \textit{avg\_income\_per\_capita}, \textit{null\_zone\_rate}).
    \item \textbf{\texttt{metric\_value}:} A numeric field (\texttt{DOUBLE}) storing the result of the check.
    \item \textbf{\texttt{notes}:} Textual field for context, such as the specific batch date range or error warnings associated with the metric.
\end{itemize}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/Silver_Schema_diagramS4}
    
    \caption{Entity-Relationship Diagram (ERD) of the Silver Layer. The schema follows a Star Schema design, centering on the \texttt{fact\_mobility} table linked to spatial (\texttt{dim\_zones}) and temporal dimensions.}
    \label{fig:silver_layer_diagram}
\end{figure}

\subsubsection{Gold Layer (Mart)}
The Gold layer constitutes the ``Data Mart'' of the lakehouse architecture. While the Silver layer focuses on data integrity and normalization, the Gold layer is organized around specific business problems. Tables in this layer are denormalized, pre-aggregated, and enriched with derived metrics to ensure low-latency performance for Business Intelligence (BI) tools and final reporting.

As illustrated in Figure \ref{fig:gold_layer_diagram}, the schema consists of three distinct analytical tables, each corresponding to a specific use case:

\begin{itemize}
    \item \textbf{\texttt{gold\_typical\_day\_patterns}:} 
    This table supports temporal analysis (Use Case 1). It reduces billions of raw trip records into a lightweight set of hourly profiles.
    \begin{itemize}
        \item \textbf{Aggregation:} Data is grouped by \texttt{cluster\_id} (representing patterns like ``Weekday'' or ``Holiday'') and \texttt{hour}.
        \item \textbf{Metrics:} It stores the \texttt{avg\_trips} to allow for immediate plotting of demand curves without recalculating the underlying raw data.
    \end{itemize}

    \item \textbf{\texttt{gold\_infrastructure\_gaps}:} 
    Designed for the Gravity Model analysis (Use Case 2), this table stores Origin-Destination (OD) pairs. Unlike standard OD matrices, this table is enriched with socio-economic context.
    \begin{itemize}
        \item \textbf{Contextual Data:} Includes \texttt{total\_population} (Origin) and \texttt{rent} (Destination proxy for economic attraction) alongside the spatial impedance \texttt{dist\_km}.
        \item \textbf{Key KPI:} The \texttt{mismatch\_ratio} is the critical derivative, quantifying the gap between theoretical potential and actual observed trips.
    \end{itemize}

    \item \textbf{\texttt{gold\_zone\_functional\_classification}:} 
    This table provides a semantic layer for the spatial zones (Use Case 3). Instead of raw ID numbers, it assigns functional roles to municipalities.
    \begin{itemize}
        \item \textbf{Flow Ratios:} It persists calculated indices such as \texttt{net\_flow\_ratio} (balance of trade in trips) and \texttt{retention\_rate} (local self-sufficiency).
        \item \textbf{Categorization:} The \texttt{functional\_label} column stores the final human-readable classification (e.g., ``Bedroom Community''), simplifying downstream map visualizations.
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/Gold_Schema_diagramS4} 
    \caption{Schema design of the Gold Layer. These tables represent the final analytical products, denormalized and enriched with calculated metrics (e.g., mismatch ratios, functional labels) ready for visualization.}
    \label{fig:gold_layer_diagram}
\end{figure}














\section{Implementation Methodology}
The operational logic of the Data Lakehouse is implemented through a modular, automated workflow orchestrated by Apache Airflow \cite{airflow}. The system adopts a decoupled architecture where distinct Directed Acyclic Graphs (DAGs) handle specific stages of the data lifecycle based on their update frequency and functional scope.

This section details the engineering strategies employed to ensure scalable data ingestion, atomic processing of daily batches, and the automated generation of analytical models. The implementation prioritizes robustness through strict transaction management, task isolation, and the separation of heavy computational ELT from on-demand reporting queries.

\subsection{Orchestration Strategy}
To manage the dependencies between the Bronze, Silver, and Gold layers efficiently, the system is architected into a multi-DAG ecosystem. This design allows for independent scaling and maintenance of static dimensions versus high-volume mobility facts.

\begin{enumerate} 
    \item \textbf{Infrastructure \& Dimensions DAG:} Handles the ingestion of low-frequency static data (INE demographics, MITMA zoning, Calendars). It establishes the schema foundations and Bronze/Silver dimension tables.
    
    \item \textbf{Mobility Ingestion DAG:} A parameterized worker pipeline designed for high-volume processing. It accepts date ranges to ingest, clean, and transform the daily mobility files (MITMA OD Matrices) from Bronze to Silver using atomic tasks.
    
    \item \textbf{Gold Generations and consultings DAGs:} These pipelines (DAGs 31–33) consolidate the logic into unified flows. Triggered on-demand with custom parameters (e.g., specific spatial polygons or date ranges), they first materialize the aggregated Gold tables using DuckDB and immediately generate the final visual assets (Kepler.gl maps, Matplotlib charts), uploading them to S3 in a single execution context.
\end{enumerate}

\subsection{Infrastructure \& Dimensions (DAG 1)}
The foundational pipeline, \texttt{infrastructure\_and\_dimensions}, is responsible for establishing the Lakehouse schema and ingesting low-velocity reference data. Unlike the daily mobility processing, this DAG is designed to be executed on-demand or annually, as zoning definitions and census statistics change infrequently.

The workflow is architected into three distinct logical phases to maximize parallelism while ensuring referential integrity.

\subsubsection{Phase 1: Infrastructure Initialization}
Before data movement begins, the pipeline ensures the environment is consistent. The task called \texttt{create\_schemas} connects to the Neon catalog to verify the existence of the logical namespaces (\textit{bronze, silver, gold}). Simultaneously, the \texttt{create\_stats\_table} task initializes the \texttt{data\_quality\_log} table in the Silver layer, which is a prerequisite for audit logging in all subsequent pipelines.

\subsubsection{Phase 2: Parallel Bronze Ingestion}
To optimize total execution time, all external data extraction tasks are configured to run in parallel. These tasks use the Airflow \textit{@task} notation arguments to specify 3 retries with a gap of 1 minute in case they fail. This phase handles heterogenous data formats through specialized ingestion logic:

\begin{itemize}
    \item \textbf{Spatial Ingestion:} The \texttt{br\_ingest\_geo\_data} task utilizes DuckDB's \texttt{spatial} extension. It performs an HTTP HEAD request to validate the existence of the remote Shapefiles (.shp, .shx, .dbf) before executing \texttt{ST\_Read} to ingest the vector geometry directly into the Bronze layer.
    \item \textbf{Statistical Ingestion:} Tasks targeting INE data (e.g., \texttt{br\_ingest\_ine\_rent}) implement specific handling for legacy formats, forcing \texttt{ISO-8859-1} encoding and tab separators to prevent parsing errors common in Spanish government datasets.
    \item \textbf{Dictionaries:} Tasks for zoning and mapping ingest standard CSVs via DuckDB's \texttt{read\_csv\_auto}, adding audit columns (\texttt{source\_url}, \texttt{ingestion\_timestamp}) on the fly.
\end{itemize}

\subsubsection{Phase 3: Silver Transformation and Convergence}
The dependency structure in the Silver layer follows a "Funnel" pattern (Figure \ref{fig:dag1_flow}). The system enforces a hard dependency on the creation of the master dimension table before populating satellite metric tables.

\begin{enumerate}
    \item \textbf{Convergence Point (\texttt{dim\_zones}):} This task acts as the synchronization barrier. It waits for the completion of three specific Bronze tasks—Geometry, Zoning names, and INE Mapping. Once available, it performs a 3-way join to construct the master \texttt{dim\_zones} table, generating the surrogate \texttt{zone\_id}.
    \item \textbf{Satellite Expansion:} Once the master dimension exists, the downstream tasks called \texttt{metric\_population}, \texttt{metric\_ine\_rent}, \texttt{dim\_zone\_distance\_matrix} and \texttt{dim\_zone\_holidays} trigger in parallel. Each task performs a join between its respective raw Bronze source and the newly created \texttt{dim\_zones} to assign the correct surrogate keys to the statistical data.
\end{enumerate}

\begin{figure}[H]
    \centering
    % Placeholder for the Graph View screenshot of DAG 1
    \includegraphics[width=1\textwidth]{imgs/infrastructure_and_dimensions-DAG}
    \caption{Dependency graph of the Infrastructure DAG. Note the convergence of spatial and dictionary data into \texttt{dim\_zones} before metric processing.}
    \label{fig:dag1_flow}
\end{figure}



\subsection{Mobility Ingestion (DAG 2)}
The second pipeline, \texttt{mobility\_ingestion}, constitutes the core factory of the Lakehouse. It is responsible for the daily ingestion and transformation of the high-volume Origin-Destination matrices. Given that the source data is partitioned by day (one compressed CSV per day), this DAG utilizes Airflow's Dynamic Task Mapping to scale horizontally, processing date ranges supplied via run-time parameters.

\subsubsection{Phase 1: Dynamic Scheduling and Initialization}
Unlike the static infrastructure DAG, this workflow begins by generating a execution plan based on user input. The \texttt{generate\_date\_list} task parses the \texttt{start\_date} and \texttt{end\_date} parameters to produce a list of strings (e.g., \texttt{['20230101', '20230102', ...]}).

Simultaneously, the pipeline executes idempotent initialization tasks (\texttt{ensure\_br\_mobility...} and \texttt{ensure\_sl\_mobility...}). These tasks utilize \texttt{CREATE TABLE IF NOT EXISTS} logic to prepare the partitioning structures in the Bronze and Silver layers, ensuring that parallel workers do not encounter race conditions when attempting to write to non-existent tables.

\subsubsection{Phase 2: Resilient Bronze Ingestion}
The \texttt{br\_process\_single\_day} task is mapped over the generated date list. Each instance handles a specific day, implementing robust checks to handle the known inconsistencies in the MITMA source server:

\begin{itemize}
    \item \textbf{Pre-flight Validation:} Before attempting extraction, the task performs an HTTP \texttt{HEAD} request to the calculated URL. If the source file is missing (a common occurrence for specific dates in late 2023), the task logs a warning and skips execution gracefully rather than failing the pipeline.
    \item \textbf{Throttling and Retries:} To prevent overwhelming the source server or saturating local memory, concurrency is strictly limited via \texttt{max\_active\_tis\_per\_dag=2}. Network resilience is enforced with a retry policy of 5 attempts with a 30-second delay.
    \item \textbf{Streaming Ingestion:} Valid files are streamed directly from the remote server into the \texttt{mobility\_data} table using DuckDB's \texttt{read\_csv\_auto}, bypassing local disk storage.
\end{itemize}

\subsubsection{Phase 3: Silver Transformation}
The transformation phase (\texttt{sl\_process\_single\_day}) mirrors the mapping of the ingestion phase but introduces a strict cross-layer dependency: Silver processing for the batch only begins once Bronze ingestion is confirmed.

Inside this task, raw text data is transformed into the analytical schema. This involves joining the raw mobility records with the \texttt{dim\_zones} table (created in DAG 1) to resolve MITMA/INE codes into internal surrogate keys. This effectively acts as a foreign key validation constraint; records with invalid zone codes are implicitly filtered during the inner join, ensuring referential integrity in the Silver layer.

\begin{figure}[H]
    \centering
    % Placeholder for the Graph View screenshot of DAG 2
    \includegraphics[width=1\textwidth]{imgs/mobility_ingestion-DAG}
    \caption{The Mobility Ingestion DAG utilizing Dynamic Task Mapping. The central block expands into $N$ parallel tasks corresponding to the number of days in the requested date range.}
    \label{fig:dag2_flow}
\end{figure}


\subsection{Gold Generations and consultings (DAGs 3)}
The analytical layer is materialized through three dedicated Airflow pipelines. Each DAG is designed to answer a specific business question by transforming Silver data into aggregated Gold tables and subsequently generating visual reports stored in S3.

\begin{itemize}
    \item \textbf{DAG 31: Typical Day Clustering (\texttt{31\_bq1\_clustering})} \\
    This pipeline addresses the characterization of mobility patterns.
    \begin{itemize}
        \item \textbf{Transformation:} It aggregates hourly trip profiles from \texttt{silver.fact\_mobility} and extracts them into a Python task. Using Scikit-Learn, it applies a K-Means algorithm ($k=3$) to classify days into distinct patterns (e.g., Weekdays vs. Weekends). The results are persisted in the \texttt{gold.typical\_day\_patterns} table.
        \item \textbf{Output:} The DAG queries the generated Gold table to plot the temporal profiles using Matplotlib. The resulting image (\texttt{.png}) is uploaded to S3, visualizing the average hourly intensity for each identified cluster.
    \end{itemize}

    \item \textbf{DAG 32: Infrastructure Gap Analysis (\texttt{32\_bq2\_gaps})} \\
    This pipeline implements the Gravity Model to identify infrastructure deficits.
    \begin{itemize}
        \item \textbf{Transformation:} It executes a batch SQL job to create the \texttt{gold.infrastructure\_gaps} table. This process joins mobility flows with population and economic metrics, calculating the ``Gravity Score'' and comparing it against actual trips to derive a \texttt{mismatch\_ratio} for every origin-destination pair.
        \item \textbf{Output:} The DAG includes two visualization tasks using Kepler.gl. One generates a bubble map ranking zones by service level, and the other generates an arc map highlighting specific connections where potential demand exceeds actual supply. Both HTML reports are saved to S3.
    \end{itemize}

    \item \textbf{DAG 33: Functional Classification (\texttt{33\_bq3\_functional\_classification})} \\
    This pipeline categorizes municipalities based on their role in the metropolitan network.
    \begin{itemize}
        \item \textbf{Transformation:} It filters mobility data via spatial polygons and calculates inflow/outflow ratios. A logic-based SQL query applies a decision tree to assign labels (e.g., ``Bedroom Community'', ``Activity Hub'') to each zone, storing the result in \texttt{gold.zone\_functional\_classification}.
        \item \textbf{Output:} The final task joins these labels with the zone geometries to generate a Kepler.gl Choropleth map. This interactive dashboard allows urban planners to visualize the functional structure of the region and is automatically published to S3.
    \end{itemize}
\end{itemize}









\section{Results and Use Cases}
% [GUIDE]: This corresponds to Page 4 and the "Results" requirement on Page 6.

\subsection{Use Case 1: Typical Mobility Patterns}
% [GUIDE]: Explain the Clustering (K-Means) approach.
To characterize the ``typical day'', we aggregated hourly flows and applied K-Means clustering...

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{path/to/your/clustering_plot.png}
    \caption{Hourly mobility profiles identified (Weekdays vs. Weekends).}
    \label{fig:clusters}
\end{figure}


\subsection{Use Case 2: Infrastructure Gap Analysis}

The second business objective addresses the identification of zones where transport infrastructure fails to meet potential demand. To solve this, we implemented a \textbf{Gravity Model} in the Gold layer, comparing the theoretical interaction potential between municipalities against the actual mobility flows recorded by MITMA.

\paragraph{Implementation Approach\\}
The solution is orchestrated via an Airflow DAG (\texttt{32\_bq2\_gaps}). The logic follows a three-step process:

\begin{enumerate}
    \item \textbf{Theoretical Modeling:} We calculate a ``Gravity Score'' for every origin-destination pair using the classic impedance formula:
    % [GUIDE]: Explain the Gravity Model implementation.
    \begin{equation}
        T_{ij} = k \cdot \frac{P_i \cdot E_j}{d_{ij}^2}
    \end{equation}
    where $P_i$ is the population at the origin (from \texttt{metric\_population}), $E_j$ is the economic attractiveness (using \texttt{income\_per\_capita} from \texttt{metric\_ine\_rent} as a proxy), and $d_{ij}$ is the distance from our pre-computed \texttt{dim\_zone\_distances}.
    
    \item \textbf{Dynamic Normalization ($k$-factor):} To make the theoretical score comparable to physical trips, the code dynamically calculates a normalization factor ($k$) by computing the ratio between the total sum of actual MITMA trips and the total sum of gravity scores for the period.
    
    \item \textbf{Gap Detection:} The final metric, \texttt{mismatch\_ratio}, is derived by dividing actual trips by the normalized potential. A ratio significantly below 1.0 indicates a connection where mobility is lower than demographic and economic factors suggest, highlighting a potential infrastructure deficit.
\end{enumerate}

\paragraph{Spatial Analytics and Visualization\\}
The pipeline includes a spatial filtering mechanism allowing users to run this analysis on specific regions (e.g., passing a WKT Polygon for the ``Comunidad de Madrid''). Using DuckDB's spatial extension (\texttt{ST\_Intersects}), the system aggregates the results to generate two key visualizations using Kepler.gl:
\begin{itemize}
    \item \textbf{Service Level Ranking:} A bubble map where zones are sized by importance ($P \times E$) and colored by their average service level, instantly highlighting underserved economic hubs.
    \item \textbf{Mobility Arcs:} A flow map visualizing specific origin-destination connections that show the highest disparity between potential and actual traffic.
\end{itemize}


\subsection{Use Case 3: Functional Zoning Classification}

The third analytical product moves beyond simple traffic counting to characterize the \textit{functional role} of each municipality within the metropolitan network. By analyzing the directionality and retention of mobility flows, this use case automatically classifies zones into semantic categories (e.g., ``Bedroom Communities'' vs. ``Activity Hubs''), aiding urban planners in understanding regional dependencies.

\paragraph{Methodology and Classification Logic\\}
The implementation is encapsulated in the DAG \texttt{33\_bq3\_functional\_classification}. The logic aggregates the granular records from \texttt{fact\_mobility} into three distinct flow types per zone: \textbf{Inflow} (trips entering), \textbf{Outflow} (trips leaving), and \textbf{Internal} (trips starting and ending in the same zone). 

Using these aggregates, two key indices are calculated:
\begin{enumerate}
    \item \textbf{Net Flow Ratio:} Measures the balance of attraction.
    \[
    R_{\text{net}} = \frac{\text{Inflow} - \text{Outflow}}{\text{Inflow} + \text{Outflow}}
    \]
    \item \textbf{Retention Rate:} Measures the zone's self-sufficiency.
    \[
    R_{\text{retention}} = \frac{\text{Internal}}{\text{Outflow} + \text{Internal}}
    \]
\end{enumerate}

Based on these metrics, the SQL pipeline applies a hierarchical decision tree to assign a \texttt{functional\_label} to each municipality:

\begin{itemize}
    \item \textbf{Self-Sustaining Cell:} Zones with high retention ($R_{\text{retention}} > 0.20$), indicating a municipality that satisfies most of its residents' needs locally.
    \item \textbf{Activity Hub (Importer):} Zones with a positive net flow ($R_{\text{net}} > 0$), typically business districts or commercial centers attracting commuters.
    \item \textbf{Bedroom Community (Exporter):} Zones with a negative net flow ($R_{\text{net}} < 0$), characterizing residential areas where the population commutes out for work.
    \item \textbf{Balanced / Transit Zone:} Areas with neutral flow ratios acting as connectors.
\end{itemize}

\paragraph{Visualization Strategy\\}
The results are materialized in a Kepler.gl dashboard combining two visual layers to provide context:
\begin{itemize}
    \item \textbf{Polygon Layer (Choropleth):} Displays the administrative boundaries colored by their \texttt{functional\_label}. This reveals regional clusters (e.g., a ring of ``Bedroom Communities'' surrounding a central ``Activity Hub'').
    \item \textbf{Centroid Layer:} Displays bubbles sized by \texttt{total\_activity} (Sum of In + Out + Internal), allowing users to distinguish between major metropolitan centers and smaller rural nodes regardless of their functional classification.
\end{itemize}

\section{Discussion}
% [GUIDE]: Discuss the value. How does this help the "Transport Expert"?
The implemented lakehouse allows experts to query billions of rows using standard SQL without managing infrastructure...

\section{Conclusions and Limitations}
% [GUIDE]: Summarize success. Mention limitations (e.g., data quality issues in Oct/Nov 2023 mentioned in PDF Page 2).
The project successfully established a scalable 3-tier architecture. However, limitations exist regarding data anomalies in late 2023...

\newpage
% --- BIBLIOGRAPHY ---
\begin{thebibliography}{9}

\bibitem{mitma}
Spanish Ministry of Transport (MITMA).
\textit{Open Data Mobility}.
Available at: \url{https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad}
(Accessed on 4 January 2026).

\bibitem{ine}
Spanish National Statistics Institute (INE).
Available at: \url{https://www.ine.es/}
(Accessed on 4 January 2026).

\bibitem{cnig}
Centro Nacional de Información Geográfica (CNIG).
Available at: \url{https://www.ign.es/}
(Accessed on 4 January 2026).

\bibitem{madrid_data}
Ayuntamiento de Madrid.
\textit{Portal de Datos Abiertos}.
Available at: \url{https://datos.madrid.es/}
(Accessed on 4 January 2026).

\bibitem{duckdb}
DuckDB Documentation.
Available at: \url{https://duckdb.org/}
(Accessed on 4 January 2026).

\bibitem{aws_batch}
Amazon Web Service Batch.
\textit{AWS Batch Documentation}.
Available at: \url{https://aws.amazon.com/es/batch/}
(Accessed on 4 January 2026).

\bibitem{ducklake}
DuckLake.
\textit{DuckLake Documentation}.
Available at: \url{https://ducklake.select/docs/stable/}
(Accessed on 4 January 2026).

\bibitem{airflow}
Apache Software Foundation.
\textit{Apache Airflow Documentation}.
Available at: \url{https://airflow.apache.org/}
(Accessed on 4 January 2026).




\end{thebibliography}


\end{document}
