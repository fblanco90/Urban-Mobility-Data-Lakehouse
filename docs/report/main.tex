\documentclass[11pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{graphicx}     % For images
\usepackage{booktabs}     % For nice tables
\usepackage{hyperref}     % For links (repo URL)
\usepackage{listings}     % For code snippets
\usepackage{xcolor}       % For code coloring
\usepackage{float}        % For figure placement
\usepackage{titlesec}     % Custom section titles
\usepackage{parskip}      % Space between paragraphs
\usepackage{amsmath}
\usepackage{pdflscape}


% --- CODE SNIPPET STYLING ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- METADATA ---
\title{\textbf{Building a Scalable 3-Tier Data Lakehouse for Mobility Analysis in Spain}\\ Big Data Engineering Project \\}
\author{
    \textbf{María López Hernández} \and \textbf{Joan Sánchez Verdú} \and \textbf{Fernando Blanco Membrives} \\    
    % \textit{Link to Code Repository:}\\ \textit{\url{https://github.com/joanstudyai-code/MUCEIM-BDET-Project.git}}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    \noindent 
    \textbf{Abstract.} The rapid digitalization of urban transport has generated massive volumes of mobility data, rendering traditional analytical workflows obsolete. This paper presents a scalable 3-tier Data Lakehouse architecture designed to analyze large-scale Spanish mobility data. By integrating high-volume Origin-Destination matrices from MITMA with INE socio-economic indicators, the system provides a robust foundation for urban analytics. The infrastructure leverages a serverless stack—utilizing \textit{DuckDB} for vectorized processing and \textit{DuckLake} for ACID transactions—orchestrated via Apache Airflow. We validate the architecture through three analytical use cases: temporal pattern clustering, infrastructure gap detection via gravity models, and functional zoning classification, demonstrating a cost-effective solution for data-driven transport planning.
\end{abstract}

% --- SECTIONS ---

\section{Introduction}

The availability of high-resolution mobility data from the Spanish Ministry of Transport (MITMA) has opened new avenues for urban planning. However, the sheer volume of these datasets (comprising billions of daily trip records) presents significant engineering challenges that exceed the capabilities of traditional analytical tools. Transport experts often lack the scalable infrastructure required to transform this raw big data into actionable insights.

This project addresses this gap by implementing a Data Lakehouse architecture. By combining the cost-efficiency of data lakes with the transactional integrity of data warehouses, we propose a solution built on DuckDB and DuckLake. This infrastructure allows for the efficient processing of large-scale mobility matrices, enriching them with socio-economic context without the overhead of enterprise systems.


\subsection{Objectives}
The primary goal is to build a robust, reproducible data platform. The specific objectives are:

\begin{itemize}
    \item \textbf{Scalable Architecture:} Implement a 3-tier Lakehouse (Bronze, Silver, Gold) that decouples storage (S3) from compute (AWS Batch) to optimize costs and performance.
    \item \textbf{Data Integration:} Automate ELT pipelines to clean and fuse heterogeneous sources, including mobility flows, census demographics, and vector geometries.
    \item \textbf{Analytical Value:} Demonstrate the system's utility through advanced use cases, such as gravity models for infrastructure analysis and functional zoning classification.
\end{itemize}











\section{Data Sources}
The data lakehouse architecture is designed to ingest and integrate public domain data exclusively, ensuring reproducibility and open access. The following subsections detail the specific datasets and sources selected to build the information system.

\subsection{MITMA Open Data}
The core mobility dataset is sourced from the Spanish Ministry of Transport, Mobility and Urban Agenda (MITMA \cite{mitma}), specifically the \textit{Estudios Básicos de Movilidad}. This dataset provides high-resolution insights into the daily movements of residents derived from mobile network big data.
From the MITMA the following files have been used:
\begin{itemize}
    \item \textbf{\textit{(202301-202312)\_Viajes\_municipios.tar}:} These files, located in the MITMA web at \textit{estudios\_basicos/por-municipios/viajes/meses-completos/}, contain the information of the mobility from an origin to a destination municipality, per day, per hour, for the year 2023.
    \item \textbf{\textit{nombres\_municipios.csv}:} This file, located in the MITMA web at \textit{zonificacion/zonificacion\_municipios/}, contains the name of each municipality.
    \item \textbf{\textit{relacion\_ine\_zonificacionMitma.csv}:} This file, located in the MITMA web at \textit{zonificacion/}, cointains the mapping between the MITMA codes and the INE codes for the municipalities.
    \item  \textbf{\textit{poblacion.csv}:} This file, located in the MITMA web at \textit{zonificacion/}, cointains the population of each municipality per year.
    \item \textbf{\textit{zonificacion\_municipios.shp}:} This ESRI Shapefile (along with its \textit{.dbf}, \textit{.shx}, and \textit{.prj} components), located in the MITMA web at \textit{zonificacion/zonificacion\_municipios/}, contains the official geospatial geometries (polygons) for the municipal study zones.
\end{itemize}

\subsection{INE Demographics and Economics}
To contextualize mobility flows and enable the gravity model analysis, mobility data is enriched with official statistics from the Spanish National Statistics Institute (INE \cite{ine}).
The statistics used from INE are the following:
\begin{itemize}
    \item \textbf{\textit{ine\_rent\_municipalities.csv}:} This file, located in the INE web at \textit{\url{https://www.ine.es/jaxiT3/files/t/es/csv_bd/}}, contains the rent information per year and municipality.
\end{itemize}

\subsection{Open Data}
To categorize mobility patterns by day type (e.g., distinguishing standard workdays from holidays), specific calendar data is integrated from Open Data portals.

\begin{itemize}
    \item \textbf{\textit{calendario\_laboral.csv}:} This file, sourced from the Madrid City Council Open Data Portal (\textit{Datos Abiertos Madrid} \cite{madrid_data}) at \textit{\url{https://datos.madrid.es/}}, contains the official working calendar. It provides the classification of dates (working days, weekends, and holidays) from 2013 to 2026 (included).
\end{itemize}







\newpage
\section{Lakehouse Architecture}
The system follows the Medallion Architecture pattern, decoupled into storage and compute layers to ensure scalability.

\subsection{Technology Stack}
The platform employs a modular architecture designed to decouple storage from compute, leveraging the following core components:

\begin{itemize}
    \item \textbf{Compute Engine (DuckDB \cite{duckdb}):} A high-performance, in-process SQL OLAP engine. It handles vectorized data processing and supports out-of-core execution, allowing for the analysis of datasets larger than available RAM.

    \item \textbf{Elastic Compute (AWS Batch \cite{aws_batch}):} Provides an ephemeral, scalable cloud compute layer. It offloads heavy transformation tasks to on-demand instances, optimizing costs through the use of Spot market resources.
    
    \item \textbf{Lakehouse Management (DuckLake \cite{ducklake}):} A transaction manager that enables ACID properties and snapshot isolation over object storage, ensuring data consistency during concurrent operations.

    \item \textbf{Cloud Storage \& Catalog (Neon + S3):} A decoupled storage design where \textbf{AWS S3} holds the physical data files (Parquet/CSV) and \textbf{Neon} (Serverless Postgres) acts as the centralized metadata catalog and locking mechanism.

    \item \textbf{Orchestration (Apache Airflow \cite{airflow}):} Manages the end-to-end data lifecycle via Directed Acyclic Graphs (DAGs), handling complex dependencies, scheduling, and automated retries.
\end{itemize}

\subsection{Data Layering Strategy}
The platform follows a ``Medallion'' Lakehouse architecture (Bronze $\rightarrow$ Silver $\rightarrow$ Gold). This section serves as the technical reference for the data models, detailing schema definitions, data types (DuckDB dialect), and transformation logic.
\subsubsection{Bronze Layer (Raw Ingestion)}
The Bronze layer acts as the immutable staging area. Data is ingested from external sources (MITMA, INE, Madrid Open Data) in its native format. To prevent pipeline failures caused by unexpected data types in the source files, strict typing is deferred to the Silver layer; consequently, almost all columns in this layer are typed as \texttt{VARCHAR}, preserving the original formatting (e.g., decimal commas, date formats).

In addition, to ensure auditability and lineage, every table in this layer includes two system columns:
\begin{itemize}
    \item \texttt{ingestion\_timestamp} (\texttt{TIMESTAMP}): The UTC timestamp of when the record was inserted.
    \item \texttt{source\_url} (\texttt{VARCHAR}): The specific file URI or API endpoint origin.
\end{itemize}

Below is the complete dictionary of tables persisted in the Bronze Schema:

\paragraph{1. Mobility Data (High Volume)}

\begin{description}
    \item[\texttt{bronze\_mobility\_data}] \hfill \\
    \textit{Description:} Stores the raw daily origin-destination matrices. This table is physically partitioned by the \texttt{fecha} column.
    \begin{itemize}
        \item \texttt{fecha} (\texttt{VARCHAR}): Date of the trip (Format: YYYYMMDD).
        \item \texttt{periodo} (\texttt{VARCHAR}): Hourly interval (00-23).
        \item \texttt{origen} (\texttt{VARCHAR}): Source MITMA Zone ID.
        \item \texttt{destino} (\texttt{VARCHAR}): Destination MITMA Zone ID.
        \item \texttt{distancia} (\texttt{VARCHAR}): Distance category (e.g., ``005-010'').
        \item \texttt{actividad\_origen} (\texttt{VARCHAR}): Imputed purpose at origin (e.g., ``casa'').
        \item \texttt{actividad\_destino} (\texttt{VARCHAR}): Imputed purpose at destination.
        \item \texttt{estudio\_origen\_posible} (\texttt{VARCHAR}): Auxiliary study flag.
        \item \texttt{estudio\_destino\_posible} (\texttt{VARCHAR}): Auxiliary study flag.
        \item \texttt{residencia} (\texttt{VARCHAR}): Zone ID of the traveler's residence.
        \item \texttt{renta} (\texttt{VARCHAR}): Income decile of the traveler.
        \item \texttt{edad} (\texttt{VARCHAR}): Age group interval.
        \item \texttt{sexo} (\texttt{VARCHAR}): Gender (1/2).
        \item \texttt{viajes} (\texttt{VARCHAR}): The expansion factor/number of trips (String with comma decimals).
        \item \texttt{viajes\_km} (\texttt{VARCHAR}): Total kilometers traveled.
    \end{itemize}
\end{description}

\paragraph{2. Spatial \& Reference Data}

\begin{description}
    \item[\texttt{bronze\_geo\_municipalities}] \hfill \\
    \textit{Description:} Ingested from ESRI Shapefiles. This is the only table containing a native spatial type upon ingestion.
    \begin{itemize}
        \item \texttt{ID} (\texttt{VARCHAR}): The MITMA Zone Code.
        \item \texttt{geom} (\texttt{GEOMETRY}): The binary geometry object (Polygon/MultiPolygon).
    \end{itemize}

    \item[\texttt{bronze\_zoning\_municipalities}] \hfill \\
    \textit{Description:} Provides the mapping between numerical IDs and text names.
    \begin{itemize}
        \item \texttt{column0} (\texttt{VARCHAR}): Index column from raw CSV.
        \item \texttt{ID} (\texttt{VARCHAR}): MITMA Zone Code.
        \item \texttt{name} (\texttt{VARCHAR}): Official Municipality Name.
        \item \texttt{filename} (\texttt{VARCHAR}): Name of the source file.
    \end{itemize}

    \item[\texttt{bronze\_mapping\_ine\_mitma}] \hfill \\
    \textit{Description:} A crosswalk table linking Transport Ministry codes (MITMA) with Statistics Institute codes (INE).
    \begin{itemize}
        \item \texttt{seccion\_ine} (\texttt{VARCHAR}): Census Section ID.
        \item \texttt{distrito\_ine} (\texttt{VARCHAR}): Census District ID.
        \item \texttt{municipio\_ine} (\texttt{VARCHAR}): INE Municipality Code.
        \item \texttt{distrito\_mitma} (\texttt{VARCHAR}): Transport District ID.
        \item \texttt{municipio\_mitma} (\texttt{VARCHAR}): Transport Municipality ID.
        \item \texttt{gau\_mitma} (\texttt{VARCHAR}): Great Urban Area (GAU) code.
        \item \texttt{filename} (\texttt{VARCHAR}): Name of the source file.
    \end{itemize}
\end{description}

\paragraph{3. Socio-Economic \& Temporal Data}

\begin{description}
    \item[\texttt{bronze\_population\_municipalities}] \hfill \\
    \textit{Description:} Raw population counts. Note the lack of headers in the source file.
    \begin{itemize}
        \item \texttt{column0} (\texttt{VARCHAR}): Zone Code.
        \item \texttt{column1} (\texttt{VARCHAR}): Population count.
        \item \texttt{filename} (\texttt{VARCHAR}): Name of the source file.
    \end{itemize}

    \item[\texttt{bronze\_ine\_rent\_municipalities}] \hfill \\
    \textit{Description:} Economic indicators from INE.
    \begin{itemize}
        \item \texttt{Municipios} (\texttt{VARCHAR}): Composite string (Code + Name).
        \item \texttt{Distritos} (\texttt{VARCHAR}): District ID (if applicable).
        \item \texttt{Secciones} (\texttt{VARCHAR}): Section ID (if applicable).
        \item \texttt{Indicadores\_de\_renta\_media} (\texttt{VARCHAR}): The name of the metric.
        \item \texttt{Periodo} (\texttt{VARCHAR}): Reference year.
        \item \texttt{Total} (\texttt{VARCHAR}): The numeric value (contains dots for thousands).
        \item \texttt{filename} (\texttt{VARCHAR}): Name of the source file.
    \end{itemize}

    \item[\texttt{bronze\_work\_calendars}] \hfill \\
    \textit{Description:} Calendar data for holiday identification. Extra columns (column5-8) represent empty fields often found in loosely structured CSVs.
    \begin{itemize}
        \item \texttt{Dia} (\texttt{VARCHAR}): Date string (DD/MM/YYYY).
        \item \texttt{Dia\_semana} (\texttt{VARCHAR}): Name of the weekday.
        \item \texttt{laborable\_festivo\_domingo\_festivo} (\texttt{VARCHAR}): Workday status.
        \item \texttt{Tipo\_de\_Festivo} (\texttt{VARCHAR}): Flag for National/Local holidays.
        \item \texttt{Festividad} (\texttt{VARCHAR}): Name of the holiday.
        \item \texttt{column5}, \texttt{column6}, \texttt{column7}, \texttt{column8} (\texttt{VARCHAR}): Parsing artifacts from raw CSV.
        \item \texttt{filename} (\texttt{VARCHAR}): Name of the source file.
    \end{itemize}
\end{description}




\newpage
\begin{landscape}
\begin{figure}[p]
    \centering
    \includegraphics[width=1.5\textwidth]{imgs/Bronze_Schema_diagramS4}
    
    \caption{Entity Diagram of the Bronze Layer. One table for each file.}
    \label{fig:bronze_layer_diagram}
\end{figure}
\end{landscape}


\subsubsection{Silver Layer (Cleaned \& Integrated)}
The Silver layer implements a Star Schema design. In this stage, data is cast to strict types, cleaned of formatting artifacts (e.g., removing thousands separators), and enriched with spatial calculations. All tables include a \texttt{processed\_at} (\texttt{TIMESTAMP}) column for versioning.

\paragraph{1. Dimension Tables (Context)}

\begin{description}
    \item[\texttt{silver.dim\_zones}] \hfill \\
    \textit{Purpose:} The central spatial authority table. \\
    \textit{Transformation:} Joins \texttt{geo}, \texttt{zoning}, and \texttt{mapping} tables. Generates a Surrogate Key.
    \begin{itemize}
        \item \texttt{zone\_id} (\texttt{BIGINT}): Sequential integer generated via \texttt{ROW\_NUMBER()}.
        \item \texttt{mitma\_code} (\texttt{VARCHAR}): Original Transport ID.
        \item \texttt{ine\_code} (\texttt{VARCHAR}): National Statistics ID (Cleaned).
        \item \texttt{zone\_name} (\texttt{VARCHAR}): Human-readable name.
        \item \texttt{polygon} (\texttt{GEOMETRY}): Boundary for spatial joins.
        \item \texttt{centroid} (\texttt{GEOMETRY}): Calculated center (\texttt{ST\_Centroid}) for distance logic.
    \end{itemize}

    \item[\texttt{silver.dim\_zone\_distances}] \hfill \\
    \textit{Purpose:} Pre-computed Distance Matrix ($N \times N$). \\
    \textit{Transformation:} Cartesian product of \texttt{dim\_zones}.
    \begin{itemize}
        \item \texttt{origin\_zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{destination\_zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{dist\_km} (\texttt{DOUBLE}): Euclidean distance in km. \textbf{Logic:} \texttt{GREATEST(0.5, dist)} ensures no zero-distances to prevent division errors in Gravity Models.
    \end{itemize}

    \item[\texttt{silver.dim\_zone\_holidays}] \hfill \\
    \textit{Purpose:} Temporal context for mobility patterns. \\
    \textit{Transformation:} Filters \texttt{work\_calendars} for ``National Holidays'' in 2023 and explodes them to all zones via Cross Join.
    \begin{itemize}
        \item \texttt{zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{holiday\_date} (\texttt{DATE}): The specific date of the holiday.
    \end{itemize}
\end{description}

\paragraph{2. Metric Tables (Auxiliary Data)}

\begin{description}
    \item[\texttt{silver.metric\_population}] \hfill \\
    \textit{Grain:} One row per Zone per Year.
    \begin{itemize}
        \item \texttt{zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{population} (\texttt{BIGINT}): Cast from \texttt{column1}. \textbf{Cleaning:} Regex filter \texttt{[a-zA-Z]} removes header rows from raw CSV.
        \item \texttt{year} (\texttt{INTEGER}): Reference year (2023).
    \end{itemize}

    \item[\texttt{silver.metric\_ine\_rent}] \hfill \\
    \textit{Grain:} One row per Zone per Year.
    \begin{itemize}
        \item \texttt{zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{income\_per\_capita} (\texttt{DOUBLE}): Cast from \texttt{Total}. \textbf{Cleaning:} Removes dots (``.'') and filters only ``Renta neta media por persona''.
        \item \texttt{year} (\texttt{INTEGER}): Reference year.
    \end{itemize}
\end{description}

\paragraph{3. Fact Table (Core Mobility)}

\begin{description}
    \item[\texttt{silver.fact\_mobility}] \hfill \\
    \textit{Purpose:} The transactional heart of the Lakehouse. \\
    \textit{Grain:} One row per trip flow (Origin $\to$ Dest $\to$ Time). \\
    \textit{Transformation:} Converts string dates/hours into Timezoned Timestamps. Filters out invalid zones via Inner Joins to \texttt{dim\_zones}.
    \begin{itemize}
        \item \texttt{period} (\texttt{TIMESTAMP WITH TIME ZONE}): \textbf{Logic:} \texttt{fecha} + \texttt{periodo} converted to ``Europe/Madrid''.
        \item \texttt{partition\_date} (\texttt{DATE}): Partition Key derived from \texttt{fecha}.
        \item \texttt{origin\_zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{destination\_zone\_id} (\texttt{BIGINT}): FK ref \texttt{dim\_zones}.
        \item \texttt{trips} (\texttt{DOUBLE}): Number of trips cast to double precision.
    \end{itemize}
\end{description}

\paragraph{4. Data Observability}

\begin{description}
    \item[\texttt{silver.data\_quality\_log}] \hfill \\
    \textit{Purpose:} Metadata registry for pipeline health.
    \begin{itemize}
        \item \texttt{check\_timestamp} (\texttt{TIMESTAMP}): Audit time.
        \item \texttt{table\_name} (\texttt{VARCHAR}): Target of the check.
        \item \texttt{metric\_name} (\texttt{VARCHAR}): e.g., ``null\_rate'', ``row\_count''.
        \item \texttt{metric\_value} (\texttt{DOUBLE}): The result of the check.
        \item \texttt{notes} (\texttt{VARCHAR}): Context or warnings.
    \end{itemize}
\end{description}



\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/Silver_Schema_diagramS4}
    
    \caption{Entity-Relationship Diagram (ERD) of the Silver Layer. The schema follows a Star Schema design, centering on the \texttt{fact\_mobility} table linked to spatial (\texttt{dim\_zones}) and temporal dimensions.}
    \label{fig:silver_layer_diagram}
\end{figure}


\subsubsection{Gold Layer (Data Mart)}
The Gold layer is the final destination for analytics, designed to answer specific business questions. Unlike the normalized Silver layer, these tables are denormalized and pre-aggregated to optimize query performance for visualization tools (Kepler.gl, Plotly).

Below is the technical definition of the three core analytical products persisted in this layer:

\paragraph{1. Temporal Analysis (Use Case 1)}

\begin{description}
    \item[\texttt{gold.typical\_day\_patterns}] \hfill \\
    \textit{Purpose:} Stores the centroids of the mobility profiles identified via Unsupervised Learning (K-Means). It reduces billions of raw records into lightweight hourly curves representing standard behaviors (e.g., ``Workday'' vs ``Holiday'').
    \textit{Grain:} One row per Cluster per Hour.
    \begin{itemize}
        \item \texttt{cluster\_id} (\texttt{INTEGER}): The label assigned by the K-Means algorithm (0, 1, or 2).
        \item \texttt{hour} (\texttt{INTEGER}): Hour of the day (0-23).
        \item \texttt{avg\_trips} (\texttt{DOUBLE}): The average normalized volume of trips for this specific hour/cluster combination. Used for plotting demand curves.
        \item \texttt{processed\_at} (\texttt{TIMESTAMP}): Audit timestamp.
    \end{itemize}
\end{description}

\paragraph{2. Infrastructure Gap Analysis (Use Case 2)}

\begin{description}
    \item[\texttt{gold.infrastructure\_gaps}] \hfill \\
    \textit{Purpose:} The output of the Gravity Model. This table joins mobility flows with socio-economic dimensions to quantify the disparity between theoretical potential and actual usage.
    \textit{Grain:} One row per Origin-Destination pair.
    \begin{itemize}
        \item \texttt{org\_zone\_id} (\texttt{BIGINT}): Origin Zone FK.
        \item \texttt{dest\_zone\_id} (\texttt{BIGINT}): Destination Zone FK.
        \item \texttt{total\_population} (\texttt{BIGINT}): Population at origin (Driver of demand).
        \item \texttt{rent} (\texttt{DOUBLE}): Income per capita at destination (Attractor).
        \item \texttt{total\_trips} (\texttt{DOUBLE}): The actual observed mobility volume from Silver.
        \item \texttt{dist\_km} (\texttt{DOUBLE}): Distance impedance between zones.
        \item \texttt{mismatch\_ratio} (\texttt{DOUBLE}): The key analytical KPI calculated as $ActualTrips / PotentialScore$. A value $\ll 1.0$ indicates an infrastructure deficit.
    \end{itemize}
\end{description}

\paragraph{3. Functional Classification (Use Case 3)}

\begin{description}
    \item[\texttt{gold.zone\_functional\_classification}] \hfill \\
    \textit{Purpose:} A semantic layer that profiles every municipality based on its role in the metropolitan network (Importer vs. Exporter of commuters).
    \textit{Grain:} One row per Zone.
    \begin{itemize}
        \item \texttt{zone\_id} (\texttt{BIGINT}): Id of the zone.
        \item \texttt{zone\_name} (\texttt{VARCHAR}): Human-readable name for map tooltips.
        \item \texttt{internal\_trips} (\texttt{DOUBLE}): Count of trips starting and ending in the same zone.
        \item \texttt{outflow} (\texttt{DOUBLE}): Count of trips leaving the zone.
        \item \texttt{inflow} (\texttt{DOUBLE}): Count of trips entering the zone.
        \item \texttt{net\_flow\_ratio} (\texttt{DOUBLE}): Calculated metric: $(In - Out) / Total$. Positive values indicate ``Activity Hubs''; negative values indicate ``Residential/Bedroom Communities''.
        \item \texttt{retention\_rate} (\texttt{DOUBLE}): Calculated metric: $Internal / (Out + Internal)$. Indicates self-sufficiency.
        \item \texttt{functional\_label} (\texttt{VARCHAR}): The final categorical classification derived from the decision tree logic (e.g., ``Self-Sustaining Cell'', ``Activity Hub'').
    \end{itemize}
\end{description}




\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/Gold_Schema_diagramS4} 
    \caption{Gold Layer Schema. These tables represent the final analytical products, denormalized and enriched with derived metrics (e.g., mismatch ratios, functional labels) ready for visualization.}
    \label{fig:gold_layer_diagram}
\end{figure}














\section{Implementation Methodology}
The operational logic of the Data Lakehouse is implemented through a modular, automated workflow orchestrated by Apache Airflow \cite{airflow}. The system adopts a decoupled architecture where distinct Directed Acyclic Graphs (DAGs) handle specific stages of the data lifecycle based on their update frequency and functional scope.

This section details the engineering strategies employed to ensure scalable data ingestion, atomic processing of daily batches, and the automated generation of analytical models. The implementation prioritizes robustness through strict transaction management, task isolation, and the separation of heavy computational ELT from on-demand reporting queries.

\subsection{Orchestration Strategy}
To manage the dependencies between the Bronze, Silver, and Gold layers efficiently, the system is architected into a multi-DAG ecosystem. This design allows for independent scaling and maintenance of static dimensions versus high-volume mobility facts.

\begin{enumerate} 
    \item \textbf{Infrastructure \& Dimensions DAG:} Handles the ingestion of low-frequency static data (INE demographics, MITMA zoning, Calendars). It establishes the schema foundations and Bronze/Silver dimension tables.
    
    \item \textbf{Mobility Ingestion DAG:} A parameterized worker pipeline designed for high-volume processing. It accepts date ranges to ingest, clean, and transform the daily mobility files (MITMA OD Matrices) from Bronze to Silver using atomic tasks.
    
    \item \textbf{Gold Generations and consultings DAGs:} These pipelines (DAGs 31–33) consolidate the logic into unified flows. Triggered on-demand with custom parameters (e.g., specific spatial polygons or date ranges), they first materialize the aggregated Gold tables using DuckDB and immediately generate the final visual assets (Kepler.gl maps, Matplotlib charts), uploading them to S3 in a single execution context.
\end{enumerate}

\subsection{Infrastructure \& Dimensions (DAG 1)}
The foundational pipeline, \texttt{infrastructure\_and\_dimensions}, is responsible for establishing the Lakehouse schema and ingesting low-velocity reference data. Unlike the daily mobility processing, this DAG is designed to be executed on-demand or annually, as zoning definitions and census statistics change infrequently.

Before data movement begins, the pipeline ensures the environment is consistent. The task called \texttt{create\_schemas} connects to the Neon catalog to verify the existence of the logical namespaces (\textit{bronze, silver, gold}). Simultaneously, the \texttt{create\_stats\_table} task initializes the \texttt{data\_quality\_log} table in the Silver layer, which is a prerequisite for audit logging in all subsequent pipelines.

To optimize total execution time, all external data extraction tasks are configured to run in parallel. These tasks use the Airflow \textit{@task} notation arguments to specify 3 retries with a gap of 1 minute in case they fail. This phase handles heterogenous data formats through specialized ingestion logic:

\begin{itemize}
    \item \textbf{Spatial Ingestion:} The \texttt{br\_ingest\_geo\_data} task utilizes DuckDB's \texttt{spatial} extension. It performs an HTTP HEAD request to validate the existence of the remote Shapefiles (.shp, .shx, .dbf) before executing \texttt{ST\_Read} to ingest the vector geometry directly into the Bronze layer.
    \item \textbf{Statistical Ingestion:} Tasks targeting INE data (e.g., \texttt{br\_ingest\_ine\_rent}) implement specific handling for legacy formats, forcing \texttt{ISO-8859-1} encoding and tab separators to prevent parsing errors common in Spanish government datasets.
    \item \textbf{Dictionaries:} Tasks for zoning and mapping ingest standard CSVs via DuckDB's \texttt{read\_csv\_auto}, adding audit columns (\texttt{source\_url}, \texttt{ingestion\_timestamp}) on the fly.
\end{itemize}

The dependency structure in the Silver layer follows a \textit{Funnel} pattern (Figure \ref{fig:dag1_flow}). The system enforces a hard dependency on the creation of the master dimension table before populating satellite metric tables.

\begin{enumerate}
    \item \textbf{Convergence Point (\texttt{dim\_zones}):} This task acts as the synchronization barrier. It waits for the completion of three specific Bronze tasks—Geometry, Zoning names, and INE Mapping. Once available, it performs a 3-way join to construct the master \texttt{dim\_zones} table, generating the surrogate \texttt{zone\_id}.
    \item \textbf{Satellite Expansion:} Once the master dimension exists, the downstream tasks called \texttt{metric\_population}, \texttt{metric\_ine\_rent}, \texttt{dim\_zone\_distance\_matrix} and \texttt{dim\_zone\_holidays} trigger in parallel. Each task performs a join between its respective raw Bronze source and the newly created \texttt{dim\_zones} to assign the correct surrogate keys to the statistical data.
\end{enumerate}

\begin{figure}[H]
    \centering
    % Placeholder for the Graph View screenshot of DAG 1
    \includegraphics[width=1\textwidth]{imgs/infrastructure_and_dimensions-DAG}
    \caption{Dependency graph of the Infrastructure DAG.}
    \label{fig:dag1_flow}
\end{figure}



\subsection{Mobility Ingestion (DAG 2)}
The second pipeline, \texttt{mobility\_ingestion}, constitutes the core factory of the Lakehouse. It is responsible for the daily ingestion and transformation of the high-volume Origin-Destination matrices. Given that the source data is partitioned by day (one compressed CSV per day), this DAG utilizes Airflow's Dynamic Task Mapping to scale horizontally, processing date ranges supplied via run-time parameters.

Unlike the static infrastructure DAG, this workflow begins by generating a execution plan based on user input. The \texttt{generate\_date\_list} task parses the \texttt{start\_date} and \texttt{end\_date} parameters to produce a list of strings (e.g., \texttt{['20230101', '20230102', ...]}).

Simultaneously, the pipeline executes idempotent initialization tasks (\texttt{ensure\_br\_mobility...} and \texttt{ensure\_sl\_mobility...}). These tasks utilize \texttt{CREATE TABLE IF NOT EXISTS} logic to prepare the partitioning structures in the Bronze and Silver layers, ensuring that parallel workers do not encounter race conditions when attempting to write to non-existent tables.

The \texttt{br\_process\_single\_day} task is mapped over the generated date list. Each instance handles a specific day, implementing robust checks to handle the known inconsistencies in the MITMA source server:

\begin{itemize}
    \item \textbf{Pre-flight Validation:} Before attempting extraction, the task performs an HTTP \texttt{HEAD} request to the calculated URL. If the source file is missing (a common occurrence for specific dates in late 2023), the task logs a warning and skips execution gracefully rather than failing the pipeline.
    \item \textbf{Throttling and Retries:} To prevent overwhelming the source server or saturating local memory, concurrency is strictly limited via \texttt{max\_active\_tis\_per\_dag=2}. Network resilience is enforced with a retry policy of 5 attempts with a 30-second delay.
    \item \textbf{Streaming Ingestion:} Valid files are streamed directly from the remote server into the \texttt{mobility\_data} table using DuckDB's \texttt{read\_csv\_auto}, bypassing local disk storage.
\end{itemize}

The transformation phase (\texttt{sl\_process\_single\_day}) mirrors the mapping of the ingestion phase but introduces a strict cross-layer dependency: Silver processing for the batch only begins once Bronze ingestion is confirmed.

Inside this task, raw text data is transformed into the analytical schema. This involves joining the raw mobility records with the \texttt{dim\_zones} table (created in DAG 1) to resolve MITMA/INE codes into internal surrogate keys. This effectively acts as a foreign key validation constraint; records with invalid zone codes are implicitly filtered during the inner join, ensuring referential integrity in the Silver layer.

\begin{figure}[H]
    \centering
    % Placeholder for the Graph View screenshot of DAG 2
    \includegraphics[width=1\textwidth]{imgs/mobility_ingestion-DAG}
    \caption{The Mobility Ingestion DAG utilizing Dynamic Task Mapping.}
    \label{fig:dag2_flow}
\end{figure}


\subsection{Gold Generations and consultings (DAGs 3)}
The analytical layer is materialized through three dedicated Airflow pipelines. Each DAG is designed to answer a specific business question by transforming Silver data into aggregated Gold tables and subsequently generating visual reports stored in S3.

\begin{itemize}
    \item \textbf{DAG 31: Typical Day Clustering (\texttt{31\_bq1\_clustering})} \\
    This pipeline addresses the characterization of mobility patterns via unsupervised learning.
    \begin{itemize}
        \item \textbf{Transformation:} It aggregates mobility data to extract hourly profiles and applies K-Means clustering ($k=3$) using Scikit-Learn to categorize days. The pipeline uses two tables: \texttt{gold.typical\_day\_patterns} and \texttt{gold.typical\_od\_matrices} for spatial distribution.
        \item \textbf{Output:} The DAG produces a comprehensive reporting suite in the results directory, including a static Matplotlib chart for temporal profiles, an interactive Plotly HTML heatmap for detailed OD analysis, and a summary Markdown report.
    \end{itemize}

    \item \textbf{DAG 32: Infrastructure Gap Analysis (\texttt{32\_bq2\_gaps})} \\
    This pipeline implements a Gravity Model to quantify the disparity between theoretical potential and actual mobility.
    \begin{itemize}
        \item \textbf{Transformation:} It executes a SQL batch job to create \texttt{gold.infrastructure\_gaps} table. By integrating population, income, and distance metrics, it calculates a theoretical ``Gravity Score'' ($P \cdot E / d^2$) and compares it with actual flows to derive a \texttt{mismatch\_ratio} for every connection.
        \item \textbf{Output:} The DAG utilizes Kepler.gl to generate two interactive geospatial reports: a \textit{Service Level Ranking} (bubble map) identifying underserved zones and a \textit{Mobility Gaps} visualization (arc map) showing specific connections where demand exceeds supply, alongside a summary Markdown report.
    \end{itemize}

    \item \textbf{DAG 33: Functional Classification (\texttt{33\_bq3\_functional\_classification})} \\
    This pipeline categorizes municipalities based on their role in the metropolitan network (e.g., residential vs. commercial hubs).
    \begin{itemize}
        \item \textbf{Transformation:} It calculates critical flow metrics (inflow/outflow ratios and retention rates) via a SQL batch job. A logic-based decision tree assigns a functional label (e.g., ``Bedroom Community'', ``Activity Hub'') to each zone, persisting the results in \texttt{gold.zone\_functional\_classification}.
        \item \textbf{Output:} The DAG generates a Kepler.gl Choropleth map that colors zone geometries by their functional role and overlays an activity intensity layer (bubbles). This interactive dashboard, accompanied by a Markdown summary, allows urban planners to visualize the structural hierarchy of the region.
    \end{itemize}
\end{itemize}









\section{Results and Use Cases}

\subsection{Use Case 1: Typical Mobility Patterns and Profiling}

The first business objective focuses on identifying and characterizing standard mobility behaviors within the reference year (2023). Instead of relying on static calendar rules (e.g., assuming all Mondays are identical), we implemented an unsupervised machine learning approach in the Gold layer to discover intrinsic daily patterns—distinguishing between standard workdays, holidays, and transition days based on actual hourly flow shapes.

\paragraph{Implementation Approach\\}
The solution is orchestrated via an Airflow DAG (\texttt{31\_bq1\_clustering}). The pipeline employs a hybrid processing strategy: using DuckDB for heavy aggregations and Python (Scikit-Learn) for in-memory clustering. The logic follows a four-step process:

\begin{enumerate}
    \item \textbf{Feature Extraction:} We aggregate the granular records from \texttt{silver.fact\_mobility} into hourly profiles. To ensure the clustering algorithm focuses on the \textit{shape} of the day (e.g., morning/evening peaks) rather than absolute volume, we normalize the data vector for every date $d$:
    \begin{equation}
        V_{d, h} = \frac{T_{d, h}}{\sum_{h=0}^{23} T_{d, h}}
    \end{equation}
    where $T_{d, h}$ is the total trips on day $d$ at hour $h$.
    
    \item \textbf{Unsupervised Classification (K-Means):} These normalized vectors are fed into a K-Means algorithm (configured with $k=3$ clusters). This automatically classifies every date in the analyzed period into distinct profiles (e.g., ``Standard Workday,'' ``Weekend/Leisure,'' or ``Holiday'').
    
    \item \textbf{Pattern Projection:} Once dates are labeled, the system projects these labels back onto the massive dataset. Using DuckDB's SQL engine to handle the scale, we compute the average Origin-Destination (OD) matrix for each identified cluster, materializing the results in \texttt{gold.typical\_od\_matrices}.
\end{enumerate}

\paragraph{Visualization and Reporting\\}
To allow transport experts to validate these patterns, the pipeline automatically generates two types of analytical outputs stored in the results bucket:

\begin{itemize}
    \item \textbf{Temporal Profiles (Static):} A line chart visualization (via Matplotlib) plotting the centroids of the identified clusters. This clearly highlights the temporal differences between profiles, such as the steep morning commute peak in workdays versus the smoother midday curve typical of weekends.
    \item \textbf{Interactive OD Heatmaps:} An HTML report generated using Plotly. This tool allows analysts to interactively select a specific cluster and hour (e.g., ``Cluster 0 at 08:00 AM'') to visualize the spatial distribution of trips. The heatmap uses a logarithmic color scale to handle the high variance in trip counts between major urban centers and rural peripheries.
\end{itemize}



\subsection{Use Case 2: Infrastructure Gap Analysis}

The second business objective addresses the identification of zones where transport infrastructure fails to meet potential demand. To solve this, we implemented a \textbf{Gravity Model} in the Gold layer, comparing the theoretical interaction potential between municipalities against the actual mobility flows recorded by MITMA.

\paragraph{Implementation Approach\\}
The solution is orchestrated via an Airflow DAG (\texttt{32\_bq2\_gaps}). The logic follows a three-step process:

\begin{enumerate}
    \item \textbf{Theoretical Modeling:} We calculate a ``Gravity Score'' for every origin-destination pair using the classic impedance formula:
    % [GUIDE]: Explain the Gravity Model implementation.
    \begin{equation}
        T_{ij} = k \cdot \frac{P_i \cdot E_j}{d_{ij}^2}
    \end{equation}
    where $P_i$ is the population at the origin (from \texttt{metric\_population}), $E_j$ is the economic attractiveness (using \texttt{income\_per\_capita} from \texttt{metric\_ine\_rent} as a proxy), and $d_{ij}$ is the distance from our pre-computed \texttt{dim\_zone\_distances}.
    
    \item \textbf{Dynamic Normalization ($k$-factor):} To make the theoretical score comparable to physical trips, the code dynamically calculates a normalization factor ($k$) by computing the ratio between the total sum of actual MITMA trips and the total sum of gravity scores for the period.
    
    \item \textbf{Gap Detection:} The final metric, \texttt{mismatch\_ratio}, is derived by dividing actual trips by the normalized potential. A ratio significantly below 1.0 indicates a connection where mobility is lower than demographic and economic factors suggest, highlighting a potential infrastructure deficit.
\end{enumerate}

\paragraph{Spatial Analytics and Visualization\\}
The pipeline includes a spatial filtering mechanism allowing users to run this analysis on specific regions (e.g., passing a WKT Polygon for the ``Comunidad de Madrid''). Using DuckDB's spatial extension (\texttt{ST\_Intersects}), the system aggregates the results to generate two key visualizations using Kepler.gl:
\begin{itemize}
    \item \textbf{Service Level Ranking:} A bubble map where zones are sized by importance ($P \times E$) and colored by their average service level, instantly highlighting underserved economic hubs.
    \item \textbf{Mobility Arcs:} A flow map visualizing specific origin-destination connections that show the highest disparity between potential and actual traffic.
\end{itemize}


\subsection{Use Case 3: Functional Zoning Classification}

The third analytical product moves beyond simple traffic counting to characterize the \textit{functional role} of each municipality within the metropolitan network. By analyzing the directionality and retention of mobility flows, this use case automatically classifies zones into semantic categories (e.g., ``Bedroom Communities'' vs. ``Activity Hubs''), aiding urban planners in understanding regional dependencies.

\paragraph{Methodology and Classification Logic\\}
The implementation is encapsulated in the DAG \texttt{33\_bq3\_functional\_classification}. The logic aggregates the granular records from \texttt{fact\_mobility} into three distinct flow types per zone: \textbf{Inflow} (trips entering), \textbf{Outflow} (trips leaving), and \textbf{Internal} (trips starting and ending in the same zone). 

Using these aggregates, two key indices are calculated:
\begin{enumerate}
    \item \textbf{Net Flow Ratio:} Measures the balance of attraction.
    \[
    R_{\text{net}} = \frac{\text{Inflow} - \text{Outflow}}{\text{Inflow} + \text{Outflow}}
    \]
    \item \textbf{Retention Rate:} Measures the zone's self-sufficiency.
    \[
    R_{\text{retention}} = \frac{\text{Internal}}{\text{Outflow} + \text{Internal}}
    \]
\end{enumerate}

Based on these metrics, the SQL pipeline applies a hierarchical decision tree to assign a \texttt{functional\_label} to each municipality:

\begin{itemize}
    \item \textbf{Self-Sustaining Cell:} Zones with high retention ($R_{\text{retention}} > 0.20$), indicating a municipality that satisfies most of its residents' needs locally.
    \item \textbf{Activity Hub (Importer):} Zones with a positive net flow ($R_{\text{net}} > 0$), typically business districts or commercial centers attracting commuters.
    \item \textbf{Bedroom Community (Exporter):} Zones with a negative net flow ($R_{\text{net}} < 0$), characterizing residential areas where the population commutes out for work.
    \item \textbf{Balanced / Transit Zone:} Areas with neutral flow ratios acting as connectors.
\end{itemize}

\paragraph{Visualization Strategy\\}
The results are materialized in a Kepler.gl dashboard combining two visual layers to provide context:
\begin{itemize}
    \item \textbf{Polygon Layer (Choropleth):} Displays the administrative boundaries colored by their \texttt{functional\_label}. This reveals regional clusters (e.g., a ring of ``Bedroom Communities'' surrounding a central ``Activity Hub'').
    \item \textbf{Centroid Layer:} Displays bubbles sized by \texttt{total\_activity} (Sum of In + Out + Internal), allowing users to distinguish between major metropolitan centers and smaller rural nodes regardless of their functional classification.
\end{itemize}

\section{Discussion}
The implementation of this 3-tier Data Lakehouse represents a significant advancement over traditional file-based workflows for mobility analysis. By transitioning from ad-hoc processing of CSV files to a structured Lakehouse architecture, the system achieves three critical operational goals:

\begin{itemize}
    \item \textbf{Democratization of Big Data:} The architecture successfully decouples the complex data engineering lifecycle (ELT) from the analytical consumption. Transport experts, who previously required specialized programming skills to parse terabytes of daily CSVs, can now access the Gold layer using standard SQL queries or BI tools. The pre-computation of metrics like the \textit{gravity score} or \textit{functional labels} reduces the time-to-insight from days to seconds.
    
    \item \textbf{Cost-Effective Scalability:} The separation of storage (S3) and compute (DuckDB) allows the infrastructure to handle years of historical data with minimal overhead. Unlike coupled Data Warehouses, where costs accrue 24/7, this architecture only incurs compute costs during active transformation or querying. The use of ephemeral AWS Batch instances demonstrates that high-performance analytics can be achieved on a limited budget.
    
    \item \textbf{Contextual Enrichment:} The true value of the system lies in the integration achieved in the Silver layer. By enriching raw mobility flows with socio-economic context (INE data) and spatial distances, the system transforms raw trip counts into actionable accessibility metrics. This was demonstrated in Use Case 2, where the combination of population, rent, and distance revealed infrastructure gaps that raw mobility matrices alone could not identify.
\end{itemize}


\section{Conclusions and Limitations}

\subsection{Conclusions}
This project successfully designed and deployed a robust Data Lakehouse capable of ingesting, cleaning, and analyzing Spanish mobility data at scale. The 3-tier architecture ensures data governance, with a raw Bronze layer for auditability, a refined Silver layer for integration, and a business-ready Gold layer.

The implementation of the three use cases demonstrates the system's versatility: from identifying temporal patterns via Unsupervised Learning (Clustering) to solving domain-specific physics problems (Gravity Models) and geospatial classification. By leveraging DuckDB and DuckLake, the solution proves that ACID-compliant, high-performance analytics are achievable using open-source tools without the need for expensive enterprise licenses.

\subsection{Limitations}
Despite the success of the architecture, several limitations were identified during implementation, concerning both data quality and infrastructure constraints:

\begin{itemize}
    \item \textbf{Source Data Anomalies:} As noted in the MITMA technical documentation, the source datasets contain significant gaps during October and November 2023 due to mobile network incidents. While our pipeline's robust ingestion logic prevents these missing files from crashing the system, the resulting analytics for Q4 2023 require statistical imputation to be fully representative.
    
    \item \textbf{Infrastructure Constraints (Neon Catalog):} A critical bottleneck was observed regarding the metadata catalog management. We utilized the Neon Serverless Postgres Free Tier as the central catalog for DuckLake. It was observed that this tier enforces strict connection lifecycle limits. During the processing of large historical backfills, the catalog would frequently terminate the connection due to timeout limits on idle transactions, even while the compute engine (DuckDB) was actively writing data to S3. This necessitated the fragmentation of batch loads into smaller temporal windows to ensure transaction commit stability.
    
    \item \textbf{Vertical Scaling vs. Horizontal Distribution:} A structural limitation of using DuckDB is its single-node architecture. Unlike distributed engines like Apache Spark which scale horizontally (adding more machines to a cluster), DuckDB scales vertically (requiring a larger single machine). While AWS Batch allows us to request high-memory instances (e.g., 64GB or 128GB RAM), there is a theoretical ceiling. If a specific join or aggregation requires more memory than the largest available single EC2 instance, the query would fail or suffer severe performance degradation due to disk spilling.
    
    \item \textbf{Latency and Real-Time Analysis:} The current architecture is designed for $T+1$ (Daily) batch processing. Due to the provisioning time of AWS Batch instances and the file-based ingestion nature of the source, this system is not suitable for real-time traffic monitoring. Use cases requiring sub-minute latency (e.g., immediate accident detection or live congestion management) would require a Streaming architecture (e.g., Kafka + Flink) rather than the current Lakehouse approach.
\end{itemize}

% --- BIBLIOGRAPHY ---
\begin{thebibliography}{9}

\bibitem{mitma}
Spanish Ministry of Transport (MITMA).
\textit{Open Data Mobility}.
Available at: \url{https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad}
(Accessed on 4 January 2026).

\bibitem{ine}
Spanish National Statistics Institute (INE).
Available at: \url{https://www.ine.es/}
(Accessed on 4 January 2026).

\bibitem{cnig}
Centro Nacional de Información Geográfica (CNIG).
Available at: \url{https://www.ign.es/}
(Accessed on 4 January 2026).

\bibitem{madrid_data}
Ayuntamiento de Madrid.
\textit{Portal de Datos Abiertos}.
Available at: \url{https://datos.madrid.es/}
(Accessed on 4 January 2026).

\bibitem{duckdb}
DuckDB Documentation.
Available at: \url{https://duckdb.org/}
(Accessed on 4 January 2026).

\bibitem{aws_batch}
Amazon Web Service Batch.
\textit{AWS Batch Documentation}.
Available at: \url{https://aws.amazon.com/es/batch/}
(Accessed on 4 January 2026).

\bibitem{ducklake}
DuckLake.
\textit{DuckLake Documentation}.
Available at: \url{https://ducklake.select/docs/stable/}
(Accessed on 4 January 2026).

\bibitem{airflow}
Apache Software Foundation.
\textit{Apache Airflow Documentation}.
Available at: \url{https://airflow.apache.org/}
(Accessed on 4 January 2026).




\end{thebibliography}


\end{document}
