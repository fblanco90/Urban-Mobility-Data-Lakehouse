\documentclass[11pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% --- TYPOGRAPHY  ---
% Palatino for the main text (Serif, very readable)
\usepackage{mathpazo} 
% Helvetica for Section Headers (Sans Serif, creates contrast)
\usepackage[scaled=.95]{helvet} 
% Inconsolata for Code (Modern Monospace, better than Courier)
\usepackage{microtype}        % Micro-typography adjustments
\usepackage{parskip}          % Block paragraph spacing

% --- GRAPHICS & UTILS ---
\usepackage{graphicx}
\usepackage{booktabs}         % Professional tables
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{titlesec}         % Custom Section styling
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{listings}

% --- COLOR PALETTE ---
% Deep Teal for primary accent (Professional, modern)
\definecolor{accentColor}{RGB}{0, 77, 64} 
% Dark Grey for text/secondary
\definecolor{darkGrey}{RGB}{50, 50, 50}
% Soft Green for code comments
\definecolor{commentGreen}{RGB}{60, 120, 60}
% Deep Purple for code keywords
\definecolor{keywordPurple}{RGB}{100, 40, 120}
% Light grey background for code
\definecolor{codeBackground}{RGB}{250, 250, 250}

% --- HYPERLINK SETUP ---
\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=accentColor,    % Internal links (TOC, citations)
    filecolor=magenta,
    urlcolor=accentColor,     % Web URLs
    citecolor=accentColor,    % Citations
    pdftitle={Big Data Project},
    pdfpagemode=FullScreen,
}

% --- SECTION STYLING ---
% Makes Section titles Sans-Serif, Bold, and Accent Colored
\titleformat{\section}
  {\sffamily\Large\bfseries\color{accentColor}}{\thesection}{1em}{}
  
\titleformat{\subsection}
  {\sffamily\large\bfseries\color{accentColor}}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
  {\sffamily\normalsize\bfseries\color{darkGrey}}{\thesubsubsection}{1em}{}

% --- CODE LISTING STYLE ---
\lstdefinestyle{modernCode}{
    backgroundcolor=\color{codeBackground},   
    commentstyle=\color{commentGreen}\itshape,
    keywordstyle=\color{keywordPurple}\bfseries,
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange!80!black},
    basicstyle=\ttfamily\small, % Uses Inconsolata
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=8pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=lines, % Top and bottom lines only (cleaner look)
    rulecolor=\color{gray!50},
    framerule=0.5pt
}
\lstset{style=modernCode}

% --- HEADER & FOOTER ---
\pagestyle{fancy}
\fancyhf{}
% Using Sans-Serif for headers to match Section Titles
\lhead{\sffamily\small\color{darkGrey} Big Data Engineering Project}
\rhead{\sffamily\small\color{darkGrey} Lakehouse for Mobility Analysis}
\cfoot{\sffamily\small Page \thepage \ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt} % Removed footer line for cleaner look

% --- METADATA ---
\newcommand{\Title}{Building a Scalable 3-Tier Data Lakehouse for Mobility Analysis in Spain}
\newcommand{\Subject}{Big Data Engineering and Technologies}
\newcommand{\authors}{María López Hernández\\
Joan Sánchez Verdú\\
Fernando Blanco Membrives}
\newcommand{\Degree}{Master’s Degree in Computational Engineering \& Industrial Mathematicse}
\newcommand{\Logo}{imgs/yourLogo}
\newcommand{\University}{Polytechnic University of Valencia}
\newcommand{\School}{Higher Polytechnic School of Alcoy}

\begin{document}



\begin{titlepage}
    \centering

    % ---------- Top banner ----------
    \vspace*{-1.5cm}
    \includegraphics[width=0.45\linewidth]{\Logo}\\[2.5ex]
    {\large \textsc{\University}}\\[0.5ex]
    {\large \School}\\[2cm]

    % ---------- Degree ----------
    {\large \Degree}\\[2.5cm]

    % ---------- Subject ----------
    {\LARGE \textbf{\Subject}}\\[2cm]

    % ---------- Title ----------
    \rule{\linewidth}{0.5mm}\\[0.6ex]
    {\Huge \Title}\\[0.6ex]
    \rule{\linewidth}{0.5mm}\\[3cm]

    % ---------- Author ----------
    \textit{Authors}\\[0.5ex]
    {\Large \authors}\\[0.5ex]


    % ---------- Bottom ----------
    \vfill
    {\today}\\[1cm]
\end{titlepage}


% --- ABSTRACT ---
\begin{abstract}
    \noindent 
    The rapid digitalization of urban transport has generated massive volumes of mobility data, creating challenges that exceed the capabilities of traditional analytical workflows. This dissertation presents a scalable, 3-tier Data Lakehouse architecture designed to analyze large-scale Spanish mobility data. By integrating high-volume Origin-Destination matrices from MITMA with INE socio-economic indicators, the system establishes a robust foundation for urban analytics. The technical infrastructure employs a serverless stack, utilizing \textit{DuckDB} for vectorized processing and \textit{DuckLake} for ACID transactions, orchestrated via Apache Airflow. We validate the architecture through three analytical use cases: temporal pattern clustering, infrastructure gap detection via gravity models, and functional zoning classification.\\ The results demonstrate a cost-effective, high-performance solution for data-driven transport planning.
\end{abstract}

\tableofcontents
\newpage

% --- SECTIONS ---


\section{Introduction}

The availability of high-resolution mobility data provided by the Spanish Ministry of Transport (MITMA) has significantly enhanced the potential for data-driven urban planning. However, the magnitude of these datasets, comprising billions of daily trip records, poses engineering challenges that surpass the limits of conventional analytical software. Transportation planners often lack the accessible, scalable infrastructure necessary to convert this raw big data into strategic insights.

This project bridges this technical gap by implementing a Data Lakehouse architecture. By integrating the cost-effectiveness of data lakes with the structured data management of data warehouses, we propose a solution leveraging \textbf{DuckDB} and \textbf{DuckLake}. This infrastructure enables the efficient processing of massive mobility matrices and their enrichment with socio-economic indicators, avoiding the high complexity and maintenance overhead of traditional enterprise clusters.

\subsection{Objectives}
The primary goal of this project is to build a robust, reproducible, and cost-efficient data platform. The specific objectives are defined as follows:

\begin{itemize}
    \item \textbf{Scalable Architecture:} Design a Medallion (Bronze, Silver, Gold) Lakehouse architecture that decouples storage (Amazon S3) from compute (AWS Batch) to maximize both cost-efficiency and query performance.
    \item \textbf{Data Integration:} Develop automated ELT pipelines to sanitize and fuse heterogeneous datasets, including mobility flows, census demographics, and geospatial vector data.
    \item \textbf{Applied Analytics:} Validate the system's utility through advanced use cases, specifically applying gravity models for infrastructure impact analysis and defining functional zoning classifications.
\end{itemize}

\vspace{2cm}

\section{Data Sources}
The Data Lakehouse architecture is designed to ingest and integrate public domain data exclusively. This approach ensures the system remains reproducible, cost-effective, and fully accessible. The following subsections detail the primary mobility datasets and the secondary enrichment sources selected to build the information system.

\subsection{MITMA Open Data}
The core mobility dataset is retrieved from the Spanish Ministry of Transport, Mobility and Urban Agenda (MITMA \cite{mitma}), specifically the \textit{Estudios Básicos de Movilidad}. This dataset provides high-resolution insights into daily population movements, derived from aggregated mobile network signaling data.

To ensure transparency and facilitate reproduction of this study, the specific files utilized from the MITMA repository are detailed in Table \ref{tab:mitma_files}.

\begin{table}[H]
\centering
\caption{MITMA Mobility and Zoning Data Sources}
\label{tab:mitma_files}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llp{8cm}@{}}
\toprule
\textbf{Data Type} & \textbf{Filename / Pattern} & \textbf{Description} \\ \midrule
Mobility Matrices & \texttt{(202301-202312)\_Viajes\_municipios.tar} & Daily mobility flows between origin and destination municipalities, aggregated by hour for the year 2023. \newline \textit{Path: \texttt{estudios\_basicos/por-municipios/}} \\ \midrule
Zoning Metadata & \texttt{nombres\_municipios.csv} & Official nomenclature for each municipality. \newline \textit{Path: \texttt{zonificacion/zonificacion\_municipios/}} \\ \midrule
Crosswalk & \texttt{relacion\_ine\_zonificacionMitma.csv} & Mapping table linking MITMA custom zoning codes to standard INE municipal codes. \newline \textit{Path: \texttt{zonificacion/}} \\ \midrule
Demographics & \texttt{poblacion.csv} & Annual resident population counts per municipality. \newline \textit{Path: \texttt{zonificacion/}} \\ \midrule
Geometries & \texttt{zonificacion\_municipios.shp} & ESRI Shapefile containing the geospatial vector polygons for the municipal study zones. \newline \textit{Path: \texttt{zonificacion/zonificacion\_municipios/}} \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Enrichment Data Sources}
To contextualize the raw mobility flows and enable advanced gravity model analysis, the system integrates socioeconomic and temporal data from external official sources. These datasets allow for the characterization of municipalities by economic attractiveness and the classification of dates by business activity.

These enrichment sources are summarized in Table \ref{tab:enrichment_data}.

\begin{table}[H]
\centering
\caption{Socioeconomic and Temporal Enrichment Data}
\label{tab:enrichment_data}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llp{8cm}@{}}
\toprule
\textbf{Source} & \textbf{Dataset / Filename} & \textbf{Description} \\ \midrule
\textbf{INE} \cite{ine} & \texttt{ine\_rent\_municipalities.csv} & \textbf{Net Income Distribution:} Provides the average net income per year and municipality. This metric serves as a key proxy for economic attractiveness in gravity models. \newline \textit{Source: \url{https://www.ine.es/jaxiT3/files/t/es/csv_bd/}} \\ \midrule
\textbf{Open Data Madrid} \cite{madrid_data} & \texttt{calendario\_laboral.csv} & \textbf{Working Calendar:} Contains the official calendar from 2013 to 2026, classifying dates into working days, weekends, and national holidays to segregate mobility patterns by day type. \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Lakehouse Architecture}
The system adheres to the \textbf{Medallion Architecture} pattern, a multi-hop design that progressively improves data quality. Critically, the architecture decouples storage from compute, ensuring that the system can scale to handle billions of records without the cost overhead of permanently running clusters.

\subsection{Technology Stack}
The platform employs a modular cloud-native stack (Figure \ref{fig:architecture}), leveraging the following core components:

\begin{description}
    \item[Compute Engine (DuckDB \cite{duckdb}):] A high-performance, in-process SQL OLAP engine. DuckDB utilizes vectorized query execution and supports out-of-core processing, enabling the analysis of datasets significantly larger than available RAM on a single node.
    
    \item[Elastic Compute (AWS Batch \cite{aws_batch}):] Provides an ephemeral, serverless compute layer. It offloads resource-intensive transformation tasks to on-demand container instances, optimizing costs by leveraging Spot market pricing.
    
    \item[Lakehouse Management (DuckLake \cite{ducklake}):] Functions as the transaction manager, enforcing ACID properties and snapshot isolation over object storage. This ensures data consistency and prevents race conditions during concurrent write operations.

    \item[Cloud Storage \& Catalog (Neon + S3):] A hybrid storage design where \textbf{AWS S3} serves as the physical data layer (storing Parquet/CSV files), while \textbf{Neon} (Serverless Postgres) acts as the centralized metadata catalog and locking mechanism.

    \item[Orchestration (Apache Airflow \cite{airflow}):] Manages the end-to-end data lifecycle via Directed Acyclic Graphs (DAGs), handling dependency resolution, task scheduling, and automated failure recovery.
\end{description}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/Architecture_diagram}
    \caption{High-level overview of the 3-Tier Data Lakehouse architecture, decoupling compute (DuckDB) from storage (S3).}
    \label{fig:architecture}
\end{figure}

\subsection{Data Layering Strategy}
The data pipeline is structured into three distinct zones—Bronze, Silver, and Gold—each serving a specific validation and analytical purpose. This design preserves the raw input for auditability while providing refined datasets for downstream consumption.

\subsubsection{Bronze Layer (Raw Ingestion)}
The Bronze layer serves as the immutable landing zone (Staging Area). Its primary objective is to capture external data in its native format with zero information loss.

In this implementation, a distinct table is instantiated for each source entity to ensure isolation. To prevent ingestion failures caused by schema drift or type mismatches, all tabular columns are initially cast as \texttt{VARCHAR}. A notable exception is the geospatial data: by leveraging DuckDB's \texttt{spatial} extension, ESRI Shapefiles are ingested using the \texttt{ST\_Read} function. This captures municipal boundaries directly as native \texttt{GEOMETRY} objects, preserving the original spatial precision.

To enhance governance, the ingestion process appends metadata columns to every record, including \texttt{ingestion\_timestamp} and \texttt{source\_url}. Furthermore, the high-volume mobility data is physically partitioned by date (\texttt{fecha}) to optimize I/O performance during batch processing.

\subsubsection{Silver Layer (Cleaned \& Integrated)}
The Silver layer functions as the enterprise-wide, trusted data repository. In this stage, data undergoes rigorous validation, standardization, and enrichment. The schema follows a \textbf{Star Schema} topology, centering on a mobility fact table linked to surrounding dimensions for spatial and socio-economic context.

\paragraph{Dimension Modeling and Spatial Integration (\texttt{dim\_zones})\\}
The foundation of the Silver layer is the unified spatial dimension, \texttt{dim\_zones}. This table integrates descriptive metadata (MITMA and INE codes) with the geospatial vectors ingested in the Bronze layer.
\begin{itemize}
    \item \textbf{Spatial Integration:} By joining municipal geometries with the \texttt{mapping\_ine\_mitma} crosswalk, the system creates a spatially-enabled master dimension.
    \item \textbf{Surrogate Keys:} A unique integer identifier (\texttt{zone\_id}) is generated for each municipality. This decouples downstream analytics from unstable string-based source codes and improves join performance.
\end{itemize}

\paragraph{Spatial Connectivity (\texttt{dim\_zone\_distances})\\}
To support network analysis, a derivative dimension table, \texttt{dim\_zone\_distances}, serves as a pre-computed distance matrix. By calculating the Euclidean distance between the centroids of geometries in \texttt{dim\_zones}, the system eliminates the need for expensive geospatial operations at query time. The \texttt{dist\_km} column stores the distance in kilometers as a \texttt{DOUBLE} precision value.

\paragraph{Socio-Economic Enrichment\\}
Complementary metric tables are generated to enrich the mobility data.
\begin{itemize}
    \item \textbf{\texttt{metric\_population}:} Raw counts are cast to \texttt{BIGINT} and stripped of metadata headers.
    \item \textbf{\texttt{metric\_ine\_rent}:} The pipeline filters the INE dataset to isolate the ``Average Net Income per Person,'' explicitly excluding sub-municipal (district/section) entries to ensure 1:1 alignment with the \texttt{dim\_zones} table.
\end{itemize}

\paragraph{Mobility Fact Table Construction (\texttt{fact\_mobility})\\}
The core mobility flows are transformed into the \texttt{fact\_mobility} table. This process converts raw CSVs into a strongly typed, time-series optimized format:
\begin{enumerate}
    \item \textbf{Temporal Standardization:} Separate date and hour fields are merged into a single \texttt{TIMESTAMP WITH TIME ZONE} column, localized to 'Europe/Madrid'.
    \item \textbf{Referential Integrity:} Origin/Destination codes are replaced by their corresponding \texttt{zone\_id} via double joins against \texttt{dim\_zones}.
    \item \textbf{Type Casting:} Trip counts are cleaned and cast to \texttt{DOUBLE} to support fractional expansion factors.
\end{enumerate}

\paragraph{Data Observability (\texttt{data\_quality\_logs})\\}
To ensure pipeline reliability, a dedicated metadata registry, \texttt{data\_quality\_logs}, persists the results of automated audit checks (e.g., null rates, row count anomalies). This allows for longitudinal tracking of data health, storing the \texttt{check\_timestamp}, \texttt{metric\_name}, and \texttt{metric\_value} for every batch execution.

\subsubsection{Gold Layer (Analytical Data Mart)}
The Gold layer is organized around specific business problems rather than data structures. Tables here are denormalized, pre-aggregated, and optimized for low-latency querying by Business Intelligence (BI) tools.

The schema consists of three purpose-built analytical tables:

\begin{description}
    \item[\texttt{gold\_typical\_day\_patterns} (Temporal Analysis):] 
    Reduces billions of raw records into lightweight hourly profiles.
    \begin{itemize}
        \item \textit{Purpose:} Enables instant plotting of demand curves without re-scanning raw data.
        \item \textit{Structure:} Aggregated by \texttt{cluster\_id} (e.g., "Weekday", "Holiday") and \texttt{hour}, storing the \texttt{avg\_trips}.
    \end{itemize}

    \item[\texttt{gold\_infrastructure\_gaps} (Gravity Models):] 
    A specialized Origin-Destination (OD) matrix enriched with socio-economic context.
    \begin{itemize}
        \item \textit{Purpose:} Supports the Gravity Model analysis (Use Case 2) by combining flow data with impedance metrics.
        \item \textit{Structure:} Contains \texttt{total\_population} (Origin), \texttt{rent} (Destination attractiveness), and \texttt{dist\_km}. The key derivative is the \texttt{mismatch\_ratio}, quantifying the gap between theoretical potential and observed trips.
    \end{itemize}

    \item[\texttt{gold\_zone\_functional\_classification} (Zoning):] 
    A semantic layer that assigns functional roles to municipalities.
    \begin{itemize}
        \item \textit{Purpose:} Simplifies map visualizations by converting raw IDs into human-readable classifications (e.g., "Bedroom Community").
        \item \textit{Structure:} Persists calculated indices such as \texttt{net\_flow\_ratio} (trade balance) and \texttt{retention\_rate} (self-sufficiency).
    \end{itemize}
\end{description}

\newpage

\section{Implementation Methodology}
The operational workflow of the Data Lakehouse is automated through a modular architecture orchestrated by \textbf{Apache Airflow} \cite{airflow}. The system adopts a decoupled design where distinct Directed Acyclic Graphs (DAGs) manage specific stages of the data lifecycle, separated by their update frequency and functional scope.

This section details the engineering strategies employed to ensure scalable data ingestion, atomic processing of daily batches, and the automated generation of analytical models. The implementation prioritizes robustness through strict transaction management, task isolation, and the separation of heavy computational ELT processes from on-demand reporting queries.

\subsection{Orchestration Strategy}
To efficiently manage dependencies between the Bronze, Silver, and Gold layers, the system is architected into a multi-DAG ecosystem. This design enables independent scaling for static dimensions versus high-volume mobility facts.

\begin{enumerate} 
    \item \textbf{Infrastructure \& Dimensions DAG:} Manages the ingestion of low-velocity static data (Demographics, Zoning, Calendars). It establishes the schema foundations and populates the dimension tables in the Bronze and Silver layers.
    
    \item \textbf{Mobility Ingestion DAG:} A parameterized, high-throughput worker pipeline. It utilizes dynamic task mapping to ingest, clean, and transform daily mobility matrices from Bronze to Silver using atomic, parallelizable tasks.
    
    \item \textbf{Analytical \& Reporting DAGs:} These pipelines (DAGs 31–33) consolidate business logic into unified flows. Triggered on-demand with custom parameters, they materialize aggregated Gold tables and immediately generate final visual assets (Kepler.gl maps, Matplotlib charts) for delivery to S3.
\end{enumerate}

\subsection{Infrastructure \& Dimensions (DAG 1)}
The foundational pipeline, \texttt{infrastructure\_and\_dimensions}, is responsible for initializing the Lakehouse schema and ingesting low-velocity reference data. Unlike the daily mobility processing, this DAG is designed for on-demand or annual execution, as zoning definitions and census statistics change infrequently.

\textbf{Initialization Phase:} \\
Before data movement begins, the pipeline ensures environmental consistency. The \texttt{create\_schemas} task connects to the Neon catalog to verify the existence of the logical namespaces (\textit{bronze, silver, gold}). Simultaneously, the \texttt{create\_stats\_table} task initializes the \texttt{data\_quality\_log} table, a prerequisite for audit logging in all subsequent pipelines.

\textbf{Ingestion Phase:} \\
To optimize execution time, external extraction tasks run in parallel. These tasks utilize Airflow's retry mechanism (3 retries, 60s delay) to handle transient network issues. Specialized ingestion logic is applied based on the data format:
\begin{itemize}
    \item \textbf{Spatial Data:} The \texttt{br\_ingest\_geo\_data} task leverages DuckDB's \texttt{spatial} extension. It performs a pre-flight HTTP HEAD request to validate the remote Shapefiles before executing \texttt{ST\_Read} to ingest vector geometries directly into the Bronze layer.
    \item \textbf{Statistical Data:} Tasks targeting INE data (e.g., \texttt{br\_ingest\_ine\_rent}) enforce \texttt{ISO-8859-1} encoding and specific separators to prevent parsing errors common in legacy government datasets.
    \item \textbf{Dictionaries:} Zoning and mapping CSVs are ingested via \texttt{read\_csv\_auto}, with metadata columns (\texttt{source\_url}, \texttt{ingestion\_timestamp}) appended dynamically.
\end{itemize}

\textbf{Silver Transformation (Funnel Pattern):} \\
The dependency structure in the Silver layer follows a "Funnel" pattern (Figure \ref{fig:dag1_flow}). The system enforces a hard dependency on the Master Dimension before populating satellite metrics.
\begin{enumerate}
    \item \textbf{Convergence Point (\texttt{dim\_zones}):} This task acts as a synchronization barrier. It awaits the completion of three Bronze tasks (Geometry, Zoning names, and INE Mapping), before performing a 3-way join to construct the master \texttt{dim\_zones} table and generate surrogate \texttt{zone\_id} keys.
    \item \textbf{Satellite Expansion:} Once the master dimension is established, downstream tasks (e.g., \texttt{metric\_population}, \texttt{dim\_zone\_distance\_matrix}) trigger in parallel. These tasks join their respective raw sources with \texttt{dim\_zones} to propagate the correct surrogate keys to the statistical data.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imgs/infrastructure_and_dimensions-DAG}
    \caption{Dependency graph of the Infrastructure DAG, illustrating the funnel pattern for dimension creation.}
    \label{fig:dag1_flow}
\end{figure}

\subsection{Mobility Ingestion (DAG 2)}
The second pipeline, \texttt{mobility\_ingestion}, constitutes the core processing engine of the Lakehouse. It handles the daily ingestion and transformation of high-volume Origin-Destination matrices. Given that the source data is partitioned by day, this DAG utilizes \textbf{Airflow Dynamic Task Mapping} to scale horizontally based on run-time parameters.

\textbf{Execution Planning:} \\
The workflow begins with the \texttt{generate\_date\_list} task, which parses the user-supplied \texttt{start\_date} and \texttt{end\_date} to produce a list of target partition keys (e.g., \texttt{['20230101', '20230102']}). Simultaneously, idempotent initialization tasks (\texttt{ensure\_br\_mobility...}) prepare the partitioning structures in the Bronze/Silver layers to prevent race conditions during parallel writes.

\textbf{Atomic Processing:} \\
The \texttt{br\_process\_single\_day} task is mapped over the date list, with each instance handling a specific day. Robustness is ensured through:
\begin{itemize}
    \item \textbf{Pre-flight Validation:} An HTTP \texttt{HEAD} check prevents pipeline failure if a specific date file is missing from the source server (a known issue with MITMA late-2023 data).
    \item \textbf{Throttling:} Concurrency is limited (\texttt{max\_active\_tis=2}) to prevent memory saturation, with a strict retry policy for network resilience.
    \item \textbf{Streaming Ingestion:} Valid files are streamed directly into DuckDB memory using \texttt{read\_csv\_auto}, bypassing local disk I/O.
\end{itemize}

\textbf{Referential Integrity:} \\
The transformation task (\texttt{sl\_process\_single\_day}) mirrors the ingestion mapping. It enforces integrity by joining raw mobility records with the \texttt{dim\_zones} table (from DAG 1). Records with invalid or unknown zone codes are implicitly filtered, ensuring that the Silver layer contains only clean, linkable data.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imgs/mobility_ingestion-DAG}
    \caption{The Mobility Ingestion DAG utilizing Dynamic Task Mapping for parallel daily processing.}
    \label{fig:dag2_flow}
\end{figure}

\newpage

\subsection{Analytical Generation and Reporting (DAGs 3)}
The analytical layer is materialized through three dedicated pipelines. Each DAG addresses a specific business question (BQ) by transforming Silver data into Gold aggregates and generating visual reports.

\paragraph{DAG 31: Typical Day Clustering (BQ1)\\}
This pipeline characterizes temporal mobility patterns via unsupervised learning.
\begin{itemize}
    \item \textbf{Transformation:} Aggregates mobility data to extract hourly profiles and applies K-Means clustering ($k=3$) to categorize days.
    \item \textbf{Output:} Generates a static Matplotlib chart for temporal profiles and an interactive Plotly HTML heatmap for OD analysis (Figure \ref{fig:bq1_results}).
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/bq1-1}
        \caption{Temporal mobility profiles (Clustering results)}
        \label{fig:bq1_profiles}
    \end{subfigure}
    
    \vspace{0.5cm}
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/bq1-2}
        \caption{Interactive Origin-Destination Heatmap}
        \label{fig:bq1_heatmap}
    \end{subfigure}
    \caption{Visualization results for BQ1: Typical Daily Mobility Patterns.}
    \label{fig:bq1_results}
\end{figure}

\paragraph{DAG 32: Infrastructure Gap Analysis (BQ2)\\}
This pipeline implements a Gravity Model to quantify the disparity between theoretical potential and actual mobility.
\begin{itemize}
    \item \textbf{Transformation:} Integrates population, income, and distance metrics to calculate a theoretical ``Gravity Score'' ($P \cdot E / d^2$). This is compared against observed flows to derive a \texttt{mismatch\_ratio}.
    \item \textbf{Output:} Uses Kepler.gl to generate a \textit{Service Level Ranking} and a \textit{Mobility Gaps} arc map (Figure \ref{fig:bq2_results}).
\end{itemize}

\vspace{1cm}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/bq2-1}
        \caption{Zone Service Level Ranking}
        \label{fig:bq2_ranking}
    \end{subfigure}

    \vspace{1cm}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/bq2-2}
        \caption{Inter-urban Mobility Gaps (Arc Map)}
        \label{fig:bq2_arcs}
    \end{subfigure}
    \caption{Visualization results for BQ2: Infrastructure Gaps and Mobility Potential.}
    \label{fig:bq2_results}
\end{figure}

\newpage

\paragraph{DAG 33: Functional Classification (BQ3)\\}
This pipeline categorizes municipalities based on their structural role in the metropolitan network.
\begin{itemize}
    \item \textbf{Transformation:} Calculates flow metrics (retention rates, net flow ratios) and applies a deterministic decision tree to assign functional labels (e.g., ``Bedroom Community'').
    \item \textbf{Output:} Produces a Kepler.gl Choropleth map illustrating the region's functional hierarchy (Figure \ref{fig:bq3_results}).
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imgs/bq3-1}
    \caption{BQ3 Results: Functional Zone Classification based on net flows and retention rates.}
    \label{fig:bq3_results}
\end{figure}

\vspace{0.5cm}

\section{Results and Use Cases}

\subsection{Use Case 1: Typical Mobility Patterns and Profiling}
The first analytical objective focuses on the identification and characterization of canonical mobility behaviors within the reference year (2023). Traditional transport planning often relies on static calendar rules, assuming for instance, that all Mondays exhibit identical traffic pressure. To overcome this limitation, we implemented an unsupervised machine learning approach in the Gold layer to discover intrinsic daily patterns, distinguishing between standard workdays, holidays, and transition days based on the geometry of their hourly flow curves.



\paragraph{Implementation Approach\\}
The solution is orchestrated via the Airflow DAG \texttt{31\_bq1\_clustering}. The pipeline employs a hybrid processing strategy, leveraging DuckDB for high-volume aggregation and Python (Scikit-Learn) for in-memory clustering. The methodology follows a three-stage process:

\begin{description}
    \item[Feature Extraction:] 
    Granular records from \texttt{silver.fact\_mobility} are aggregated into daily hourly profiles. To ensure the algorithm clusters based on the \textit{temporal shape} of demand (e.g., the presence of AM/PM peaks) rather than absolute volume, the feature vector for every date $d$ is normalized:
    \begin{equation}
        V_{d, h} = \frac{T_{d, h}}{\sum_{h=0}^{23} T_{d, h}}
    \end{equation}
    where $T_{d, h}$ is the total trip count on day $d$ at hour $h$.

    \item[Unsupervised Classification (K-Means):] 
    The normalized vectors are processed using the K-Means algorithm ($k=3$). This automatically classifies every date in the 2023 calendar into a distinct behavioral profile (e.g., "Standard Commute," "Leisure/Weekend," or "National Holiday").

    \item[Pattern Projection:] 
    Once the semantic labels are assigned, the system projects these classifications back onto the full dataset. Using DuckDB's vectorized engine, we compute the average Origin-Destination (OD) matrix for each cluster, materializing the results in \texttt{gold.typical\_od\_matrices}.
\end{description}

\paragraph{Visualization and Reporting\\}
To facilitate validation by transport experts, the pipeline generates two automated reporting assets:
\begin{itemize}
    \item \textbf{Temporal Profiles (Static):} A Matplotlib line chart plotting the centroids of the identified clusters. This visualizes the structural differences between profiles, such as the steep morning commute peak characteristic of workdays versus the bell-shaped curve typical of weekends.
    \item \textbf{Interactive OD Heatmaps:} A Plotly HTML report allowing analysts to filter by cluster and hour (e.g., "Cluster 0 at 08:00 AM"). This uses a logarithmic color scale to visualize the spatial distribution of trips, handling the high variance between urban cores and rural peripheries.
\end{itemize}

\subsection{Use Case 2: Infrastructure Gap Analysis}
The second objective addresses the detection of spatial inefficiencies where transport infrastructure fails to meet potential demand. To quantify this, we implemented a \textbf{Gravity Model} in the Gold layer. This approach models the theoretical interaction potential between municipalities based on their mass (population/economy) and impedance (distance) and compares it against the actual flows observed in the MITMA dataset.



\paragraph{Implementation Approach\\}
The analysis is encapsulated in the \texttt{32\_bq2\_gaps} DAG and executes in three logical steps:

\begin{enumerate}
    \item \textbf{Theoretical Modeling:} 
    A "Gravity Score" is calculated for every Origin-Destination pair $(i, j)$ using the classic interaction formula:
    \begin{equation}
        G_{ij} = k \cdot \frac{P_i \cdot E_j}{d_{ij}^2}
    \end{equation}
    where $P_i$ represents the population at the origin (supply), $E_j$ represents the economic attractiveness at the destination (using net income per capita as a proxy), and $d_{ij}$ is the Euclidean distance derived from \texttt{dim\_zone\_distances}.

    \item \textbf{Dynamic Normalization ($k$-factor):} 
    To render the theoretical score comparable to physical trip counts, the system calculates a global normalization factor $k$. This is derived by equating the total sum of observed trips in the network to the total sum of gravity scores.

    \item \textbf{Gap Detection:} 
    The critical metric, \texttt{mismatch\_ratio}, is derived by dividing the actual observed trips by the normalized potential ($G_{ij}$). A ratio significantly below $1.0$ identifies connections where mobility is suppressed relative to demographic pressure, signaling a potential infrastructure deficit.
\end{enumerate}

\paragraph{Spatial Analytics and Visualization\\}
The pipeline includes a spatial filtering mechanism (accepting WKT Polygons) to restrict analysis to specific regions (e.g., "Comunidad de Madrid"). Results are visualized via Kepler.gl:
\begin{itemize}
    \item \textbf{Service Level Ranking:} A bubble map where zones are sized by theoretical importance ($P \times E$) and colored by their service level ratio.
    \item \textbf{Mobility Arcs:} A geospatial flow map highlighting specific links where the disparity between potential and actual traffic is most severe.
\end{itemize}

\subsection{Use Case 3: Functional Zoning Classification}
The third analytical product moves beyond raw traffic counting to characterize the \textit{functional role} of each municipality within the metropolitan system. By analyzing the directionality and retention of flows, this use case automatically classifies zones into semantic categories (e.g., "Bedroom Communities" vs. "Activity Hubs").



\paragraph{Methodology and Classification Logic\\}
The logic, encapsulated in DAG \texttt{33\_bq3\_functional\_classification}, aggregates granular records into three flow types per zone: \textbf{Inflow} (entering), \textbf{Outflow} (leaving), and \textbf{Internal} (intra-zonal). 

From these aggregates, two defining indices are calculated:

\begin{description}
    \item[Net Flow Ratio ($R_{net}$):] Measures the balance of trade in trips.
    \[ R_{\text{net}} = \frac{\text{Inflow} - \text{Outflow}}{\text{Inflow} + \text{Outflow}} \]

    \item[Retention Rate ($R_{ret}$):] Measures the zone's self-sufficiency.
    \[ R_{\text{ret}} = \frac{\text{Internal}}{\text{Outflow} + \text{Internal}} \]
\end{description}

Based on these metrics, a deterministic decision tree assigns a \texttt{functional\_label} to each municipality:
\begin{itemize}
    \item \textbf{Self-Sustaining Cell ($R_{ret} > 0.20$):} Zones that satisfy a significant portion of their residents' needs locally.
    \item \textbf{Activity Hub / Importer ($R_{net} > 0$):} Zones with a positive flow balance, typically business districts attracting commuters.
    \item \textbf{Bedroom Community / Exporter ($R_{net} < 0$):} Zones with a negative flow balance, characterizing residential areas reliant on external employment centers.
    \item \textbf{Balanced / Transit Zone:} Areas with neutral flow ratios acting as connectors.
\end{itemize}

\paragraph{Visualization Strategy\\}
The results are materialized in a Kepler.gl dashboard combining two visual layers:
\begin{itemize}
    \item \textbf{Choropleth Layer:} Displays administrative boundaries colored by their \texttt{functional\_label}, revealing regional clusters (e.g., residential rings surrounding central hubs).
    \item \textbf{Centroid Layer:} Displays bubbles sized by \texttt{total\_activity} (Sum of In + Out + Internal), distinguishing major metropolitan centers from smaller rural nodes regardless of their functional type.
\end{itemize}

\vspace{1cm}

\section{Discussion}
The implementation of this 3-tier Data Lakehouse represents a paradigm shift from traditional, file-based workflows in mobility analysis. By transitioning from ad-hoc processing of disparate CSV files to a structured, transactional Lakehouse architecture, the system successfully bridges the gap between raw data availability and actionable urban planning insights.

The results of the implementation validate the architecture's ability to achieve three critical operational milestones:

\begin{itemize}
    \item \textbf{Operational Democratization and Accessibility:} 
    The architecture effectively decouples the complex data engineering lifecycle (ELT) from analytical consumption. Transport experts, who previously faced significant technical barriers when parsing terabytes of raw daily records, can now interact with the Gold layer using standard SQL queries or Business Intelligence (BI) tools. By pre-computing complex derivatives—such as \textit{gravity scores} and \textit{functional zoning labels}—the system reduces analytical latency from days to seconds, enabling rapid hypothesis testing.
    
    \item \textbf{Cost-Effective Scalability:} 
    The architectural decision to decouple storage (Amazon S3) from compute (DuckDB/AWS Batch) ensures the system can retain years of historical data with negligible holding costs. Unlike tightly coupled Data Warehouses where infrastructure costs accrue continuously, this serverless approach incurs compute costs only during active transformation or querying. This empirically validates that high-performance analytics on national-scale datasets is achievable within a constrained academic or municipal budget.
    
    \item \textbf{High-Dimensional Contextualization:} 
    The primary analytical value is derived from the integration logic within the Silver layer. By enriching raw mobility flows with socio-economic indicators (INE income/demographics) and geospatial impedance metrics, the system transforms simple trip counts into sophisticated accessibility indicators. As demonstrated in Use Case 2, this fusion reveals infrastructure deficits that would remain invisible if mobility matrices were analyzed in isolation.
\end{itemize}

Ultimately, this platform demonstrates that open-source tools (DuckDB, Apache Airflow) can replicate the capabilities of enterprise-grade big data systems. It provides a reproducible blueprint for public administrations to internalize the processing of mobility data, reducing reliance on third-party aggregators and ensuring data sovereignty.


\section{Conclusions and Limitations}

\subsection{Conclusions}
This dissertation presents the design and successful deployment of a robust Data Lakehouse architecture capable of ingesting, cleaning, and analyzing national-scale mobility data. The implementation confirms that modern open-source technologies can effectively democratize access to big data in the urban planning domain. The key conclusions drawn from this work are:

\begin{itemize}
    \item \textbf{Architectural Viability:} The 3-tier Medallion architecture (Bronze, Silver, Gold) proved essential for data governance. By decoupling storage (S3) from compute (AWS Batch), the system achieved a high degree of cost-efficiency and reproducibility, validating the "Lakehouse" paradigm as a superior alternative to monolithic Data Warehouses for intermittent analytical workloads.
    
    \item \textbf{Analytical Versatility:} The deployment of three distinct analytical pipelines demonstrated the platform's flexibility. The system successfully supported diverse computational patterns, ranging from unsupervised machine learning (Clustering for temporal profiling) to physics-based simulations (Gravity Models) and semantic geospatial classification.
    
    \item \textbf{Technology Stack Validation:} The integration of \textbf{DuckDB} and \textbf{DuckLake} provided a performant, ACID-compliant SQL engine without the overhead of enterprise clusters. This confirms that single-node, vectorized processing is sufficient for handling billions of records when leveraging modern columnar formats (Parquet).
\end{itemize}

\subsection{Limitations}
While the proposed architecture meets the primary research objectives, several limitations were identified regarding external dependencies and infrastructure constraints. These challenges delineate the boundaries of the current system and suggest areas for future optimization.

\begin{itemize}
    \item \textbf{Source Data Continuity:} 
    The reliability of the system is strictly bound by the upstream provider. As documented in the MITMA technical specifications, the source datasets exhibit significant discontinuities during Q4 2023 due to mobile network signaling incidents. Although the pipeline's ingestion logic successfully handles missing files without failure, downstream analytics for this period require statistical imputation to be statistically representative.
    
    \item \textbf{Metadata Catalog Constraints (Neon):} 
    A critical bottleneck was identified in the interaction between the compute engine and the metadata catalog. The implementation utilized the generic Free Tier of Neon (Serverless Postgres) to manage DuckLake transactions. It was observed that the strict idle-connection timeouts and auto-suspend policies of the serverless tier are incompatible with long-running bulk ingestion tasks. This necessitated the artificial fragmentation of historical backfills into smaller temporal windows to ensure transaction stability, increasing total processing time.
    
    \item \textbf{Vertical Scalability Ceiling:} 
    A fundamental structural limitation of DuckDB is its single-node architecture. Unlike distributed systems (e.g., Apache Spark) which scale horizontally by adding nodes to a cluster, DuckDB relies on vertical scaling (Scale-Up). While AWS Batch allows for the provisioning of high-memory instances (e.g., 128GB RAM), the system has a theoretical hard limit. Analytical queries requiring memory exceeding the capacity of the largest available single EC2 instance will degrade significantly due to disk spilling or fail entirely.
    
    \item \textbf{Latency and Real-Time Applicability:} 
    The architecture is intrinsically designed for $T+1$ (Daily) batch processing. Due to the inherent latency in provisioning ephemeral AWS Batch containers and the file-based delivery mechanism of the source data, this platform is not suitable for real-time monitoring. Use cases requiring sub-minute latency—such as immediate accident detection or live traffic signal optimization—would necessitate a Streaming architecture (e.g., Apache Kafka/Flink) rather than the current batch-oriented Lakehouse approach.
\end{itemize}



\newpage

% --- BIBLIOGRAPHY ---
\begin{thebibliography}{9}

\bibitem{mitma}
Spanish Ministry of Transport (MITMA).
\textit{Open Data Mobility}.
Available at: \url{https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad}
(Accessed on 4 January 2026).

\bibitem{ine}
Spanish National Statistics Institute (INE).
Available at: \url{https://www.ine.es/}
(Accessed on 4 January 2026).

\bibitem{cnig}
Centro Nacional de Información Geográfica (CNIG).
Available at: \url{https://www.ign.es/}
(Accessed on 4 January 2026).

\bibitem{madrid_data}
Ayuntamiento de Madrid.
\textit{Portal de Datos Abiertos}.
Available at: \url{https://datos.madrid.es/}
(Accessed on 4 January 2026).

\bibitem{duckdb}
DuckDB Documentation.
Available at: \url{https://duckdb.org/}
(Accessed on 4 January 2026).

\bibitem{aws_batch}
Amazon Web Service Batch.
\textit{AWS Batch Documentation}.
Available at: \url{https://aws.amazon.com/es/batch/}
(Accessed on 4 January 2026).

\bibitem{ducklake}
DuckLake.
\textit{DuckLake Documentation}.
Available at: \url{https://ducklake.select/docs/stable/}
(Accessed on 4 January 2026).

\bibitem{airflow}
Apache Software Foundation.
\textit{Apache Airflow Documentation}.
Available at: \url{https://airflow.apache.org/}
(Accessed on 4 January 2026).

\end{thebibliography}

\end{document}
